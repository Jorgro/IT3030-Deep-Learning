{
    "regularization": "lr1",
    "regularization_weight": 0.001,
    "learning_rate": 0.01,
    "loss_function": "mean_squared",
    "input_dimension": 30,
    "output_activation": "softmax",
    "layers": [{
            "size": 25,
            "activation_function": "ReLU",
            "weight_range": "",
            "learning_rate": 0.01,
            "bias_range": ""
        },
        {
            "size": 1,
            "activation_function": "Sigmoid",
            "weight_range": "",
            "bias_range": ""
        }
    ]
}
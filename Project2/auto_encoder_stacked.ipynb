{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "from stacked_mnist import StackedMNISTData, DataMode\n",
    "from tensorflow import keras\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, encoded_space_dim, fc2_input_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        ### Convolutional section\n",
    "        self.encoder_cnn = nn.Sequential(\n",
    "            # First convolutional layer\n",
    "            nn.Conv3d(1, 8, (3, 3, 1), stride=2, padding=1),\n",
    "            # nn.BatchNorm2d(8),\n",
    "            nn.ReLU(True),\n",
    "            # Second convolutional layer\n",
    "            nn.Conv3d(8, 16, (3, 3, 1), stride=2, padding=1),\n",
    "            nn.BatchNorm3d(16),\n",
    "            nn.ReLU(True),\n",
    "            # Third convolutional layer\n",
    "            nn.Conv3d(16, 32, (3, 3, 1), stride=2, padding=1),\n",
    "            # nn.BatchNorm2d(32),\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "\n",
    "        ### Flatten layer\n",
    "        self.flatten = nn.Flatten(start_dim=1)\n",
    "\n",
    "        ### Linear section\n",
    "        self.encoder_lin = nn.Sequential(\n",
    "            # First linear layer\n",
    "            nn.Linear(4 * 4 * 32*3, 128),\n",
    "            nn.ReLU(True),\n",
    "            # Second linear layer\n",
    "            nn.Linear(128, encoded_space_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply convolutions\n",
    "        x = self.encoder_cnn(x)\n",
    "        # Flatten\n",
    "        x = self.flatten(x)\n",
    "        # # Apply linear layers\n",
    "        x = self.encoder_lin(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, encoded_space_dim,fc2_input_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        ### Linear section\n",
    "        self.decoder_lin = nn.Sequential(\n",
    "            # First linear layer\n",
    "            nn.Linear(encoded_space_dim, 128),\n",
    "            nn.ReLU(True),\n",
    "            # Second linear layer\n",
    "            nn.Linear(128, 4 * 4 * 3 * 32),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "\n",
    "        ### Unflatten\n",
    "        self.unflatten = nn.Unflatten(dim=1, unflattened_size=(32, 4, 4, 3))\n",
    "\n",
    "        ### Convolutional section\n",
    "        self.decoder_conv = nn.Sequential(\n",
    "            # First transposed convolution\n",
    "            nn.ConvTranspose3d(32, 16, (3, 3, 1), stride=2, padding=1, output_padding=0),\n",
    "            nn.BatchNorm3d(16),\n",
    "            nn.ReLU(True),\n",
    "            # Second transposed convolution\n",
    "            nn.ConvTranspose3d(16, 8, (3, 3, 1), stride=2, padding=1, output_padding=(1, 1, 0)),\n",
    "            nn.BatchNorm3d(8),\n",
    "            nn.ReLU(True),\n",
    "            # Third transposed convolution\n",
    "            nn.ConvTranspose3d(8, 1, (3, 3, 1), stride=2, padding=1, output_padding=(1, 1, 0))\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Apply linear layers\n",
    "        x = self.decoder_lin(x)\n",
    "        # Unflatten\n",
    "        x = self.unflatten(x)\n",
    "        # Apply transposed convolutions\n",
    "        x = self.decoder_conv(x)\n",
    "        # Apply a sigmoid to force the output to be between 0 and 1 (valid pixel values)\n",
    "        x = torch.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv3d-1         [-1, 8, 14, 14, 3]              80\n",
      "              ReLU-2         [-1, 8, 14, 14, 3]               0\n",
      "            Conv3d-3          [-1, 16, 7, 7, 3]           1,168\n",
      "       BatchNorm3d-4          [-1, 16, 7, 7, 3]              32\n",
      "              ReLU-5          [-1, 16, 7, 7, 3]               0\n",
      "            Conv3d-6          [-1, 32, 4, 4, 3]           4,640\n",
      "              ReLU-7          [-1, 32, 4, 4, 3]               0\n",
      "           Flatten-8                 [-1, 1536]               0\n",
      "            Linear-9                  [-1, 128]         196,736\n",
      "             ReLU-10                  [-1, 128]               0\n",
      "           Linear-11                    [-1, 8]           1,032\n",
      "================================================================\n",
      "Total params: 203,688\n",
      "Trainable params: 203,688\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 0.16\n",
      "Params size (MB): 0.78\n",
      "Estimated Total Size (MB): 0.95\n",
      "----------------------------------------------------------------\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1                  [-1, 128]           1,152\n",
      "              ReLU-2                  [-1, 128]               0\n",
      "            Linear-3                 [-1, 1536]         198,144\n",
      "              ReLU-4                 [-1, 1536]               0\n",
      "         Unflatten-5          [-1, 32, 4, 4, 3]               0\n",
      "   ConvTranspose3d-6          [-1, 16, 7, 7, 3]           4,624\n",
      "       BatchNorm3d-7          [-1, 16, 7, 7, 3]              32\n",
      "              ReLU-8          [-1, 16, 7, 7, 3]               0\n",
      "   ConvTranspose3d-9         [-1, 8, 14, 14, 3]           1,160\n",
      "      BatchNorm3d-10         [-1, 8, 14, 14, 3]              16\n",
      "             ReLU-11         [-1, 8, 14, 14, 3]               0\n",
      "  ConvTranspose3d-12         [-1, 1, 28, 28, 3]              73\n",
      "================================================================\n",
      "Total params: 205,201\n",
      "Trainable params: 205,201\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.22\n",
      "Params size (MB): 0.78\n",
      "Estimated Total Size (MB): 1.00\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "d = 24\n",
    "encoder = Encoder(encoded_space_dim=d,fc2_input_dim=128)\n",
    "decoder = Decoder(encoded_space_dim=d,fc2_input_dim=128)\n",
    "#print(encoder)\n",
    "#print(decoder)\n",
    "summary(encoder, (1, 28, 28, 3))\n",
    "summary(decoder, (8,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.MSELoss()\n",
    "lr= 0.001\n",
    "params_to_optimize = [\n",
    "    {'params': encoder.parameters()},\n",
    "    {'params': decoder.parameters()}\n",
    "]\n",
    "optim = torch.optim.Adam(params_to_optimize, lr=lr, weight_decay=1e-05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Training function\n",
    "def train_epoch(encoder, decoder, loss_fn, optimizer, x):\n",
    "    # Set train mode for both the encoder and the decoder\n",
    "    encoder.train()\n",
    "    decoder.train()\n",
    "    train_loss = []\n",
    "\n",
    "    # Encode data\n",
    "    encoded_data = encoder(x)\n",
    "    # Decode data\n",
    "    decoded_data = decoder(encoded_data)\n",
    "    # Evaluate loss\n",
    "    loss = loss_fn(decoded_data, x)\n",
    "    # Backward pass\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    # Print batch loss\n",
    "    print('\\t partial train loss (single batch): %f' % (loss.data))\n",
    "    train_loss.append(loss.detach().numpy())\n",
    "\n",
    "    return train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Testing function\n",
    "def test_epoch(encoder, decoder, loss_fn, x):\n",
    "    # Set evaluation mode for encoder and decoder\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    with torch.no_grad(): # No need to track the gradients\n",
    "        # Define the lists to store the outputs for each batch\n",
    "        conc_out = []\n",
    "        conc_label = []\n",
    "        # Encode data\n",
    "        encoded_data = encoder(x)\n",
    "        # Decode data\n",
    "        decoded_data = decoder(encoded_data)\n",
    "        # Append the network output and the original image to the lists\n",
    "        conc_out.append(decoded_data)\n",
    "        conc_label.append(x)\n",
    "        # Create a single tensor with all the values in the lists\n",
    "        conc_out = torch.cat(conc_out)\n",
    "        conc_label = torch.cat(conc_label) \n",
    "        # Evaluate global loss\n",
    "        val_loss = loss_fn(conc_out, conc_label)\n",
    "    return val_loss.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 3)\n",
      "(10000, 3)\n"
     ]
    }
   ],
   "source": [
    "generator = StackedMNISTData(mode=DataMode.COLOR_BINARY_COMPLETE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t partial train loss (single batch): 0.214959\n",
      "Epoch 1/1000 train loss: [array(0.2149593, dtype=float32)], val loss: 0.19601519405841827\n",
      "\t partial train loss (single batch): 0.210839\n",
      "Epoch 2/1000 train loss: [array(0.21083863, dtype=float32)], val loss: 0.19600771367549896\n",
      "\t partial train loss (single batch): 0.207082\n",
      "Epoch 3/1000 train loss: [array(0.20708239, dtype=float32)], val loss: 0.19615569710731506\n",
      "\t partial train loss (single batch): 0.204025\n",
      "Epoch 4/1000 train loss: [array(0.20402499, dtype=float32)], val loss: 0.1960400491952896\n",
      "\t partial train loss (single batch): 0.201083\n",
      "Epoch 5/1000 train loss: [array(0.20108308, dtype=float32)], val loss: 0.1955767720937729\n",
      "\t partial train loss (single batch): 0.198399\n",
      "Epoch 6/1000 train loss: [array(0.19839886, dtype=float32)], val loss: 0.19580622017383575\n",
      "\t partial train loss (single batch): 0.196775\n",
      "Epoch 7/1000 train loss: [array(0.19677502, dtype=float32)], val loss: 0.1957424134016037\n",
      "\t partial train loss (single batch): 0.194178\n",
      "Epoch 8/1000 train loss: [array(0.19417816, dtype=float32)], val loss: 0.19527538120746613\n",
      "\t partial train loss (single batch): 0.192387\n",
      "Epoch 9/1000 train loss: [array(0.19238724, dtype=float32)], val loss: 0.19500967860221863\n",
      "\t partial train loss (single batch): 0.190977\n",
      "Epoch 10/1000 train loss: [array(0.19097693, dtype=float32)], val loss: 0.19514822959899902\n",
      "\t partial train loss (single batch): 0.189375\n",
      "Epoch 11/1000 train loss: [array(0.18937476, dtype=float32)], val loss: 0.1954868882894516\n",
      "\t partial train loss (single batch): 0.188311\n",
      "Epoch 12/1000 train loss: [array(0.18831086, dtype=float32)], val loss: 0.1950485110282898\n",
      "\t partial train loss (single batch): 0.187041\n",
      "Epoch 13/1000 train loss: [array(0.18704093, dtype=float32)], val loss: 0.1947302520275116\n",
      "\t partial train loss (single batch): 0.186341\n",
      "Epoch 14/1000 train loss: [array(0.18634059, dtype=float32)], val loss: 0.19457197189331055\n",
      "\t partial train loss (single batch): 0.185065\n",
      "Epoch 15/1000 train loss: [array(0.18506457, dtype=float32)], val loss: 0.1947917491197586\n",
      "\t partial train loss (single batch): 0.184201\n",
      "Epoch 16/1000 train loss: [array(0.18420143, dtype=float32)], val loss: 0.19467298686504364\n",
      "\t partial train loss (single batch): 0.184019\n",
      "Epoch 17/1000 train loss: [array(0.18401879, dtype=float32)], val loss: 0.19401800632476807\n",
      "\t partial train loss (single batch): 0.182113\n",
      "Epoch 18/1000 train loss: [array(0.18211333, dtype=float32)], val loss: 0.1941295862197876\n",
      "\t partial train loss (single batch): 0.181041\n",
      "Epoch 19/1000 train loss: [array(0.18104136, dtype=float32)], val loss: 0.19425569474697113\n",
      "\t partial train loss (single batch): 0.181481\n",
      "Epoch 20/1000 train loss: [array(0.18148063, dtype=float32)], val loss: 0.19407367706298828\n",
      "\t partial train loss (single batch): 0.179528\n",
      "Epoch 21/1000 train loss: [array(0.17952818, dtype=float32)], val loss: 0.194467693567276\n",
      "\t partial train loss (single batch): 0.179885\n",
      "Epoch 22/1000 train loss: [array(0.17988516, dtype=float32)], val loss: 0.19406303763389587\n",
      "\t partial train loss (single batch): 0.179211\n",
      "Epoch 23/1000 train loss: [array(0.17921108, dtype=float32)], val loss: 0.1936529576778412\n",
      "\t partial train loss (single batch): 0.177857\n",
      "Epoch 24/1000 train loss: [array(0.1778568, dtype=float32)], val loss: 0.19277873635292053\n",
      "\t partial train loss (single batch): 0.177572\n",
      "Epoch 25/1000 train loss: [array(0.1775722, dtype=float32)], val loss: 0.19310946762561798\n",
      "\t partial train loss (single batch): 0.176459\n",
      "Epoch 26/1000 train loss: [array(0.17645861, dtype=float32)], val loss: 0.19290518760681152\n",
      "\t partial train loss (single batch): 0.177354\n",
      "Epoch 27/1000 train loss: [array(0.17735447, dtype=float32)], val loss: 0.19233566522598267\n",
      "\t partial train loss (single batch): 0.175648\n",
      "Epoch 28/1000 train loss: [array(0.17564811, dtype=float32)], val loss: 0.1915057748556137\n",
      "\t partial train loss (single batch): 0.175142\n",
      "Epoch 29/1000 train loss: [array(0.17514244, dtype=float32)], val loss: 0.19192931056022644\n",
      "\t partial train loss (single batch): 0.175039\n",
      "Epoch 30/1000 train loss: [array(0.17503922, dtype=float32)], val loss: 0.1905285120010376\n",
      "\t partial train loss (single batch): 0.174601\n",
      "Epoch 31/1000 train loss: [array(0.17460121, dtype=float32)], val loss: 0.19095060229301453\n",
      "\t partial train loss (single batch): 0.174274\n",
      "Epoch 32/1000 train loss: [array(0.17427413, dtype=float32)], val loss: 0.18962408602237701\n",
      "\t partial train loss (single batch): 0.173697\n",
      "Epoch 33/1000 train loss: [array(0.17369711, dtype=float32)], val loss: 0.1898336410522461\n",
      "\t partial train loss (single batch): 0.172685\n",
      "Epoch 34/1000 train loss: [array(0.17268524, dtype=float32)], val loss: 0.1889396458864212\n",
      "\t partial train loss (single batch): 0.172300\n",
      "Epoch 35/1000 train loss: [array(0.17230015, dtype=float32)], val loss: 0.1881069839000702\n",
      "\t partial train loss (single batch): 0.172121\n",
      "Epoch 36/1000 train loss: [array(0.17212065, dtype=float32)], val loss: 0.1873859465122223\n",
      "\t partial train loss (single batch): 0.171406\n",
      "Epoch 37/1000 train loss: [array(0.17140621, dtype=float32)], val loss: 0.18694598972797394\n",
      "\t partial train loss (single batch): 0.171202\n",
      "Epoch 38/1000 train loss: [array(0.17120211, dtype=float32)], val loss: 0.18586377799510956\n",
      "\t partial train loss (single batch): 0.170703\n",
      "Epoch 39/1000 train loss: [array(0.17070274, dtype=float32)], val loss: 0.1847730576992035\n",
      "\t partial train loss (single batch): 0.169301\n",
      "Epoch 40/1000 train loss: [array(0.16930124, dtype=float32)], val loss: 0.18526573479175568\n",
      "\t partial train loss (single batch): 0.169721\n",
      "Epoch 41/1000 train loss: [array(0.16972138, dtype=float32)], val loss: 0.18411269783973694\n",
      "\t partial train loss (single batch): 0.169406\n",
      "Epoch 42/1000 train loss: [array(0.16940613, dtype=float32)], val loss: 0.18276286125183105\n",
      "\t partial train loss (single batch): 0.168529\n",
      "Epoch 43/1000 train loss: [array(0.16852923, dtype=float32)], val loss: 0.18194562196731567\n",
      "\t partial train loss (single batch): 0.168452\n",
      "Epoch 44/1000 train loss: [array(0.1684522, dtype=float32)], val loss: 0.18097393214702606\n",
      "\t partial train loss (single batch): 0.167672\n",
      "Epoch 45/1000 train loss: [array(0.16767219, dtype=float32)], val loss: 0.1808260828256607\n",
      "\t partial train loss (single batch): 0.167266\n",
      "Epoch 46/1000 train loss: [array(0.16726632, dtype=float32)], val loss: 0.17870432138442993\n",
      "\t partial train loss (single batch): 0.165990\n",
      "Epoch 47/1000 train loss: [array(0.16599019, dtype=float32)], val loss: 0.1783902496099472\n",
      "\t partial train loss (single batch): 0.166921\n",
      "Epoch 48/1000 train loss: [array(0.16692124, dtype=float32)], val loss: 0.17695169150829315\n",
      "\t partial train loss (single batch): 0.165574\n",
      "Epoch 49/1000 train loss: [array(0.16557376, dtype=float32)], val loss: 0.17599445581436157\n",
      "\t partial train loss (single batch): 0.166324\n",
      "Epoch 50/1000 train loss: [array(0.16632368, dtype=float32)], val loss: 0.17570191621780396\n",
      "\t partial train loss (single batch): 0.166327\n",
      "Epoch 51/1000 train loss: [array(0.16632682, dtype=float32)], val loss: 0.17441776394844055\n",
      "\t partial train loss (single batch): 0.164570\n",
      "Epoch 52/1000 train loss: [array(0.16456966, dtype=float32)], val loss: 0.17305971682071686\n",
      "\t partial train loss (single batch): 0.164627\n",
      "Epoch 53/1000 train loss: [array(0.16462749, dtype=float32)], val loss: 0.17254340648651123\n",
      "\t partial train loss (single batch): 0.164598\n",
      "Epoch 54/1000 train loss: [array(0.1645977, dtype=float32)], val loss: 0.17112858593463898\n",
      "\t partial train loss (single batch): 0.164569\n",
      "Epoch 55/1000 train loss: [array(0.1645688, dtype=float32)], val loss: 0.17087142169475555\n",
      "\t partial train loss (single batch): 0.163852\n",
      "Epoch 56/1000 train loss: [array(0.16385223, dtype=float32)], val loss: 0.16950678825378418\n",
      "\t partial train loss (single batch): 0.164190\n",
      "Epoch 57/1000 train loss: [array(0.16419034, dtype=float32)], val loss: 0.16837431490421295\n",
      "\t partial train loss (single batch): 0.163131\n",
      "Epoch 58/1000 train loss: [array(0.16313088, dtype=float32)], val loss: 0.16848842799663544\n",
      "\t partial train loss (single batch): 0.161967\n",
      "Epoch 59/1000 train loss: [array(0.16196653, dtype=float32)], val loss: 0.16763781011104584\n",
      "\t partial train loss (single batch): 0.162726\n",
      "Epoch 60/1000 train loss: [array(0.16272588, dtype=float32)], val loss: 0.1662643551826477\n",
      "\t partial train loss (single batch): 0.161963\n",
      "Epoch 61/1000 train loss: [array(0.16196294, dtype=float32)], val loss: 0.16581344604492188\n",
      "\t partial train loss (single batch): 0.161915\n",
      "Epoch 62/1000 train loss: [array(0.16191502, dtype=float32)], val loss: 0.16476333141326904\n",
      "\t partial train loss (single batch): 0.161895\n",
      "Epoch 63/1000 train loss: [array(0.1618946, dtype=float32)], val loss: 0.16431450843811035\n",
      "\t partial train loss (single batch): 0.161384\n",
      "Epoch 64/1000 train loss: [array(0.16138403, dtype=float32)], val loss: 0.16421620547771454\n",
      "\t partial train loss (single batch): 0.160931\n",
      "Epoch 65/1000 train loss: [array(0.16093095, dtype=float32)], val loss: 0.16318866610527039\n",
      "\t partial train loss (single batch): 0.160883\n",
      "Epoch 66/1000 train loss: [array(0.16088311, dtype=float32)], val loss: 0.16224369406700134\n",
      "\t partial train loss (single batch): 0.160082\n",
      "Epoch 67/1000 train loss: [array(0.16008219, dtype=float32)], val loss: 0.16203270852565765\n",
      "\t partial train loss (single batch): 0.159923\n",
      "Epoch 68/1000 train loss: [array(0.15992327, dtype=float32)], val loss: 0.16105659306049347\n",
      "\t partial train loss (single batch): 0.160193\n",
      "Epoch 69/1000 train loss: [array(0.16019285, dtype=float32)], val loss: 0.1611415594816208\n",
      "\t partial train loss (single batch): 0.160123\n",
      "Epoch 70/1000 train loss: [array(0.16012327, dtype=float32)], val loss: 0.1602950245141983\n",
      "\t partial train loss (single batch): 0.158831\n",
      "Epoch 71/1000 train loss: [array(0.15883079, dtype=float32)], val loss: 0.15991750359535217\n",
      "\t partial train loss (single batch): 0.159663\n",
      "Epoch 72/1000 train loss: [array(0.15966289, dtype=float32)], val loss: 0.15938688814640045\n",
      "\t partial train loss (single batch): 0.157953\n",
      "Epoch 73/1000 train loss: [array(0.15795322, dtype=float32)], val loss: 0.1580185443162918\n",
      "\t partial train loss (single batch): 0.158654\n",
      "Epoch 74/1000 train loss: [array(0.15865365, dtype=float32)], val loss: 0.15887002646923065\n",
      "\t partial train loss (single batch): 0.158840\n",
      "Epoch 75/1000 train loss: [array(0.15884039, dtype=float32)], val loss: 0.1577717512845993\n",
      "\t partial train loss (single batch): 0.158332\n",
      "Epoch 76/1000 train loss: [array(0.15833247, dtype=float32)], val loss: 0.158577099442482\n",
      "\t partial train loss (single batch): 0.157877\n",
      "Epoch 77/1000 train loss: [array(0.15787716, dtype=float32)], val loss: 0.15676650404930115\n",
      "\t partial train loss (single batch): 0.157700\n",
      "Epoch 78/1000 train loss: [array(0.15769984, dtype=float32)], val loss: 0.15718859434127808\n",
      "\t partial train loss (single batch): 0.156819\n",
      "Epoch 79/1000 train loss: [array(0.15681872, dtype=float32)], val loss: 0.1568966805934906\n",
      "\t partial train loss (single batch): 0.156791\n",
      "Epoch 80/1000 train loss: [array(0.15679078, dtype=float32)], val loss: 0.1571669727563858\n",
      "\t partial train loss (single batch): 0.156649\n",
      "Epoch 81/1000 train loss: [array(0.15664889, dtype=float32)], val loss: 0.15675854682922363\n",
      "\t partial train loss (single batch): 0.156401\n",
      "Epoch 82/1000 train loss: [array(0.15640147, dtype=float32)], val loss: 0.15574616193771362\n",
      "\t partial train loss (single batch): 0.155820\n",
      "Epoch 83/1000 train loss: [array(0.15581979, dtype=float32)], val loss: 0.15568509697914124\n",
      "\t partial train loss (single batch): 0.155699\n",
      "Epoch 84/1000 train loss: [array(0.15569866, dtype=float32)], val loss: 0.15655472874641418\n",
      "\t partial train loss (single batch): 0.156431\n",
      "Epoch 85/1000 train loss: [array(0.15643084, dtype=float32)], val loss: 0.1555250585079193\n",
      "\t partial train loss (single batch): 0.154806\n",
      "Epoch 86/1000 train loss: [array(0.15480569, dtype=float32)], val loss: 0.15515035390853882\n",
      "\t partial train loss (single batch): 0.156305\n",
      "Epoch 87/1000 train loss: [array(0.15630537, dtype=float32)], val loss: 0.1559513658285141\n",
      "\t partial train loss (single batch): 0.154573\n",
      "Epoch 88/1000 train loss: [array(0.15457253, dtype=float32)], val loss: 0.155621737241745\n",
      "\t partial train loss (single batch): 0.154680\n",
      "Epoch 89/1000 train loss: [array(0.15467954, dtype=float32)], val loss: 0.15476633608341217\n",
      "\t partial train loss (single batch): 0.154325\n",
      "Epoch 90/1000 train loss: [array(0.15432473, dtype=float32)], val loss: 0.1541338711977005\n",
      "\t partial train loss (single batch): 0.155251\n",
      "Epoch 91/1000 train loss: [array(0.15525068, dtype=float32)], val loss: 0.15469661355018616\n",
      "\t partial train loss (single batch): 0.154576\n",
      "Epoch 92/1000 train loss: [array(0.1545763, dtype=float32)], val loss: 0.15439927577972412\n",
      "\t partial train loss (single batch): 0.154170\n",
      "Epoch 93/1000 train loss: [array(0.15417026, dtype=float32)], val loss: 0.154028981924057\n",
      "\t partial train loss (single batch): 0.153041\n",
      "Epoch 94/1000 train loss: [array(0.15304057, dtype=float32)], val loss: 0.15572847425937653\n",
      "\t partial train loss (single batch): 0.153390\n",
      "Epoch 95/1000 train loss: [array(0.15338992, dtype=float32)], val loss: 0.15411491692066193\n",
      "\t partial train loss (single batch): 0.152792\n",
      "Epoch 96/1000 train loss: [array(0.15279211, dtype=float32)], val loss: 0.15415611863136292\n",
      "\t partial train loss (single batch): 0.153432\n",
      "Epoch 97/1000 train loss: [array(0.15343206, dtype=float32)], val loss: 0.15366311371326447\n",
      "\t partial train loss (single batch): 0.152397\n",
      "Epoch 98/1000 train loss: [array(0.1523972, dtype=float32)], val loss: 0.1539713740348816\n",
      "\t partial train loss (single batch): 0.152172\n",
      "Epoch 99/1000 train loss: [array(0.15217204, dtype=float32)], val loss: 0.1527661234140396\n",
      "\t partial train loss (single batch): 0.152390\n",
      "Epoch 100/1000 train loss: [array(0.15238957, dtype=float32)], val loss: 0.1533624827861786\n",
      "\t partial train loss (single batch): 0.153210\n",
      "Epoch 101/1000 train loss: [array(0.15320972, dtype=float32)], val loss: 0.1521051675081253\n",
      "\t partial train loss (single batch): 0.152446\n",
      "Epoch 102/1000 train loss: [array(0.15244591, dtype=float32)], val loss: 0.15176939964294434\n",
      "\t partial train loss (single batch): 0.152369\n",
      "Epoch 103/1000 train loss: [array(0.15236863, dtype=float32)], val loss: 0.15262769162654877\n",
      "\t partial train loss (single batch): 0.151810\n",
      "Epoch 104/1000 train loss: [array(0.15181042, dtype=float32)], val loss: 0.15191395580768585\n",
      "\t partial train loss (single batch): 0.151267\n",
      "Epoch 105/1000 train loss: [array(0.15126726, dtype=float32)], val loss: 0.15185385942459106\n",
      "\t partial train loss (single batch): 0.150411\n",
      "Epoch 106/1000 train loss: [array(0.15041128, dtype=float32)], val loss: 0.1519496887922287\n",
      "\t partial train loss (single batch): 0.151199\n",
      "Epoch 107/1000 train loss: [array(0.15119919, dtype=float32)], val loss: 0.1521083265542984\n",
      "\t partial train loss (single batch): 0.151670\n",
      "Epoch 108/1000 train loss: [array(0.15166959, dtype=float32)], val loss: 0.15127691626548767\n",
      "\t partial train loss (single batch): 0.151157\n",
      "Epoch 109/1000 train loss: [array(0.15115723, dtype=float32)], val loss: 0.15180644392967224\n",
      "\t partial train loss (single batch): 0.150764\n",
      "Epoch 110/1000 train loss: [array(0.15076356, dtype=float32)], val loss: 0.15130019187927246\n",
      "\t partial train loss (single batch): 0.150659\n",
      "Epoch 111/1000 train loss: [array(0.1506594, dtype=float32)], val loss: 0.1502513289451599\n",
      "\t partial train loss (single batch): 0.150057\n",
      "Epoch 112/1000 train loss: [array(0.15005666, dtype=float32)], val loss: 0.15036356449127197\n",
      "\t partial train loss (single batch): 0.150286\n",
      "Epoch 113/1000 train loss: [array(0.15028611, dtype=float32)], val loss: 0.1516154557466507\n",
      "\t partial train loss (single batch): 0.150094\n",
      "Epoch 114/1000 train loss: [array(0.15009427, dtype=float32)], val loss: 0.15003018081188202\n",
      "\t partial train loss (single batch): 0.149396\n",
      "Epoch 115/1000 train loss: [array(0.14939645, dtype=float32)], val loss: 0.1498541384935379\n",
      "\t partial train loss (single batch): 0.150093\n",
      "Epoch 116/1000 train loss: [array(0.15009256, dtype=float32)], val loss: 0.1500398963689804\n",
      "\t partial train loss (single batch): 0.148919\n",
      "Epoch 117/1000 train loss: [array(0.14891937, dtype=float32)], val loss: 0.14904777705669403\n",
      "\t partial train loss (single batch): 0.149161\n",
      "Epoch 118/1000 train loss: [array(0.14916098, dtype=float32)], val loss: 0.14993321895599365\n",
      "\t partial train loss (single batch): 0.149333\n",
      "Epoch 119/1000 train loss: [array(0.14933325, dtype=float32)], val loss: 0.14950433373451233\n",
      "\t partial train loss (single batch): 0.149493\n",
      "Epoch 120/1000 train loss: [array(0.1494927, dtype=float32)], val loss: 0.1498638540506363\n",
      "\t partial train loss (single batch): 0.147952\n",
      "Epoch 121/1000 train loss: [array(0.14795218, dtype=float32)], val loss: 0.1483655869960785\n",
      "\t partial train loss (single batch): 0.148857\n",
      "Epoch 122/1000 train loss: [array(0.14885743, dtype=float32)], val loss: 0.14902015030384064\n",
      "\t partial train loss (single batch): 0.148036\n",
      "Epoch 123/1000 train loss: [array(0.14803594, dtype=float32)], val loss: 0.14868460595607758\n",
      "\t partial train loss (single batch): 0.148378\n",
      "Epoch 124/1000 train loss: [array(0.14837842, dtype=float32)], val loss: 0.1482371687889099\n",
      "\t partial train loss (single batch): 0.147806\n",
      "Epoch 125/1000 train loss: [array(0.14780588, dtype=float32)], val loss: 0.14843641221523285\n",
      "\t partial train loss (single batch): 0.147280\n",
      "Epoch 126/1000 train loss: [array(0.14728035, dtype=float32)], val loss: 0.14841637015342712\n",
      "\t partial train loss (single batch): 0.147689\n",
      "Epoch 127/1000 train loss: [array(0.14768864, dtype=float32)], val loss: 0.14813245832920074\n",
      "\t partial train loss (single batch): 0.147438\n",
      "Epoch 128/1000 train loss: [array(0.1474382, dtype=float32)], val loss: 0.1488361656665802\n",
      "\t partial train loss (single batch): 0.147383\n",
      "Epoch 129/1000 train loss: [array(0.14738275, dtype=float32)], val loss: 0.14842545986175537\n",
      "\t partial train loss (single batch): 0.146789\n",
      "Epoch 130/1000 train loss: [array(0.14678891, dtype=float32)], val loss: 0.1476067155599594\n",
      "\t partial train loss (single batch): 0.146678\n",
      "Epoch 131/1000 train loss: [array(0.14667843, dtype=float32)], val loss: 0.1466391235589981\n",
      "\t partial train loss (single batch): 0.147124\n",
      "Epoch 132/1000 train loss: [array(0.14712422, dtype=float32)], val loss: 0.14730390906333923\n",
      "\t partial train loss (single batch): 0.146627\n",
      "Epoch 133/1000 train loss: [array(0.14662707, dtype=float32)], val loss: 0.14732956886291504\n",
      "\t partial train loss (single batch): 0.146398\n",
      "Epoch 134/1000 train loss: [array(0.14639807, dtype=float32)], val loss: 0.14693741500377655\n",
      "\t partial train loss (single batch): 0.146394\n",
      "Epoch 135/1000 train loss: [array(0.14639379, dtype=float32)], val loss: 0.14657750725746155\n",
      "\t partial train loss (single batch): 0.147065\n",
      "Epoch 136/1000 train loss: [array(0.14706522, dtype=float32)], val loss: 0.14649426937103271\n",
      "\t partial train loss (single batch): 0.145928\n",
      "Epoch 137/1000 train loss: [array(0.14592785, dtype=float32)], val loss: 0.1468551903963089\n",
      "\t partial train loss (single batch): 0.145107\n",
      "Epoch 138/1000 train loss: [array(0.14510661, dtype=float32)], val loss: 0.14707928895950317\n",
      "\t partial train loss (single batch): 0.146807\n",
      "Epoch 139/1000 train loss: [array(0.1468072, dtype=float32)], val loss: 0.14709505438804626\n",
      "\t partial train loss (single batch): 0.146448\n",
      "Epoch 140/1000 train loss: [array(0.14644752, dtype=float32)], val loss: 0.14533919095993042\n",
      "\t partial train loss (single batch): 0.144627\n",
      "Epoch 141/1000 train loss: [array(0.14462727, dtype=float32)], val loss: 0.14571985602378845\n",
      "\t partial train loss (single batch): 0.145608\n",
      "Epoch 142/1000 train loss: [array(0.14560843, dtype=float32)], val loss: 0.14606240391731262\n",
      "\t partial train loss (single batch): 0.145532\n",
      "Epoch 143/1000 train loss: [array(0.14553235, dtype=float32)], val loss: 0.14556899666786194\n",
      "\t partial train loss (single batch): 0.145423\n",
      "Epoch 144/1000 train loss: [array(0.14542344, dtype=float32)], val loss: 0.14618298411369324\n",
      "\t partial train loss (single batch): 0.145002\n",
      "Epoch 145/1000 train loss: [array(0.14500155, dtype=float32)], val loss: 0.14582130312919617\n",
      "\t partial train loss (single batch): 0.144533\n",
      "Epoch 146/1000 train loss: [array(0.1445329, dtype=float32)], val loss: 0.14547672867774963\n",
      "\t partial train loss (single batch): 0.145049\n",
      "Epoch 147/1000 train loss: [array(0.145049, dtype=float32)], val loss: 0.14515963196754456\n",
      "\t partial train loss (single batch): 0.144902\n",
      "Epoch 148/1000 train loss: [array(0.14490242, dtype=float32)], val loss: 0.14524272084236145\n",
      "\t partial train loss (single batch): 0.144072\n",
      "Epoch 149/1000 train loss: [array(0.14407194, dtype=float32)], val loss: 0.14541105926036835\n",
      "\t partial train loss (single batch): 0.144674\n",
      "Epoch 150/1000 train loss: [array(0.14467448, dtype=float32)], val loss: 0.14474496245384216\n",
      "\t partial train loss (single batch): 0.144114\n",
      "Epoch 151/1000 train loss: [array(0.14411396, dtype=float32)], val loss: 0.1445363312959671\n",
      "\t partial train loss (single batch): 0.143851\n",
      "Epoch 152/1000 train loss: [array(0.14385091, dtype=float32)], val loss: 0.14461489021778107\n",
      "\t partial train loss (single batch): 0.143787\n",
      "Epoch 153/1000 train loss: [array(0.14378692, dtype=float32)], val loss: 0.1449751853942871\n",
      "\t partial train loss (single batch): 0.144206\n",
      "Epoch 154/1000 train loss: [array(0.14420591, dtype=float32)], val loss: 0.14510826766490936\n",
      "\t partial train loss (single batch): 0.143900\n",
      "Epoch 155/1000 train loss: [array(0.14390047, dtype=float32)], val loss: 0.1455378532409668\n",
      "\t partial train loss (single batch): 0.143093\n",
      "Epoch 156/1000 train loss: [array(0.14309257, dtype=float32)], val loss: 0.14382189512252808\n",
      "\t partial train loss (single batch): 0.142911\n",
      "Epoch 157/1000 train loss: [array(0.1429114, dtype=float32)], val loss: 0.14422453939914703\n",
      "\t partial train loss (single batch): 0.143570\n",
      "Epoch 158/1000 train loss: [array(0.14356995, dtype=float32)], val loss: 0.14448675513267517\n",
      "\t partial train loss (single batch): 0.143193\n",
      "Epoch 159/1000 train loss: [array(0.14319266, dtype=float32)], val loss: 0.1438220888376236\n",
      "\t partial train loss (single batch): 0.143615\n",
      "Epoch 160/1000 train loss: [array(0.14361499, dtype=float32)], val loss: 0.1434224396944046\n",
      "\t partial train loss (single batch): 0.142537\n",
      "Epoch 161/1000 train loss: [array(0.14253663, dtype=float32)], val loss: 0.14293310046195984\n",
      "\t partial train loss (single batch): 0.143033\n",
      "Epoch 162/1000 train loss: [array(0.14303333, dtype=float32)], val loss: 0.14397957921028137\n",
      "\t partial train loss (single batch): 0.142954\n",
      "Epoch 163/1000 train loss: [array(0.14295402, dtype=float32)], val loss: 0.14359529316425323\n",
      "\t partial train loss (single batch): 0.142728\n",
      "Epoch 164/1000 train loss: [array(0.14272766, dtype=float32)], val loss: 0.14224158227443695\n",
      "\t partial train loss (single batch): 0.142410\n",
      "Epoch 165/1000 train loss: [array(0.1424098, dtype=float32)], val loss: 0.1421925276517868\n",
      "\t partial train loss (single batch): 0.142078\n",
      "Epoch 166/1000 train loss: [array(0.14207795, dtype=float32)], val loss: 0.1434289813041687\n",
      "\t partial train loss (single batch): 0.142702\n",
      "Epoch 167/1000 train loss: [array(0.14270236, dtype=float32)], val loss: 0.14255769550800323\n",
      "\t partial train loss (single batch): 0.142764\n",
      "Epoch 168/1000 train loss: [array(0.1427638, dtype=float32)], val loss: 0.14263178408145905\n",
      "\t partial train loss (single batch): 0.142564\n",
      "Epoch 169/1000 train loss: [array(0.14256372, dtype=float32)], val loss: 0.14202837646007538\n",
      "\t partial train loss (single batch): 0.141661\n",
      "Epoch 170/1000 train loss: [array(0.14166053, dtype=float32)], val loss: 0.14296066761016846\n",
      "\t partial train loss (single batch): 0.142257\n",
      "Epoch 171/1000 train loss: [array(0.14225692, dtype=float32)], val loss: 0.14241105318069458\n",
      "\t partial train loss (single batch): 0.142134\n",
      "Epoch 172/1000 train loss: [array(0.1421336, dtype=float32)], val loss: 0.14236138761043549\n",
      "\t partial train loss (single batch): 0.140827\n",
      "Epoch 173/1000 train loss: [array(0.14082675, dtype=float32)], val loss: 0.14101433753967285\n",
      "\t partial train loss (single batch): 0.142239\n",
      "Epoch 174/1000 train loss: [array(0.14223926, dtype=float32)], val loss: 0.1416359543800354\n",
      "\t partial train loss (single batch): 0.140447\n",
      "Epoch 175/1000 train loss: [array(0.14044683, dtype=float32)], val loss: 0.1414862424135208\n",
      "\t partial train loss (single batch): 0.140934\n",
      "Epoch 176/1000 train loss: [array(0.14093389, dtype=float32)], val loss: 0.14172253012657166\n",
      "\t partial train loss (single batch): 0.140612\n",
      "Epoch 177/1000 train loss: [array(0.14061157, dtype=float32)], val loss: 0.14147603511810303\n",
      "\t partial train loss (single batch): 0.141464\n",
      "Epoch 178/1000 train loss: [array(0.14146428, dtype=float32)], val loss: 0.14138464629650116\n",
      "\t partial train loss (single batch): 0.141004\n",
      "Epoch 179/1000 train loss: [array(0.14100423, dtype=float32)], val loss: 0.14064417779445648\n",
      "\t partial train loss (single batch): 0.141765\n",
      "Epoch 180/1000 train loss: [array(0.14176524, dtype=float32)], val loss: 0.14101137220859528\n",
      "\t partial train loss (single batch): 0.139623\n",
      "Epoch 181/1000 train loss: [array(0.13962294, dtype=float32)], val loss: 0.14104992151260376\n",
      "\t partial train loss (single batch): 0.139617\n",
      "Epoch 182/1000 train loss: [array(0.13961689, dtype=float32)], val loss: 0.14017456769943237\n",
      "\t partial train loss (single batch): 0.140438\n",
      "Epoch 183/1000 train loss: [array(0.14043784, dtype=float32)], val loss: 0.14079447090625763\n",
      "\t partial train loss (single batch): 0.139730\n",
      "Epoch 184/1000 train loss: [array(0.13972984, dtype=float32)], val loss: 0.14059056341648102\n",
      "\t partial train loss (single batch): 0.139370\n",
      "Epoch 185/1000 train loss: [array(0.13936995, dtype=float32)], val loss: 0.13984765112400055\n",
      "\t partial train loss (single batch): 0.139133\n",
      "Epoch 186/1000 train loss: [array(0.13913268, dtype=float32)], val loss: 0.13929983973503113\n",
      "\t partial train loss (single batch): 0.139526\n",
      "Epoch 187/1000 train loss: [array(0.13952552, dtype=float32)], val loss: 0.13941872119903564\n",
      "\t partial train loss (single batch): 0.139992\n",
      "Epoch 188/1000 train loss: [array(0.13999246, dtype=float32)], val loss: 0.13938674330711365\n",
      "\t partial train loss (single batch): 0.139399\n",
      "Epoch 189/1000 train loss: [array(0.13939865, dtype=float32)], val loss: 0.1406150907278061\n",
      "\t partial train loss (single batch): 0.138985\n",
      "Epoch 190/1000 train loss: [array(0.13898498, dtype=float32)], val loss: 0.13952109217643738\n",
      "\t partial train loss (single batch): 0.139362\n",
      "Epoch 191/1000 train loss: [array(0.13936156, dtype=float32)], val loss: 0.14009523391723633\n",
      "\t partial train loss (single batch): 0.138865\n",
      "Epoch 192/1000 train loss: [array(0.13886507, dtype=float32)], val loss: 0.1391630619764328\n",
      "\t partial train loss (single batch): 0.138470\n",
      "Epoch 193/1000 train loss: [array(0.13846956, dtype=float32)], val loss: 0.13985618948936462\n",
      "\t partial train loss (single batch): 0.138019\n",
      "Epoch 194/1000 train loss: [array(0.1380193, dtype=float32)], val loss: 0.14001457393169403\n",
      "\t partial train loss (single batch): 0.138084\n",
      "Epoch 195/1000 train loss: [array(0.13808413, dtype=float32)], val loss: 0.13925278186798096\n",
      "\t partial train loss (single batch): 0.138291\n",
      "Epoch 196/1000 train loss: [array(0.13829082, dtype=float32)], val loss: 0.13810032606124878\n",
      "\t partial train loss (single batch): 0.138057\n",
      "Epoch 197/1000 train loss: [array(0.13805686, dtype=float32)], val loss: 0.13820019364356995\n",
      "\t partial train loss (single batch): 0.137726\n",
      "Epoch 198/1000 train loss: [array(0.13772568, dtype=float32)], val loss: 0.13859573006629944\n",
      "\t partial train loss (single batch): 0.137762\n",
      "Epoch 199/1000 train loss: [array(0.13776183, dtype=float32)], val loss: 0.1385580450296402\n",
      "\t partial train loss (single batch): 0.137178\n",
      "Epoch 200/1000 train loss: [array(0.13717802, dtype=float32)], val loss: 0.13764773309230804\n",
      "\t partial train loss (single batch): 0.137187\n",
      "Epoch 201/1000 train loss: [array(0.13718684, dtype=float32)], val loss: 0.1368984580039978\n",
      "\t partial train loss (single batch): 0.137418\n",
      "Epoch 202/1000 train loss: [array(0.13741812, dtype=float32)], val loss: 0.13731622695922852\n",
      "\t partial train loss (single batch): 0.136701\n",
      "Epoch 203/1000 train loss: [array(0.13670059, dtype=float32)], val loss: 0.13706743717193604\n",
      "\t partial train loss (single batch): 0.136156\n",
      "Epoch 204/1000 train loss: [array(0.1361562, dtype=float32)], val loss: 0.13672380149364471\n",
      "\t partial train loss (single batch): 0.137074\n",
      "Epoch 205/1000 train loss: [array(0.13707387, dtype=float32)], val loss: 0.13687390089035034\n",
      "\t partial train loss (single batch): 0.135768\n",
      "Epoch 206/1000 train loss: [array(0.13576843, dtype=float32)], val loss: 0.13565894961357117\n",
      "\t partial train loss (single batch): 0.136172\n",
      "Epoch 207/1000 train loss: [array(0.13617218, dtype=float32)], val loss: 0.13649393618106842\n",
      "\t partial train loss (single batch): 0.136392\n",
      "Epoch 208/1000 train loss: [array(0.13639234, dtype=float32)], val loss: 0.1369197964668274\n",
      "\t partial train loss (single batch): 0.135456\n",
      "Epoch 209/1000 train loss: [array(0.13545589, dtype=float32)], val loss: 0.1358916163444519\n",
      "\t partial train loss (single batch): 0.136002\n",
      "Epoch 210/1000 train loss: [array(0.13600218, dtype=float32)], val loss: 0.13508117198944092\n",
      "\t partial train loss (single batch): 0.135806\n",
      "Epoch 211/1000 train loss: [array(0.13580619, dtype=float32)], val loss: 0.13636383414268494\n",
      "\t partial train loss (single batch): 0.135956\n",
      "Epoch 212/1000 train loss: [array(0.13595636, dtype=float32)], val loss: 0.13570620119571686\n",
      "\t partial train loss (single batch): 0.134650\n",
      "Epoch 213/1000 train loss: [array(0.13465014, dtype=float32)], val loss: 0.13574200868606567\n",
      "\t partial train loss (single batch): 0.134861\n",
      "Epoch 214/1000 train loss: [array(0.1348606, dtype=float32)], val loss: 0.1358233094215393\n",
      "\t partial train loss (single batch): 0.135605\n",
      "Epoch 215/1000 train loss: [array(0.13560502, dtype=float32)], val loss: 0.13488177955150604\n",
      "\t partial train loss (single batch): 0.135225\n",
      "Epoch 216/1000 train loss: [array(0.13522542, dtype=float32)], val loss: 0.13436824083328247\n",
      "\t partial train loss (single batch): 0.134821\n",
      "Epoch 217/1000 train loss: [array(0.1348208, dtype=float32)], val loss: 0.13474492728710175\n",
      "\t partial train loss (single batch): 0.135422\n",
      "Epoch 218/1000 train loss: [array(0.13542189, dtype=float32)], val loss: 0.13451972603797913\n",
      "\t partial train loss (single batch): 0.134082\n",
      "Epoch 219/1000 train loss: [array(0.1340823, dtype=float32)], val loss: 0.13530677556991577\n",
      "\t partial train loss (single batch): 0.134272\n",
      "Epoch 220/1000 train loss: [array(0.13427223, dtype=float32)], val loss: 0.13389794528484344\n",
      "\t partial train loss (single batch): 0.134157\n",
      "Epoch 221/1000 train loss: [array(0.13415709, dtype=float32)], val loss: 0.13436564803123474\n",
      "\t partial train loss (single batch): 0.133711\n",
      "Epoch 222/1000 train loss: [array(0.1337105, dtype=float32)], val loss: 0.13383646309375763\n",
      "\t partial train loss (single batch): 0.134038\n",
      "Epoch 223/1000 train loss: [array(0.13403751, dtype=float32)], val loss: 0.13434205949306488\n",
      "\t partial train loss (single batch): 0.133696\n",
      "Epoch 224/1000 train loss: [array(0.13369556, dtype=float32)], val loss: 0.13382752239704132\n",
      "\t partial train loss (single batch): 0.133821\n",
      "Epoch 225/1000 train loss: [array(0.13382144, dtype=float32)], val loss: 0.13448205590248108\n",
      "\t partial train loss (single batch): 0.132019\n",
      "Epoch 226/1000 train loss: [array(0.1320194, dtype=float32)], val loss: 0.1330874115228653\n",
      "\t partial train loss (single batch): 0.133297\n",
      "Epoch 227/1000 train loss: [array(0.13329671, dtype=float32)], val loss: 0.13369746506214142\n",
      "\t partial train loss (single batch): 0.133256\n",
      "Epoch 228/1000 train loss: [array(0.13325576, dtype=float32)], val loss: 0.1338503509759903\n",
      "\t partial train loss (single batch): 0.133071\n",
      "Epoch 229/1000 train loss: [array(0.13307127, dtype=float32)], val loss: 0.13302551209926605\n",
      "\t partial train loss (single batch): 0.134077\n",
      "Epoch 230/1000 train loss: [array(0.13407663, dtype=float32)], val loss: 0.13219304382801056\n",
      "\t partial train loss (single batch): 0.132777\n",
      "Epoch 231/1000 train loss: [array(0.13277708, dtype=float32)], val loss: 0.13277114927768707\n",
      "\t partial train loss (single batch): 0.132744\n",
      "Epoch 232/1000 train loss: [array(0.13274361, dtype=float32)], val loss: 0.1328873336315155\n",
      "\t partial train loss (single batch): 0.131718\n",
      "Epoch 233/1000 train loss: [array(0.13171807, dtype=float32)], val loss: 0.1325175166130066\n",
      "\t partial train loss (single batch): 0.132013\n",
      "Epoch 234/1000 train loss: [array(0.1320127, dtype=float32)], val loss: 0.13292063772678375\n",
      "\t partial train loss (single batch): 0.131943\n",
      "Epoch 235/1000 train loss: [array(0.13194288, dtype=float32)], val loss: 0.132029190659523\n",
      "\t partial train loss (single batch): 0.132133\n",
      "Epoch 236/1000 train loss: [array(0.13213281, dtype=float32)], val loss: 0.13220712542533875\n",
      "\t partial train loss (single batch): 0.132256\n",
      "Epoch 237/1000 train loss: [array(0.13225563, dtype=float32)], val loss: 0.13287648558616638\n",
      "\t partial train loss (single batch): 0.131557\n",
      "Epoch 238/1000 train loss: [array(0.1315571, dtype=float32)], val loss: 0.13182960450649261\n",
      "\t partial train loss (single batch): 0.132105\n",
      "Epoch 239/1000 train loss: [array(0.13210455, dtype=float32)], val loss: 0.1318819671869278\n",
      "\t partial train loss (single batch): 0.131038\n",
      "Epoch 240/1000 train loss: [array(0.13103835, dtype=float32)], val loss: 0.13207069039344788\n",
      "\t partial train loss (single batch): 0.131756\n",
      "Epoch 241/1000 train loss: [array(0.1317562, dtype=float32)], val loss: 0.13129135966300964\n",
      "\t partial train loss (single batch): 0.131169\n",
      "Epoch 242/1000 train loss: [array(0.13116875, dtype=float32)], val loss: 0.1314902901649475\n",
      "\t partial train loss (single batch): 0.130935\n",
      "Epoch 243/1000 train loss: [array(0.1309345, dtype=float32)], val loss: 0.131538525223732\n",
      "\t partial train loss (single batch): 0.130703\n",
      "Epoch 244/1000 train loss: [array(0.13070261, dtype=float32)], val loss: 0.13080322742462158\n",
      "\t partial train loss (single batch): 0.131323\n",
      "Epoch 245/1000 train loss: [array(0.13132347, dtype=float32)], val loss: 0.1318213790655136\n",
      "\t partial train loss (single batch): 0.130919\n",
      "Epoch 246/1000 train loss: [array(0.13091859, dtype=float32)], val loss: 0.13082382082939148\n",
      "\t partial train loss (single batch): 0.130653\n",
      "Epoch 247/1000 train loss: [array(0.13065273, dtype=float32)], val loss: 0.1309058964252472\n",
      "\t partial train loss (single batch): 0.130810\n",
      "Epoch 248/1000 train loss: [array(0.1308101, dtype=float32)], val loss: 0.13108520209789276\n",
      "\t partial train loss (single batch): 0.130690\n",
      "Epoch 249/1000 train loss: [array(0.13069028, dtype=float32)], val loss: 0.13099384307861328\n",
      "\t partial train loss (single batch): 0.129881\n",
      "Epoch 250/1000 train loss: [array(0.12988126, dtype=float32)], val loss: 0.13067953288555145\n",
      "\t partial train loss (single batch): 0.131153\n",
      "Epoch 251/1000 train loss: [array(0.13115275, dtype=float32)], val loss: 0.13000206649303436\n",
      "\t partial train loss (single batch): 0.129904\n",
      "Epoch 252/1000 train loss: [array(0.12990384, dtype=float32)], val loss: 0.13047251105308533\n",
      "\t partial train loss (single batch): 0.130158\n",
      "Epoch 253/1000 train loss: [array(0.13015813, dtype=float32)], val loss: 0.13072775304317474\n",
      "\t partial train loss (single batch): 0.129804\n",
      "Epoch 254/1000 train loss: [array(0.12980393, dtype=float32)], val loss: 0.13064421713352203\n",
      "\t partial train loss (single batch): 0.129614\n",
      "Epoch 255/1000 train loss: [array(0.12961443, dtype=float32)], val loss: 0.129912868142128\n",
      "\t partial train loss (single batch): 0.130090\n",
      "Epoch 256/1000 train loss: [array(0.13008997, dtype=float32)], val loss: 0.12867426872253418\n",
      "\t partial train loss (single batch): 0.129646\n",
      "Epoch 257/1000 train loss: [array(0.12964572, dtype=float32)], val loss: 0.12948372960090637\n",
      "\t partial train loss (single batch): 0.128732\n",
      "Epoch 258/1000 train loss: [array(0.12873176, dtype=float32)], val loss: 0.13009017705917358\n",
      "\t partial train loss (single batch): 0.129976\n",
      "Epoch 259/1000 train loss: [array(0.12997644, dtype=float32)], val loss: 0.12974433600902557\n",
      "\t partial train loss (single batch): 0.128533\n",
      "Epoch 260/1000 train loss: [array(0.12853257, dtype=float32)], val loss: 0.12959831953048706\n",
      "\t partial train loss (single batch): 0.129784\n",
      "Epoch 261/1000 train loss: [array(0.12978378, dtype=float32)], val loss: 0.12919466197490692\n",
      "\t partial train loss (single batch): 0.129368\n",
      "Epoch 262/1000 train loss: [array(0.12936771, dtype=float32)], val loss: 0.1290617138147354\n",
      "\t partial train loss (single batch): 0.129334\n",
      "Epoch 263/1000 train loss: [array(0.12933445, dtype=float32)], val loss: 0.12866170704364777\n",
      "\t partial train loss (single batch): 0.128629\n",
      "Epoch 264/1000 train loss: [array(0.12862863, dtype=float32)], val loss: 0.12955887615680695\n",
      "\t partial train loss (single batch): 0.129315\n",
      "Epoch 265/1000 train loss: [array(0.12931456, dtype=float32)], val loss: 0.12871938943862915\n",
      "\t partial train loss (single batch): 0.129163\n",
      "Epoch 266/1000 train loss: [array(0.12916319, dtype=float32)], val loss: 0.1286611258983612\n",
      "\t partial train loss (single batch): 0.129321\n",
      "Epoch 267/1000 train loss: [array(0.12932104, dtype=float32)], val loss: 0.12875285744667053\n",
      "\t partial train loss (single batch): 0.129264\n",
      "Epoch 268/1000 train loss: [array(0.12926362, dtype=float32)], val loss: 0.12938696146011353\n",
      "\t partial train loss (single batch): 0.128990\n",
      "Epoch 269/1000 train loss: [array(0.12898956, dtype=float32)], val loss: 0.12910839915275574\n",
      "\t partial train loss (single batch): 0.128436\n",
      "Epoch 270/1000 train loss: [array(0.12843622, dtype=float32)], val loss: 0.12952764332294464\n",
      "\t partial train loss (single batch): 0.128015\n",
      "Epoch 271/1000 train loss: [array(0.12801507, dtype=float32)], val loss: 0.12817123532295227\n",
      "\t partial train loss (single batch): 0.127912\n",
      "Epoch 272/1000 train loss: [array(0.12791173, dtype=float32)], val loss: 0.12852631509304047\n",
      "\t partial train loss (single batch): 0.129281\n",
      "Epoch 273/1000 train loss: [array(0.12928055, dtype=float32)], val loss: 0.1276075392961502\n",
      "\t partial train loss (single batch): 0.128340\n",
      "Epoch 274/1000 train loss: [array(0.12833962, dtype=float32)], val loss: 0.1280195564031601\n",
      "\t partial train loss (single batch): 0.128078\n",
      "Epoch 275/1000 train loss: [array(0.12807776, dtype=float32)], val loss: 0.1281147599220276\n",
      "\t partial train loss (single batch): 0.127466\n",
      "Epoch 276/1000 train loss: [array(0.12746584, dtype=float32)], val loss: 0.12801891565322876\n",
      "\t partial train loss (single batch): 0.127889\n",
      "Epoch 277/1000 train loss: [array(0.12788919, dtype=float32)], val loss: 0.12794417142868042\n",
      "\t partial train loss (single batch): 0.127058\n",
      "Epoch 278/1000 train loss: [array(0.12705816, dtype=float32)], val loss: 0.1287086457014084\n",
      "\t partial train loss (single batch): 0.126887\n",
      "Epoch 279/1000 train loss: [array(0.12688726, dtype=float32)], val loss: 0.1273660808801651\n",
      "\t partial train loss (single batch): 0.126983\n",
      "Epoch 280/1000 train loss: [array(0.12698323, dtype=float32)], val loss: 0.1272304505109787\n",
      "\t partial train loss (single batch): 0.128435\n",
      "Epoch 281/1000 train loss: [array(0.12843452, dtype=float32)], val loss: 0.12711572647094727\n",
      "\t partial train loss (single batch): 0.127070\n",
      "Epoch 282/1000 train loss: [array(0.12707011, dtype=float32)], val loss: 0.1272592544555664\n",
      "\t partial train loss (single batch): 0.126634\n",
      "Epoch 283/1000 train loss: [array(0.12663351, dtype=float32)], val loss: 0.12720508873462677\n",
      "\t partial train loss (single batch): 0.127076\n",
      "Epoch 284/1000 train loss: [array(0.12707576, dtype=float32)], val loss: 0.12761056423187256\n",
      "\t partial train loss (single batch): 0.126289\n",
      "Epoch 285/1000 train loss: [array(0.12628892, dtype=float32)], val loss: 0.12684331834316254\n",
      "\t partial train loss (single batch): 0.126642\n",
      "Epoch 286/1000 train loss: [array(0.12664172, dtype=float32)], val loss: 0.12657630443572998\n",
      "\t partial train loss (single batch): 0.126456\n",
      "Epoch 287/1000 train loss: [array(0.12645607, dtype=float32)], val loss: 0.127472385764122\n",
      "\t partial train loss (single batch): 0.127207\n",
      "Epoch 288/1000 train loss: [array(0.12720746, dtype=float32)], val loss: 0.12745919823646545\n",
      "\t partial train loss (single batch): 0.127174\n",
      "Epoch 289/1000 train loss: [array(0.12717406, dtype=float32)], val loss: 0.12736791372299194\n",
      "\t partial train loss (single batch): 0.126259\n",
      "Epoch 290/1000 train loss: [array(0.12625882, dtype=float32)], val loss: 0.12740130722522736\n",
      "\t partial train loss (single batch): 0.126851\n",
      "Epoch 291/1000 train loss: [array(0.12685062, dtype=float32)], val loss: 0.12700079381465912\n",
      "\t partial train loss (single batch): 0.126027\n",
      "Epoch 292/1000 train loss: [array(0.1260271, dtype=float32)], val loss: 0.12706580758094788\n",
      "\t partial train loss (single batch): 0.126233\n",
      "Epoch 293/1000 train loss: [array(0.12623274, dtype=float32)], val loss: 0.1272883266210556\n",
      "\t partial train loss (single batch): 0.125632\n",
      "Epoch 294/1000 train loss: [array(0.1256323, dtype=float32)], val loss: 0.12579262256622314\n",
      "\t partial train loss (single batch): 0.126006\n",
      "Epoch 295/1000 train loss: [array(0.12600562, dtype=float32)], val loss: 0.12622171640396118\n",
      "\t partial train loss (single batch): 0.125974\n",
      "Epoch 296/1000 train loss: [array(0.12597387, dtype=float32)], val loss: 0.12694048881530762\n",
      "\t partial train loss (single batch): 0.125647\n",
      "Epoch 297/1000 train loss: [array(0.12564696, dtype=float32)], val loss: 0.12613412737846375\n",
      "\t partial train loss (single batch): 0.126225\n",
      "Epoch 298/1000 train loss: [array(0.12622508, dtype=float32)], val loss: 0.12588422000408173\n",
      "\t partial train loss (single batch): 0.126345\n",
      "Epoch 299/1000 train loss: [array(0.12634508, dtype=float32)], val loss: 0.12573041021823883\n",
      "\t partial train loss (single batch): 0.126376\n",
      "Epoch 300/1000 train loss: [array(0.1263763, dtype=float32)], val loss: 0.12647123634815216\n",
      "\t partial train loss (single batch): 0.125529\n",
      "Epoch 301/1000 train loss: [array(0.12552863, dtype=float32)], val loss: 0.12681008875370026\n",
      "\t partial train loss (single batch): 0.125483\n",
      "Epoch 302/1000 train loss: [array(0.12548256, dtype=float32)], val loss: 0.12611359357833862\n",
      "\t partial train loss (single batch): 0.126042\n",
      "Epoch 303/1000 train loss: [array(0.1260421, dtype=float32)], val loss: 0.12608075141906738\n",
      "\t partial train loss (single batch): 0.126049\n",
      "Epoch 304/1000 train loss: [array(0.12604858, dtype=float32)], val loss: 0.12581497430801392\n",
      "\t partial train loss (single batch): 0.124954\n",
      "Epoch 305/1000 train loss: [array(0.12495408, dtype=float32)], val loss: 0.12616674602031708\n",
      "\t partial train loss (single batch): 0.125163\n",
      "Epoch 306/1000 train loss: [array(0.12516324, dtype=float32)], val loss: 0.1251489818096161\n",
      "\t partial train loss (single batch): 0.126334\n",
      "Epoch 307/1000 train loss: [array(0.12633358, dtype=float32)], val loss: 0.1263325959444046\n",
      "\t partial train loss (single batch): 0.125510\n",
      "Epoch 308/1000 train loss: [array(0.12550966, dtype=float32)], val loss: 0.12540431320667267\n",
      "\t partial train loss (single batch): 0.125158\n",
      "Epoch 309/1000 train loss: [array(0.1251579, dtype=float32)], val loss: 0.12515679001808167\n",
      "\t partial train loss (single batch): 0.125487\n",
      "Epoch 310/1000 train loss: [array(0.12548703, dtype=float32)], val loss: 0.12519027292728424\n",
      "\t partial train loss (single batch): 0.124934\n",
      "Epoch 311/1000 train loss: [array(0.12493357, dtype=float32)], val loss: 0.12528324127197266\n",
      "\t partial train loss (single batch): 0.124711\n",
      "Epoch 312/1000 train loss: [array(0.12471126, dtype=float32)], val loss: 0.1264858990907669\n",
      "\t partial train loss (single batch): 0.124520\n",
      "Epoch 313/1000 train loss: [array(0.12452022, dtype=float32)], val loss: 0.12502934038639069\n",
      "\t partial train loss (single batch): 0.125392\n",
      "Epoch 314/1000 train loss: [array(0.12539203, dtype=float32)], val loss: 0.12519899010658264\n",
      "\t partial train loss (single batch): 0.124988\n",
      "Epoch 315/1000 train loss: [array(0.12498768, dtype=float32)], val loss: 0.1256043016910553\n",
      "\t partial train loss (single batch): 0.124527\n",
      "Epoch 316/1000 train loss: [array(0.12452691, dtype=float32)], val loss: 0.1246342658996582\n",
      "\t partial train loss (single batch): 0.124914\n",
      "Epoch 317/1000 train loss: [array(0.12491433, dtype=float32)], val loss: 0.12430945038795471\n",
      "\t partial train loss (single batch): 0.124123\n",
      "Epoch 318/1000 train loss: [array(0.12412263, dtype=float32)], val loss: 0.12488631159067154\n",
      "\t partial train loss (single batch): 0.124520\n",
      "Epoch 319/1000 train loss: [array(0.1245201, dtype=float32)], val loss: 0.12522852420806885\n",
      "\t partial train loss (single batch): 0.123796\n",
      "Epoch 320/1000 train loss: [array(0.12379634, dtype=float32)], val loss: 0.12409833073616028\n",
      "\t partial train loss (single batch): 0.123210\n",
      "Epoch 321/1000 train loss: [array(0.12320993, dtype=float32)], val loss: 0.12451833486557007\n",
      "\t partial train loss (single batch): 0.123841\n",
      "Epoch 322/1000 train loss: [array(0.12384126, dtype=float32)], val loss: 0.12323188781738281\n",
      "\t partial train loss (single batch): 0.123241\n",
      "Epoch 323/1000 train loss: [array(0.12324081, dtype=float32)], val loss: 0.12408582121133804\n",
      "\t partial train loss (single batch): 0.124004\n",
      "Epoch 324/1000 train loss: [array(0.12400382, dtype=float32)], val loss: 0.1242247074842453\n",
      "\t partial train loss (single batch): 0.123711\n",
      "Epoch 325/1000 train loss: [array(0.12371121, dtype=float32)], val loss: 0.12476453930139542\n",
      "\t partial train loss (single batch): 0.123719\n",
      "Epoch 326/1000 train loss: [array(0.12371909, dtype=float32)], val loss: 0.1236768588423729\n",
      "\t partial train loss (single batch): 0.122639\n",
      "Epoch 327/1000 train loss: [array(0.12263863, dtype=float32)], val loss: 0.12422420084476471\n",
      "\t partial train loss (single batch): 0.123285\n",
      "Epoch 328/1000 train loss: [array(0.12328497, dtype=float32)], val loss: 0.12324849516153336\n",
      "\t partial train loss (single batch): 0.123435\n",
      "Epoch 329/1000 train loss: [array(0.12343521, dtype=float32)], val loss: 0.12294281274080276\n",
      "\t partial train loss (single batch): 0.123194\n",
      "Epoch 330/1000 train loss: [array(0.12319418, dtype=float32)], val loss: 0.12343750149011612\n",
      "\t partial train loss (single batch): 0.122894\n",
      "Epoch 331/1000 train loss: [array(0.12289376, dtype=float32)], val loss: 0.12331561744213104\n",
      "\t partial train loss (single batch): 0.123102\n",
      "Epoch 332/1000 train loss: [array(0.12310196, dtype=float32)], val loss: 0.1227622777223587\n",
      "\t partial train loss (single batch): 0.123535\n",
      "Epoch 333/1000 train loss: [array(0.12353468, dtype=float32)], val loss: 0.12388040870428085\n",
      "\t partial train loss (single batch): 0.123991\n",
      "Epoch 334/1000 train loss: [array(0.12399117, dtype=float32)], val loss: 0.12291107326745987\n",
      "\t partial train loss (single batch): 0.122804\n",
      "Epoch 335/1000 train loss: [array(0.12280375, dtype=float32)], val loss: 0.12340869754552841\n",
      "\t partial train loss (single batch): 0.123727\n",
      "Epoch 336/1000 train loss: [array(0.12372725, dtype=float32)], val loss: 0.12326132506132126\n",
      "\t partial train loss (single batch): 0.122590\n",
      "Epoch 337/1000 train loss: [array(0.12259018, dtype=float32)], val loss: 0.12303977459669113\n",
      "\t partial train loss (single batch): 0.122487\n",
      "Epoch 338/1000 train loss: [array(0.12248728, dtype=float32)], val loss: 0.12418610602617264\n",
      "\t partial train loss (single batch): 0.122759\n",
      "Epoch 339/1000 train loss: [array(0.12275913, dtype=float32)], val loss: 0.12355270236730576\n",
      "\t partial train loss (single batch): 0.123089\n",
      "Epoch 340/1000 train loss: [array(0.12308942, dtype=float32)], val loss: 0.12393001466989517\n",
      "\t partial train loss (single batch): 0.123235\n",
      "Epoch 341/1000 train loss: [array(0.12323529, dtype=float32)], val loss: 0.12278882414102554\n",
      "\t partial train loss (single batch): 0.122697\n",
      "Epoch 342/1000 train loss: [array(0.12269729, dtype=float32)], val loss: 0.1225716695189476\n",
      "\t partial train loss (single batch): 0.122228\n",
      "Epoch 343/1000 train loss: [array(0.12222768, dtype=float32)], val loss: 0.1227848008275032\n",
      "\t partial train loss (single batch): 0.122161\n",
      "Epoch 344/1000 train loss: [array(0.12216111, dtype=float32)], val loss: 0.12275266647338867\n",
      "\t partial train loss (single batch): 0.123054\n",
      "Epoch 345/1000 train loss: [array(0.1230536, dtype=float32)], val loss: 0.12191440165042877\n",
      "\t partial train loss (single batch): 0.121532\n",
      "Epoch 346/1000 train loss: [array(0.12153205, dtype=float32)], val loss: 0.12272202968597412\n",
      "\t partial train loss (single batch): 0.122082\n",
      "Epoch 347/1000 train loss: [array(0.12208249, dtype=float32)], val loss: 0.12266834080219269\n",
      "\t partial train loss (single batch): 0.121748\n",
      "Epoch 348/1000 train loss: [array(0.12174809, dtype=float32)], val loss: 0.1227327436208725\n",
      "\t partial train loss (single batch): 0.123543\n",
      "Epoch 349/1000 train loss: [array(0.12354288, dtype=float32)], val loss: 0.12301743775606155\n",
      "\t partial train loss (single batch): 0.121662\n",
      "Epoch 350/1000 train loss: [array(0.12166235, dtype=float32)], val loss: 0.12264741212129593\n",
      "\t partial train loss (single batch): 0.122513\n",
      "Epoch 351/1000 train loss: [array(0.12251296, dtype=float32)], val loss: 0.12215743958950043\n",
      "\t partial train loss (single batch): 0.122075\n",
      "Epoch 352/1000 train loss: [array(0.12207519, dtype=float32)], val loss: 0.12143875658512115\n",
      "\t partial train loss (single batch): 0.121262\n",
      "Epoch 353/1000 train loss: [array(0.12126196, dtype=float32)], val loss: 0.122981496155262\n",
      "\t partial train loss (single batch): 0.121574\n",
      "Epoch 354/1000 train loss: [array(0.12157381, dtype=float32)], val loss: 0.12166336178779602\n",
      "\t partial train loss (single batch): 0.121483\n",
      "Epoch 355/1000 train loss: [array(0.12148285, dtype=float32)], val loss: 0.12169068306684494\n",
      "\t partial train loss (single batch): 0.121111\n",
      "Epoch 356/1000 train loss: [array(0.12111074, dtype=float32)], val loss: 0.12259375303983688\n",
      "\t partial train loss (single batch): 0.121633\n",
      "Epoch 357/1000 train loss: [array(0.12163305, dtype=float32)], val loss: 0.1214054524898529\n",
      "\t partial train loss (single batch): 0.121725\n",
      "Epoch 358/1000 train loss: [array(0.12172545, dtype=float32)], val loss: 0.12187469750642776\n",
      "\t partial train loss (single batch): 0.120528\n",
      "Epoch 359/1000 train loss: [array(0.12052818, dtype=float32)], val loss: 0.12059668451547623\n",
      "\t partial train loss (single batch): 0.121526\n",
      "Epoch 360/1000 train loss: [array(0.12152577, dtype=float32)], val loss: 0.1218409389257431\n",
      "\t partial train loss (single batch): 0.120729\n",
      "Epoch 361/1000 train loss: [array(0.1207288, dtype=float32)], val loss: 0.12189104408025742\n",
      "\t partial train loss (single batch): 0.122117\n",
      "Epoch 362/1000 train loss: [array(0.12211671, dtype=float32)], val loss: 0.12222106009721756\n",
      "\t partial train loss (single batch): 0.121116\n",
      "Epoch 363/1000 train loss: [array(0.12111609, dtype=float32)], val loss: 0.12234320491552353\n",
      "\t partial train loss (single batch): 0.120536\n",
      "Epoch 364/1000 train loss: [array(0.12053563, dtype=float32)], val loss: 0.1216704472899437\n",
      "\t partial train loss (single batch): 0.121782\n",
      "Epoch 365/1000 train loss: [array(0.12178232, dtype=float32)], val loss: 0.12086686491966248\n",
      "\t partial train loss (single batch): 0.120247\n",
      "Epoch 366/1000 train loss: [array(0.12024666, dtype=float32)], val loss: 0.1215265616774559\n",
      "\t partial train loss (single batch): 0.120496\n",
      "Epoch 367/1000 train loss: [array(0.12049577, dtype=float32)], val loss: 0.12078574299812317\n",
      "\t partial train loss (single batch): 0.120077\n",
      "Epoch 368/1000 train loss: [array(0.12007727, dtype=float32)], val loss: 0.12128119170665741\n",
      "\t partial train loss (single batch): 0.121760\n",
      "Epoch 369/1000 train loss: [array(0.12176026, dtype=float32)], val loss: 0.12055032700300217\n",
      "\t partial train loss (single batch): 0.120303\n",
      "Epoch 370/1000 train loss: [array(0.12030343, dtype=float32)], val loss: 0.12145824730396271\n",
      "\t partial train loss (single batch): 0.120158\n",
      "Epoch 371/1000 train loss: [array(0.12015814, dtype=float32)], val loss: 0.12119590491056442\n",
      "\t partial train loss (single batch): 0.120037\n",
      "Epoch 372/1000 train loss: [array(0.12003726, dtype=float32)], val loss: 0.12146930396556854\n",
      "\t partial train loss (single batch): 0.120328\n",
      "Epoch 373/1000 train loss: [array(0.12032757, dtype=float32)], val loss: 0.11982689052820206\n",
      "\t partial train loss (single batch): 0.120493\n",
      "Epoch 374/1000 train loss: [array(0.12049309, dtype=float32)], val loss: 0.12144583463668823\n",
      "\t partial train loss (single batch): 0.120797\n",
      "Epoch 375/1000 train loss: [array(0.12079737, dtype=float32)], val loss: 0.12055785208940506\n",
      "\t partial train loss (single batch): 0.119880\n",
      "Epoch 376/1000 train loss: [array(0.11987963, dtype=float32)], val loss: 0.12091615796089172\n",
      "\t partial train loss (single batch): 0.119836\n",
      "Epoch 377/1000 train loss: [array(0.11983582, dtype=float32)], val loss: 0.12020018696784973\n",
      "\t partial train loss (single batch): 0.120055\n",
      "Epoch 378/1000 train loss: [array(0.1200554, dtype=float32)], val loss: 0.12075478583574295\n",
      "\t partial train loss (single batch): 0.120163\n",
      "Epoch 379/1000 train loss: [array(0.12016329, dtype=float32)], val loss: 0.12050427496433258\n",
      "\t partial train loss (single batch): 0.119207\n",
      "Epoch 380/1000 train loss: [array(0.11920742, dtype=float32)], val loss: 0.11973177641630173\n",
      "\t partial train loss (single batch): 0.119208\n",
      "Epoch 381/1000 train loss: [array(0.11920836, dtype=float32)], val loss: 0.11992771178483963\n",
      "\t partial train loss (single batch): 0.120102\n",
      "Epoch 382/1000 train loss: [array(0.12010226, dtype=float32)], val loss: 0.12047223001718521\n",
      "\t partial train loss (single batch): 0.119446\n",
      "Epoch 383/1000 train loss: [array(0.11944591, dtype=float32)], val loss: 0.12089043855667114\n",
      "\t partial train loss (single batch): 0.119331\n",
      "Epoch 384/1000 train loss: [array(0.11933105, dtype=float32)], val loss: 0.11921682953834534\n",
      "\t partial train loss (single batch): 0.119679\n",
      "Epoch 385/1000 train loss: [array(0.11967918, dtype=float32)], val loss: 0.12073258310556412\n",
      "\t partial train loss (single batch): 0.119527\n",
      "Epoch 386/1000 train loss: [array(0.11952721, dtype=float32)], val loss: 0.12042563408613205\n",
      "\t partial train loss (single batch): 0.119308\n",
      "Epoch 387/1000 train loss: [array(0.11930795, dtype=float32)], val loss: 0.11984676867723465\n",
      "\t partial train loss (single batch): 0.119861\n",
      "Epoch 388/1000 train loss: [array(0.11986145, dtype=float32)], val loss: 0.11918246001005173\n",
      "\t partial train loss (single batch): 0.119951\n",
      "Epoch 389/1000 train loss: [array(0.11995092, dtype=float32)], val loss: 0.12043217569589615\n",
      "\t partial train loss (single batch): 0.119786\n",
      "Epoch 390/1000 train loss: [array(0.11978554, dtype=float32)], val loss: 0.12040971219539642\n",
      "\t partial train loss (single batch): 0.119656\n",
      "Epoch 391/1000 train loss: [array(0.11965613, dtype=float32)], val loss: 0.11917830258607864\n",
      "\t partial train loss (single batch): 0.119206\n",
      "Epoch 392/1000 train loss: [array(0.11920649, dtype=float32)], val loss: 0.1195446327328682\n",
      "\t partial train loss (single batch): 0.119062\n",
      "Epoch 393/1000 train loss: [array(0.11906178, dtype=float32)], val loss: 0.11945561319589615\n",
      "\t partial train loss (single batch): 0.119161\n",
      "Epoch 394/1000 train loss: [array(0.1191607, dtype=float32)], val loss: 0.1199490949511528\n",
      "\t partial train loss (single batch): 0.118256\n",
      "Epoch 395/1000 train loss: [array(0.11825635, dtype=float32)], val loss: 0.11920524388551712\n",
      "\t partial train loss (single batch): 0.118137\n",
      "Epoch 396/1000 train loss: [array(0.11813749, dtype=float32)], val loss: 0.1200137510895729\n",
      "\t partial train loss (single batch): 0.119258\n",
      "Epoch 397/1000 train loss: [array(0.1192575, dtype=float32)], val loss: 0.11985339969396591\n",
      "\t partial train loss (single batch): 0.118457\n",
      "Epoch 398/1000 train loss: [array(0.11845697, dtype=float32)], val loss: 0.11832574754953384\n",
      "\t partial train loss (single batch): 0.119265\n",
      "Epoch 399/1000 train loss: [array(0.11926523, dtype=float32)], val loss: 0.11825226992368698\n",
      "\t partial train loss (single batch): 0.118515\n",
      "Epoch 400/1000 train loss: [array(0.11851549, dtype=float32)], val loss: 0.11900635808706284\n",
      "\t partial train loss (single batch): 0.119373\n",
      "Epoch 401/1000 train loss: [array(0.1193725, dtype=float32)], val loss: 0.11894652992486954\n",
      "\t partial train loss (single batch): 0.119266\n",
      "Epoch 402/1000 train loss: [array(0.11926588, dtype=float32)], val loss: 0.11954983323812485\n",
      "\t partial train loss (single batch): 0.116903\n",
      "Epoch 403/1000 train loss: [array(0.11690313, dtype=float32)], val loss: 0.11817135661840439\n",
      "\t partial train loss (single batch): 0.119550\n",
      "Epoch 404/1000 train loss: [array(0.11955013, dtype=float32)], val loss: 0.11982739716768265\n",
      "\t partial train loss (single batch): 0.118679\n",
      "Epoch 405/1000 train loss: [array(0.11867928, dtype=float32)], val loss: 0.12013496458530426\n",
      "\t partial train loss (single batch): 0.118555\n",
      "Epoch 406/1000 train loss: [array(0.11855511, dtype=float32)], val loss: 0.11959889531135559\n",
      "\t partial train loss (single batch): 0.118784\n",
      "Epoch 407/1000 train loss: [array(0.11878448, dtype=float32)], val loss: 0.11840176582336426\n",
      "\t partial train loss (single batch): 0.117615\n",
      "Epoch 408/1000 train loss: [array(0.11761506, dtype=float32)], val loss: 0.11915518343448639\n",
      "\t partial train loss (single batch): 0.118378\n",
      "Epoch 409/1000 train loss: [array(0.11837826, dtype=float32)], val loss: 0.11909759789705276\n",
      "\t partial train loss (single batch): 0.118584\n",
      "Epoch 410/1000 train loss: [array(0.11858351, dtype=float32)], val loss: 0.1186780110001564\n",
      "\t partial train loss (single batch): 0.118435\n",
      "Epoch 411/1000 train loss: [array(0.11843476, dtype=float32)], val loss: 0.11849372088909149\n",
      "\t partial train loss (single batch): 0.118417\n",
      "Epoch 412/1000 train loss: [array(0.11841749, dtype=float32)], val loss: 0.11770401149988174\n",
      "\t partial train loss (single batch): 0.118165\n",
      "Epoch 413/1000 train loss: [array(0.11816462, dtype=float32)], val loss: 0.11852051317691803\n",
      "\t partial train loss (single batch): 0.118223\n",
      "Epoch 414/1000 train loss: [array(0.11822303, dtype=float32)], val loss: 0.1180412620306015\n",
      "\t partial train loss (single batch): 0.117644\n",
      "Epoch 415/1000 train loss: [array(0.11764356, dtype=float32)], val loss: 0.11818165332078934\n",
      "\t partial train loss (single batch): 0.117561\n",
      "Epoch 416/1000 train loss: [array(0.11756054, dtype=float32)], val loss: 0.11834227293729782\n",
      "\t partial train loss (single batch): 0.117320\n",
      "Epoch 417/1000 train loss: [array(0.11731981, dtype=float32)], val loss: 0.11846589297056198\n",
      "\t partial train loss (single batch): 0.117906\n",
      "Epoch 418/1000 train loss: [array(0.11790571, dtype=float32)], val loss: 0.11803290992975235\n",
      "\t partial train loss (single batch): 0.118097\n",
      "Epoch 419/1000 train loss: [array(0.11809672, dtype=float32)], val loss: 0.11776053160429001\n",
      "\t partial train loss (single batch): 0.117120\n",
      "Epoch 420/1000 train loss: [array(0.11712025, dtype=float32)], val loss: 0.11771108955144882\n",
      "\t partial train loss (single batch): 0.118478\n",
      "Epoch 421/1000 train loss: [array(0.11847849, dtype=float32)], val loss: 0.11850738525390625\n",
      "\t partial train loss (single batch): 0.116458\n",
      "Epoch 422/1000 train loss: [array(0.11645754, dtype=float32)], val loss: 0.1175365075469017\n",
      "\t partial train loss (single batch): 0.116684\n",
      "Epoch 423/1000 train loss: [array(0.11668389, dtype=float32)], val loss: 0.11699391156435013\n",
      "\t partial train loss (single batch): 0.117791\n",
      "Epoch 424/1000 train loss: [array(0.11779095, dtype=float32)], val loss: 0.11840993165969849\n",
      "\t partial train loss (single batch): 0.117189\n",
      "Epoch 425/1000 train loss: [array(0.11718894, dtype=float32)], val loss: 0.11798897385597229\n",
      "\t partial train loss (single batch): 0.117561\n",
      "Epoch 426/1000 train loss: [array(0.11756089, dtype=float32)], val loss: 0.11621732264757156\n",
      "\t partial train loss (single batch): 0.116973\n",
      "Epoch 427/1000 train loss: [array(0.11697332, dtype=float32)], val loss: 0.11711272597312927\n",
      "\t partial train loss (single batch): 0.117031\n",
      "Epoch 428/1000 train loss: [array(0.11703093, dtype=float32)], val loss: 0.1181584820151329\n",
      "\t partial train loss (single batch): 0.116621\n",
      "Epoch 429/1000 train loss: [array(0.11662111, dtype=float32)], val loss: 0.11805299669504166\n",
      "\t partial train loss (single batch): 0.117420\n",
      "Epoch 430/1000 train loss: [array(0.11741967, dtype=float32)], val loss: 0.11812308430671692\n",
      "\t partial train loss (single batch): 0.117680\n",
      "Epoch 431/1000 train loss: [array(0.11767983, dtype=float32)], val loss: 0.11696430295705795\n",
      "\t partial train loss (single batch): 0.117297\n",
      "Epoch 432/1000 train loss: [array(0.11729728, dtype=float32)], val loss: 0.11707604676485062\n",
      "\t partial train loss (single batch): 0.117198\n",
      "Epoch 433/1000 train loss: [array(0.11719804, dtype=float32)], val loss: 0.11715880036354065\n",
      "\t partial train loss (single batch): 0.117064\n",
      "Epoch 434/1000 train loss: [array(0.11706356, dtype=float32)], val loss: 0.11731762439012527\n",
      "\t partial train loss (single batch): 0.116878\n",
      "Epoch 435/1000 train loss: [array(0.11687756, dtype=float32)], val loss: 0.11708590388298035\n",
      "\t partial train loss (single batch): 0.116731\n",
      "Epoch 436/1000 train loss: [array(0.11673106, dtype=float32)], val loss: 0.11646068096160889\n",
      "\t partial train loss (single batch): 0.116519\n",
      "Epoch 437/1000 train loss: [array(0.11651874, dtype=float32)], val loss: 0.1176566556096077\n",
      "\t partial train loss (single batch): 0.116197\n",
      "Epoch 438/1000 train loss: [array(0.11619692, dtype=float32)], val loss: 0.11701260507106781\n",
      "\t partial train loss (single batch): 0.116419\n",
      "Epoch 439/1000 train loss: [array(0.11641917, dtype=float32)], val loss: 0.11683971434831619\n",
      "\t partial train loss (single batch): 0.117324\n",
      "Epoch 440/1000 train loss: [array(0.11732356, dtype=float32)], val loss: 0.11638597398996353\n",
      "\t partial train loss (single batch): 0.116404\n",
      "Epoch 441/1000 train loss: [array(0.11640433, dtype=float32)], val loss: 0.1174946203827858\n",
      "\t partial train loss (single batch): 0.115992\n",
      "Epoch 442/1000 train loss: [array(0.11599247, dtype=float32)], val loss: 0.11685460805892944\n",
      "\t partial train loss (single batch): 0.114977\n",
      "Epoch 443/1000 train loss: [array(0.11497661, dtype=float32)], val loss: 0.11699031293392181\n",
      "\t partial train loss (single batch): 0.115614\n",
      "Epoch 444/1000 train loss: [array(0.11561368, dtype=float32)], val loss: 0.11602292954921722\n",
      "\t partial train loss (single batch): 0.116663\n",
      "Epoch 445/1000 train loss: [array(0.11666296, dtype=float32)], val loss: 0.11709820479154587\n",
      "\t partial train loss (single batch): 0.116859\n",
      "Epoch 446/1000 train loss: [array(0.11685941, dtype=float32)], val loss: 0.11565778404474258\n",
      "\t partial train loss (single batch): 0.116721\n",
      "Epoch 447/1000 train loss: [array(0.11672091, dtype=float32)], val loss: 0.1157313659787178\n",
      "\t partial train loss (single batch): 0.115816\n",
      "Epoch 448/1000 train loss: [array(0.11581644, dtype=float32)], val loss: 0.11646571755409241\n",
      "\t partial train loss (single batch): 0.115549\n",
      "Epoch 449/1000 train loss: [array(0.11554879, dtype=float32)], val loss: 0.11636529117822647\n",
      "\t partial train loss (single batch): 0.115722\n",
      "Epoch 450/1000 train loss: [array(0.11572161, dtype=float32)], val loss: 0.11609140038490295\n",
      "\t partial train loss (single batch): 0.116656\n",
      "Epoch 451/1000 train loss: [array(0.11665604, dtype=float32)], val loss: 0.11559145152568817\n",
      "\t partial train loss (single batch): 0.115837\n",
      "Epoch 452/1000 train loss: [array(0.11583689, dtype=float32)], val loss: 0.11689719557762146\n",
      "\t partial train loss (single batch): 0.116185\n",
      "Epoch 453/1000 train loss: [array(0.11618504, dtype=float32)], val loss: 0.11549993604421616\n",
      "\t partial train loss (single batch): 0.115739\n",
      "Epoch 454/1000 train loss: [array(0.1157389, dtype=float32)], val loss: 0.116347536444664\n",
      "\t partial train loss (single batch): 0.116757\n",
      "Epoch 455/1000 train loss: [array(0.11675693, dtype=float32)], val loss: 0.11577650159597397\n",
      "\t partial train loss (single batch): 0.114861\n",
      "Epoch 456/1000 train loss: [array(0.11486069, dtype=float32)], val loss: 0.11602805554866791\n",
      "\t partial train loss (single batch): 0.115375\n",
      "Epoch 457/1000 train loss: [array(0.11537536, dtype=float32)], val loss: 0.11634504795074463\n",
      "\t partial train loss (single batch): 0.114934\n",
      "Epoch 458/1000 train loss: [array(0.11493436, dtype=float32)], val loss: 0.11547020077705383\n",
      "\t partial train loss (single batch): 0.115504\n",
      "Epoch 459/1000 train loss: [array(0.11550403, dtype=float32)], val loss: 0.1159067451953888\n",
      "\t partial train loss (single batch): 0.115047\n",
      "Epoch 460/1000 train loss: [array(0.11504743, dtype=float32)], val loss: 0.11557066440582275\n",
      "\t partial train loss (single batch): 0.115508\n",
      "Epoch 461/1000 train loss: [array(0.11550832, dtype=float32)], val loss: 0.11605837196111679\n",
      "\t partial train loss (single batch): 0.115618\n",
      "Epoch 462/1000 train loss: [array(0.11561842, dtype=float32)], val loss: 0.11660788953304291\n",
      "\t partial train loss (single batch): 0.114172\n",
      "Epoch 463/1000 train loss: [array(0.11417191, dtype=float32)], val loss: 0.11574158817529678\n",
      "\t partial train loss (single batch): 0.115974\n",
      "Epoch 464/1000 train loss: [array(0.11597409, dtype=float32)], val loss: 0.11552777886390686\n",
      "\t partial train loss (single batch): 0.114532\n",
      "Epoch 465/1000 train loss: [array(0.11453191, dtype=float32)], val loss: 0.11593016982078552\n",
      "\t partial train loss (single batch): 0.115462\n",
      "Epoch 466/1000 train loss: [array(0.11546206, dtype=float32)], val loss: 0.11508673429489136\n",
      "\t partial train loss (single batch): 0.114752\n",
      "Epoch 467/1000 train loss: [array(0.11475189, dtype=float32)], val loss: 0.11512116342782974\n",
      "\t partial train loss (single batch): 0.115010\n",
      "Epoch 468/1000 train loss: [array(0.11501023, dtype=float32)], val loss: 0.11525952070951462\n",
      "\t partial train loss (single batch): 0.115125\n",
      "Epoch 469/1000 train loss: [array(0.11512466, dtype=float32)], val loss: 0.11579536646604538\n",
      "\t partial train loss (single batch): 0.113902\n",
      "Epoch 470/1000 train loss: [array(0.113902, dtype=float32)], val loss: 0.1159631758928299\n",
      "\t partial train loss (single batch): 0.115340\n",
      "Epoch 471/1000 train loss: [array(0.11534026, dtype=float32)], val loss: 0.11603564023971558\n",
      "\t partial train loss (single batch): 0.113945\n",
      "Epoch 472/1000 train loss: [array(0.11394524, dtype=float32)], val loss: 0.11520686745643616\n",
      "\t partial train loss (single batch): 0.113768\n",
      "Epoch 473/1000 train loss: [array(0.11376813, dtype=float32)], val loss: 0.11512934416532516\n",
      "\t partial train loss (single batch): 0.114448\n",
      "Epoch 474/1000 train loss: [array(0.11444767, dtype=float32)], val loss: 0.11471779644489288\n",
      "\t partial train loss (single batch): 0.115162\n",
      "Epoch 475/1000 train loss: [array(0.11516174, dtype=float32)], val loss: 0.1149621456861496\n",
      "\t partial train loss (single batch): 0.114870\n",
      "Epoch 476/1000 train loss: [array(0.11486965, dtype=float32)], val loss: 0.1149032860994339\n",
      "\t partial train loss (single batch): 0.114771\n",
      "Epoch 477/1000 train loss: [array(0.11477056, dtype=float32)], val loss: 0.11449068784713745\n",
      "\t partial train loss (single batch): 0.114975\n",
      "Epoch 478/1000 train loss: [array(0.11497542, dtype=float32)], val loss: 0.11423572897911072\n",
      "\t partial train loss (single batch): 0.114864\n",
      "Epoch 479/1000 train loss: [array(0.1148636, dtype=float32)], val loss: 0.11518924683332443\n",
      "\t partial train loss (single batch): 0.114135\n",
      "Epoch 480/1000 train loss: [array(0.11413478, dtype=float32)], val loss: 0.11502383649349213\n",
      "\t partial train loss (single batch): 0.114048\n",
      "Epoch 481/1000 train loss: [array(0.11404785, dtype=float32)], val loss: 0.1148654893040657\n",
      "\t partial train loss (single batch): 0.113742\n",
      "Epoch 482/1000 train loss: [array(0.11374248, dtype=float32)], val loss: 0.1149083599448204\n",
      "\t partial train loss (single batch): 0.114368\n",
      "Epoch 483/1000 train loss: [array(0.11436761, dtype=float32)], val loss: 0.11412573605775833\n",
      "\t partial train loss (single batch): 0.114554\n",
      "Epoch 484/1000 train loss: [array(0.11455356, dtype=float32)], val loss: 0.11392222344875336\n",
      "\t partial train loss (single batch): 0.113997\n",
      "Epoch 485/1000 train loss: [array(0.11399718, dtype=float32)], val loss: 0.11471311002969742\n",
      "\t partial train loss (single batch): 0.113758\n",
      "Epoch 486/1000 train loss: [array(0.11375793, dtype=float32)], val loss: 0.11510219424962997\n",
      "\t partial train loss (single batch): 0.114353\n",
      "Epoch 487/1000 train loss: [array(0.11435287, dtype=float32)], val loss: 0.11445830762386322\n",
      "\t partial train loss (single batch): 0.114437\n",
      "Epoch 488/1000 train loss: [array(0.11443718, dtype=float32)], val loss: 0.11409229785203934\n",
      "\t partial train loss (single batch): 0.114179\n",
      "Epoch 489/1000 train loss: [array(0.11417861, dtype=float32)], val loss: 0.11512704193592072\n",
      "\t partial train loss (single batch): 0.113769\n",
      "Epoch 490/1000 train loss: [array(0.11376908, dtype=float32)], val loss: 0.11539478600025177\n",
      "\t partial train loss (single batch): 0.114582\n",
      "Epoch 491/1000 train loss: [array(0.11458164, dtype=float32)], val loss: 0.1145855262875557\n",
      "\t partial train loss (single batch): 0.114898\n",
      "Epoch 492/1000 train loss: [array(0.1148981, dtype=float32)], val loss: 0.114081010222435\n",
      "\t partial train loss (single batch): 0.112899\n",
      "Epoch 493/1000 train loss: [array(0.11289874, dtype=float32)], val loss: 0.11396215111017227\n",
      "\t partial train loss (single batch): 0.114228\n",
      "Epoch 494/1000 train loss: [array(0.11422779, dtype=float32)], val loss: 0.11380338668823242\n",
      "\t partial train loss (single batch): 0.112937\n",
      "Epoch 495/1000 train loss: [array(0.11293728, dtype=float32)], val loss: 0.11366575956344604\n",
      "\t partial train loss (single batch): 0.113370\n",
      "Epoch 496/1000 train loss: [array(0.11336969, dtype=float32)], val loss: 0.11395380645990372\n",
      "\t partial train loss (single batch): 0.112946\n",
      "Epoch 497/1000 train loss: [array(0.11294613, dtype=float32)], val loss: 0.1136549711227417\n",
      "\t partial train loss (single batch): 0.112914\n",
      "Epoch 498/1000 train loss: [array(0.11291432, dtype=float32)], val loss: 0.11382218450307846\n",
      "\t partial train loss (single batch): 0.113377\n",
      "Epoch 499/1000 train loss: [array(0.1133775, dtype=float32)], val loss: 0.11335154622793198\n",
      "\t partial train loss (single batch): 0.113504\n",
      "Epoch 500/1000 train loss: [array(0.11350425, dtype=float32)], val loss: 0.11375422030687332\n",
      "\t partial train loss (single batch): 0.112177\n",
      "Epoch 501/1000 train loss: [array(0.11217654, dtype=float32)], val loss: 0.11455187201499939\n",
      "\t partial train loss (single batch): 0.113420\n",
      "Epoch 502/1000 train loss: [array(0.11341956, dtype=float32)], val loss: 0.11415227502584457\n",
      "\t partial train loss (single batch): 0.112529\n",
      "Epoch 503/1000 train loss: [array(0.11252898, dtype=float32)], val loss: 0.11202369630336761\n",
      "\t partial train loss (single batch): 0.113209\n",
      "Epoch 504/1000 train loss: [array(0.11320905, dtype=float32)], val loss: 0.11402576416730881\n",
      "\t partial train loss (single batch): 0.111808\n",
      "Epoch 505/1000 train loss: [array(0.11180767, dtype=float32)], val loss: 0.11311228573322296\n",
      "\t partial train loss (single batch): 0.114016\n",
      "Epoch 506/1000 train loss: [array(0.11401603, dtype=float32)], val loss: 0.11246725171804428\n",
      "\t partial train loss (single batch): 0.112748\n",
      "Epoch 507/1000 train loss: [array(0.11274803, dtype=float32)], val loss: 0.11442973464727402\n",
      "\t partial train loss (single batch): 0.112189\n",
      "Epoch 508/1000 train loss: [array(0.11218934, dtype=float32)], val loss: 0.11367122083902359\n",
      "\t partial train loss (single batch): 0.113060\n",
      "Epoch 509/1000 train loss: [array(0.11306007, dtype=float32)], val loss: 0.11312583088874817\n",
      "\t partial train loss (single batch): 0.112901\n",
      "Epoch 510/1000 train loss: [array(0.11290137, dtype=float32)], val loss: 0.11440826207399368\n",
      "\t partial train loss (single batch): 0.112955\n",
      "Epoch 511/1000 train loss: [array(0.1129553, dtype=float32)], val loss: 0.1151045486330986\n",
      "\t partial train loss (single batch): 0.111668\n",
      "Epoch 512/1000 train loss: [array(0.11166777, dtype=float32)], val loss: 0.11229964345693588\n",
      "\t partial train loss (single batch): 0.112547\n",
      "Epoch 513/1000 train loss: [array(0.11254688, dtype=float32)], val loss: 0.11331017315387726\n",
      "\t partial train loss (single batch): 0.111713\n",
      "Epoch 514/1000 train loss: [array(0.11171269, dtype=float32)], val loss: 0.11355974525213242\n",
      "\t partial train loss (single batch): 0.112510\n",
      "Epoch 515/1000 train loss: [array(0.11250995, dtype=float32)], val loss: 0.11365636438131332\n",
      "\t partial train loss (single batch): 0.112314\n",
      "Epoch 516/1000 train loss: [array(0.11231425, dtype=float32)], val loss: 0.11379916965961456\n",
      "\t partial train loss (single batch): 0.111760\n",
      "Epoch 517/1000 train loss: [array(0.11175971, dtype=float32)], val loss: 0.11234883219003677\n",
      "\t partial train loss (single batch): 0.111814\n",
      "Epoch 518/1000 train loss: [array(0.11181389, dtype=float32)], val loss: 0.11280100047588348\n",
      "\t partial train loss (single batch): 0.112611\n",
      "Epoch 519/1000 train loss: [array(0.11261103, dtype=float32)], val loss: 0.11300832033157349\n",
      "\t partial train loss (single batch): 0.112485\n",
      "Epoch 520/1000 train loss: [array(0.11248484, dtype=float32)], val loss: 0.11268643289804459\n",
      "\t partial train loss (single batch): 0.112693\n",
      "Epoch 521/1000 train loss: [array(0.1126928, dtype=float32)], val loss: 0.11227700859308243\n",
      "\t partial train loss (single batch): 0.112647\n",
      "Epoch 522/1000 train loss: [array(0.11264718, dtype=float32)], val loss: 0.11205761134624481\n",
      "\t partial train loss (single batch): 0.112353\n",
      "Epoch 523/1000 train loss: [array(0.11235259, dtype=float32)], val loss: 0.11293766647577286\n",
      "\t partial train loss (single batch): 0.112301\n",
      "Epoch 524/1000 train loss: [array(0.11230142, dtype=float32)], val loss: 0.11269614845514297\n",
      "\t partial train loss (single batch): 0.113380\n",
      "Epoch 525/1000 train loss: [array(0.11337972, dtype=float32)], val loss: 0.113004669547081\n",
      "\t partial train loss (single batch): 0.112329\n",
      "Epoch 526/1000 train loss: [array(0.1123286, dtype=float32)], val loss: 0.11258000880479813\n",
      "\t partial train loss (single batch): 0.111697\n",
      "Epoch 527/1000 train loss: [array(0.11169659, dtype=float32)], val loss: 0.11280526965856552\n",
      "\t partial train loss (single batch): 0.112690\n",
      "Epoch 528/1000 train loss: [array(0.11268993, dtype=float32)], val loss: 0.11300762742757797\n",
      "\t partial train loss (single batch): 0.111716\n",
      "Epoch 529/1000 train loss: [array(0.11171579, dtype=float32)], val loss: 0.11285632848739624\n",
      "\t partial train loss (single batch): 0.111982\n",
      "Epoch 530/1000 train loss: [array(0.11198228, dtype=float32)], val loss: 0.11167074739933014\n",
      "\t partial train loss (single batch): 0.111758\n",
      "Epoch 531/1000 train loss: [array(0.11175796, dtype=float32)], val loss: 0.11249701678752899\n",
      "\t partial train loss (single batch): 0.111328\n",
      "Epoch 532/1000 train loss: [array(0.11132823, dtype=float32)], val loss: 0.11280673742294312\n",
      "\t partial train loss (single batch): 0.111492\n",
      "Epoch 533/1000 train loss: [array(0.11149167, dtype=float32)], val loss: 0.11264066398143768\n",
      "\t partial train loss (single batch): 0.111502\n",
      "Epoch 534/1000 train loss: [array(0.11150166, dtype=float32)], val loss: 0.11214515566825867\n",
      "\t partial train loss (single batch): 0.111872\n",
      "Epoch 535/1000 train loss: [array(0.11187186, dtype=float32)], val loss: 0.11201220005750656\n",
      "\t partial train loss (single batch): 0.111580\n",
      "Epoch 536/1000 train loss: [array(0.11158039, dtype=float32)], val loss: 0.1122475191950798\n",
      "\t partial train loss (single batch): 0.111263\n",
      "Epoch 537/1000 train loss: [array(0.11126273, dtype=float32)], val loss: 0.11162388324737549\n",
      "\t partial train loss (single batch): 0.111703\n",
      "Epoch 538/1000 train loss: [array(0.11170347, dtype=float32)], val loss: 0.11209329962730408\n",
      "\t partial train loss (single batch): 0.112322\n",
      "Epoch 539/1000 train loss: [array(0.11232165, dtype=float32)], val loss: 0.11254044622182846\n",
      "\t partial train loss (single batch): 0.112120\n",
      "Epoch 540/1000 train loss: [array(0.11211953, dtype=float32)], val loss: 0.11187746375799179\n",
      "\t partial train loss (single batch): 0.111738\n",
      "Epoch 541/1000 train loss: [array(0.11173845, dtype=float32)], val loss: 0.111576147377491\n",
      "\t partial train loss (single batch): 0.112118\n",
      "Epoch 542/1000 train loss: [array(0.11211824, dtype=float32)], val loss: 0.11153432726860046\n",
      "\t partial train loss (single batch): 0.111906\n",
      "Epoch 543/1000 train loss: [array(0.11190614, dtype=float32)], val loss: 0.11128976941108704\n",
      "\t partial train loss (single batch): 0.112169\n",
      "Epoch 544/1000 train loss: [array(0.11216868, dtype=float32)], val loss: 0.11319439113140106\n",
      "\t partial train loss (single batch): 0.111808\n",
      "Epoch 545/1000 train loss: [array(0.11180773, dtype=float32)], val loss: 0.11195120215415955\n",
      "\t partial train loss (single batch): 0.111063\n",
      "Epoch 546/1000 train loss: [array(0.1110632, dtype=float32)], val loss: 0.1119641661643982\n",
      "\t partial train loss (single batch): 0.111515\n",
      "Epoch 547/1000 train loss: [array(0.11151522, dtype=float32)], val loss: 0.11156677454710007\n",
      "\t partial train loss (single batch): 0.111967\n",
      "Epoch 548/1000 train loss: [array(0.11196712, dtype=float32)], val loss: 0.11022474616765976\n",
      "\t partial train loss (single batch): 0.112014\n",
      "Epoch 549/1000 train loss: [array(0.11201355, dtype=float32)], val loss: 0.11208999902009964\n",
      "\t partial train loss (single batch): 0.111703\n",
      "Epoch 550/1000 train loss: [array(0.11170287, dtype=float32)], val loss: 0.11139454692602158\n",
      "\t partial train loss (single batch): 0.110297\n",
      "Epoch 551/1000 train loss: [array(0.11029739, dtype=float32)], val loss: 0.11100385338068008\n",
      "\t partial train loss (single batch): 0.110210\n",
      "Epoch 552/1000 train loss: [array(0.11021049, dtype=float32)], val loss: 0.11212290823459625\n",
      "\t partial train loss (single batch): 0.111264\n",
      "Epoch 553/1000 train loss: [array(0.11126405, dtype=float32)], val loss: 0.11202935129404068\n",
      "\t partial train loss (single batch): 0.111630\n",
      "Epoch 554/1000 train loss: [array(0.11162954, dtype=float32)], val loss: 0.11178678274154663\n",
      "\t partial train loss (single batch): 0.110979\n",
      "Epoch 555/1000 train loss: [array(0.11097904, dtype=float32)], val loss: 0.1123628169298172\n",
      "\t partial train loss (single batch): 0.111726\n",
      "Epoch 556/1000 train loss: [array(0.11172624, dtype=float32)], val loss: 0.11087050288915634\n",
      "\t partial train loss (single batch): 0.110408\n",
      "Epoch 557/1000 train loss: [array(0.11040808, dtype=float32)], val loss: 0.11191295087337494\n",
      "\t partial train loss (single batch): 0.110914\n",
      "Epoch 558/1000 train loss: [array(0.11091404, dtype=float32)], val loss: 0.11096978932619095\n",
      "\t partial train loss (single batch): 0.110684\n",
      "Epoch 559/1000 train loss: [array(0.11068414, dtype=float32)], val loss: 0.11106935143470764\n",
      "\t partial train loss (single batch): 0.110846\n",
      "Epoch 560/1000 train loss: [array(0.11084636, dtype=float32)], val loss: 0.11171143501996994\n",
      "\t partial train loss (single batch): 0.111002\n",
      "Epoch 561/1000 train loss: [array(0.11100198, dtype=float32)], val loss: 0.11076841503381729\n",
      "\t partial train loss (single batch): 0.110721\n",
      "Epoch 562/1000 train loss: [array(0.1107205, dtype=float32)], val loss: 0.112006776034832\n",
      "\t partial train loss (single batch): 0.110048\n",
      "Epoch 563/1000 train loss: [array(0.11004791, dtype=float32)], val loss: 0.11147166788578033\n",
      "\t partial train loss (single batch): 0.110621\n",
      "Epoch 564/1000 train loss: [array(0.11062064, dtype=float32)], val loss: 0.11084707081317902\n",
      "\t partial train loss (single batch): 0.111264\n",
      "Epoch 565/1000 train loss: [array(0.11126406, dtype=float32)], val loss: 0.11043933779001236\n",
      "\t partial train loss (single batch): 0.110063\n",
      "Epoch 566/1000 train loss: [array(0.11006333, dtype=float32)], val loss: 0.11128367483615875\n",
      "\t partial train loss (single batch): 0.111140\n",
      "Epoch 567/1000 train loss: [array(0.11113998, dtype=float32)], val loss: 0.11065585911273956\n",
      "\t partial train loss (single batch): 0.110320\n",
      "Epoch 568/1000 train loss: [array(0.1103199, dtype=float32)], val loss: 0.11123543232679367\n",
      "\t partial train loss (single batch): 0.111254\n",
      "Epoch 569/1000 train loss: [array(0.11125416, dtype=float32)], val loss: 0.10959777981042862\n",
      "\t partial train loss (single batch): 0.110771\n",
      "Epoch 570/1000 train loss: [array(0.11077097, dtype=float32)], val loss: 0.11019226908683777\n",
      "\t partial train loss (single batch): 0.110904\n",
      "Epoch 571/1000 train loss: [array(0.11090368, dtype=float32)], val loss: 0.11091113090515137\n",
      "\t partial train loss (single batch): 0.110344\n",
      "Epoch 572/1000 train loss: [array(0.11034383, dtype=float32)], val loss: 0.11086481809616089\n",
      "\t partial train loss (single batch): 0.110021\n",
      "Epoch 573/1000 train loss: [array(0.11002145, dtype=float32)], val loss: 0.11161334812641144\n",
      "\t partial train loss (single batch): 0.111024\n",
      "Epoch 574/1000 train loss: [array(0.11102433, dtype=float32)], val loss: 0.1104760468006134\n",
      "\t partial train loss (single batch): 0.109856\n",
      "Epoch 575/1000 train loss: [array(0.10985638, dtype=float32)], val loss: 0.11028889566659927\n",
      "\t partial train loss (single batch): 0.109760\n",
      "Epoch 576/1000 train loss: [array(0.10975983, dtype=float32)], val loss: 0.11089557409286499\n",
      "\t partial train loss (single batch): 0.109691\n",
      "Epoch 577/1000 train loss: [array(0.1096912, dtype=float32)], val loss: 0.11101705580949783\n",
      "\t partial train loss (single batch): 0.108858\n",
      "Epoch 578/1000 train loss: [array(0.10885755, dtype=float32)], val loss: 0.10993599146604538\n",
      "\t partial train loss (single batch): 0.109655\n",
      "Epoch 579/1000 train loss: [array(0.10965485, dtype=float32)], val loss: 0.111322782933712\n",
      "\t partial train loss (single batch): 0.110143\n",
      "Epoch 580/1000 train loss: [array(0.11014305, dtype=float32)], val loss: 0.11018161475658417\n",
      "\t partial train loss (single batch): 0.110272\n",
      "Epoch 581/1000 train loss: [array(0.11027209, dtype=float32)], val loss: 0.11039317399263382\n",
      "\t partial train loss (single batch): 0.110066\n",
      "Epoch 582/1000 train loss: [array(0.11006647, dtype=float32)], val loss: 0.10932185500860214\n",
      "\t partial train loss (single batch): 0.111007\n",
      "Epoch 583/1000 train loss: [array(0.11100727, dtype=float32)], val loss: 0.11017177999019623\n",
      "\t partial train loss (single batch): 0.109148\n",
      "Epoch 584/1000 train loss: [array(0.10914841, dtype=float32)], val loss: 0.11052966117858887\n",
      "\t partial train loss (single batch): 0.110528\n",
      "Epoch 585/1000 train loss: [array(0.11052767, dtype=float32)], val loss: 0.11089292168617249\n",
      "\t partial train loss (single batch): 0.109525\n",
      "Epoch 586/1000 train loss: [array(0.10952537, dtype=float32)], val loss: 0.11036087572574615\n",
      "\t partial train loss (single batch): 0.109381\n",
      "Epoch 587/1000 train loss: [array(0.10938081, dtype=float32)], val loss: 0.11039607971906662\n",
      "\t partial train loss (single batch): 0.109827\n",
      "Epoch 588/1000 train loss: [array(0.10982722, dtype=float32)], val loss: 0.11013970524072647\n",
      "\t partial train loss (single batch): 0.108975\n",
      "Epoch 589/1000 train loss: [array(0.10897453, dtype=float32)], val loss: 0.10874606668949127\n",
      "\t partial train loss (single batch): 0.109060\n",
      "Epoch 590/1000 train loss: [array(0.10905952, dtype=float32)], val loss: 0.11031145602464676\n",
      "\t partial train loss (single batch): 0.109630\n",
      "Epoch 591/1000 train loss: [array(0.10962978, dtype=float32)], val loss: 0.10898836702108383\n",
      "\t partial train loss (single batch): 0.109935\n",
      "Epoch 592/1000 train loss: [array(0.10993463, dtype=float32)], val loss: 0.10994093865156174\n",
      "\t partial train loss (single batch): 0.109075\n",
      "Epoch 593/1000 train loss: [array(0.10907502, dtype=float32)], val loss: 0.10892052948474884\n",
      "\t partial train loss (single batch): 0.109270\n",
      "Epoch 594/1000 train loss: [array(0.10927027, dtype=float32)], val loss: 0.11080016940832138\n",
      "\t partial train loss (single batch): 0.108838\n",
      "Epoch 595/1000 train loss: [array(0.10883794, dtype=float32)], val loss: 0.1100650206208229\n",
      "\t partial train loss (single batch): 0.108904\n",
      "Epoch 596/1000 train loss: [array(0.10890387, dtype=float32)], val loss: 0.1095111221075058\n",
      "\t partial train loss (single batch): 0.110324\n",
      "Epoch 597/1000 train loss: [array(0.11032413, dtype=float32)], val loss: 0.10992640256881714\n",
      "\t partial train loss (single batch): 0.110046\n",
      "Epoch 598/1000 train loss: [array(0.11004627, dtype=float32)], val loss: 0.1090712621808052\n",
      "\t partial train loss (single batch): 0.108825\n",
      "Epoch 599/1000 train loss: [array(0.1088254, dtype=float32)], val loss: 0.10994965583086014\n",
      "\t partial train loss (single batch): 0.108781\n",
      "Epoch 600/1000 train loss: [array(0.10878127, dtype=float32)], val loss: 0.11010386794805527\n",
      "\t partial train loss (single batch): 0.108105\n",
      "Epoch 601/1000 train loss: [array(0.10810455, dtype=float32)], val loss: 0.1091243326663971\n",
      "\t partial train loss (single batch): 0.109627\n",
      "Epoch 602/1000 train loss: [array(0.1096274, dtype=float32)], val loss: 0.10918141156435013\n",
      "\t partial train loss (single batch): 0.109564\n",
      "Epoch 603/1000 train loss: [array(0.10956383, dtype=float32)], val loss: 0.10912904143333435\n",
      "\t partial train loss (single batch): 0.107672\n",
      "Epoch 604/1000 train loss: [array(0.10767196, dtype=float32)], val loss: 0.10891813039779663\n",
      "\t partial train loss (single batch): 0.108921\n",
      "Epoch 605/1000 train loss: [array(0.1089207, dtype=float32)], val loss: 0.11003538221120834\n",
      "\t partial train loss (single batch): 0.109052\n",
      "Epoch 606/1000 train loss: [array(0.10905249, dtype=float32)], val loss: 0.11000761389732361\n",
      "\t partial train loss (single batch): 0.108855\n",
      "Epoch 607/1000 train loss: [array(0.10885549, dtype=float32)], val loss: 0.10826549679040909\n",
      "\t partial train loss (single batch): 0.108758\n",
      "Epoch 608/1000 train loss: [array(0.10875798, dtype=float32)], val loss: 0.10957423597574234\n",
      "\t partial train loss (single batch): 0.108352\n",
      "Epoch 609/1000 train loss: [array(0.10835157, dtype=float32)], val loss: 0.10893744230270386\n",
      "\t partial train loss (single batch): 0.109011\n",
      "Epoch 610/1000 train loss: [array(0.10901149, dtype=float32)], val loss: 0.10940586775541306\n",
      "\t partial train loss (single batch): 0.108990\n",
      "Epoch 611/1000 train loss: [array(0.10899045, dtype=float32)], val loss: 0.10873624682426453\n",
      "\t partial train loss (single batch): 0.108776\n",
      "Epoch 612/1000 train loss: [array(0.10877585, dtype=float32)], val loss: 0.10918835550546646\n",
      "\t partial train loss (single batch): 0.109824\n",
      "Epoch 613/1000 train loss: [array(0.1098239, dtype=float32)], val loss: 0.10836538672447205\n",
      "\t partial train loss (single batch): 0.108730\n",
      "Epoch 614/1000 train loss: [array(0.10873035, dtype=float32)], val loss: 0.10947463661432266\n",
      "\t partial train loss (single batch): 0.108504\n",
      "Epoch 615/1000 train loss: [array(0.10850387, dtype=float32)], val loss: 0.10810211300849915\n",
      "\t partial train loss (single batch): 0.108666\n",
      "Epoch 616/1000 train loss: [array(0.10866583, dtype=float32)], val loss: 0.10828802734613419\n",
      "\t partial train loss (single batch): 0.109473\n",
      "Epoch 617/1000 train loss: [array(0.10947283, dtype=float32)], val loss: 0.1091356873512268\n",
      "\t partial train loss (single batch): 0.108595\n",
      "Epoch 618/1000 train loss: [array(0.10859513, dtype=float32)], val loss: 0.10873020440340042\n",
      "\t partial train loss (single batch): 0.107955\n",
      "Epoch 619/1000 train loss: [array(0.10795546, dtype=float32)], val loss: 0.1084984540939331\n",
      "\t partial train loss (single batch): 0.109188\n",
      "Epoch 620/1000 train loss: [array(0.10918813, dtype=float32)], val loss: 0.10890960693359375\n",
      "\t partial train loss (single batch): 0.108191\n",
      "Epoch 621/1000 train loss: [array(0.10819104, dtype=float32)], val loss: 0.10808828473091125\n",
      "\t partial train loss (single batch): 0.107912\n",
      "Epoch 622/1000 train loss: [array(0.107912, dtype=float32)], val loss: 0.10853611677885056\n",
      "\t partial train loss (single batch): 0.108287\n",
      "Epoch 623/1000 train loss: [array(0.10828668, dtype=float32)], val loss: 0.10896047204732895\n",
      "\t partial train loss (single batch): 0.108669\n",
      "Epoch 624/1000 train loss: [array(0.10866915, dtype=float32)], val loss: 0.10879971086978912\n",
      "\t partial train loss (single batch): 0.108374\n",
      "Epoch 625/1000 train loss: [array(0.10837445, dtype=float32)], val loss: 0.108673594892025\n",
      "\t partial train loss (single batch): 0.108090\n",
      "Epoch 626/1000 train loss: [array(0.10808972, dtype=float32)], val loss: 0.10875315219163895\n",
      "\t partial train loss (single batch): 0.107949\n",
      "Epoch 627/1000 train loss: [array(0.10794915, dtype=float32)], val loss: 0.10914628952741623\n",
      "\t partial train loss (single batch): 0.107948\n",
      "Epoch 628/1000 train loss: [array(0.10794796, dtype=float32)], val loss: 0.10776276141405106\n",
      "\t partial train loss (single batch): 0.108185\n",
      "Epoch 629/1000 train loss: [array(0.10818489, dtype=float32)], val loss: 0.10800816118717194\n",
      "\t partial train loss (single batch): 0.108252\n",
      "Epoch 630/1000 train loss: [array(0.10825226, dtype=float32)], val loss: 0.10906749218702316\n",
      "\t partial train loss (single batch): 0.107984\n",
      "Epoch 631/1000 train loss: [array(0.10798432, dtype=float32)], val loss: 0.10789080709218979\n",
      "\t partial train loss (single batch): 0.107003\n",
      "Epoch 632/1000 train loss: [array(0.10700271, dtype=float32)], val loss: 0.10815301537513733\n",
      "\t partial train loss (single batch): 0.107528\n",
      "Epoch 633/1000 train loss: [array(0.1075279, dtype=float32)], val loss: 0.10872530937194824\n",
      "\t partial train loss (single batch): 0.107048\n",
      "Epoch 634/1000 train loss: [array(0.10704819, dtype=float32)], val loss: 0.1082928329706192\n",
      "\t partial train loss (single batch): 0.107555\n",
      "Epoch 635/1000 train loss: [array(0.10755471, dtype=float32)], val loss: 0.10833767801523209\n",
      "\t partial train loss (single batch): 0.108167\n",
      "Epoch 636/1000 train loss: [array(0.10816735, dtype=float32)], val loss: 0.10796615481376648\n",
      "\t partial train loss (single batch): 0.107201\n",
      "Epoch 637/1000 train loss: [array(0.10720126, dtype=float32)], val loss: 0.10759402066469193\n",
      "\t partial train loss (single batch): 0.107559\n",
      "Epoch 638/1000 train loss: [array(0.10755911, dtype=float32)], val loss: 0.10845407843589783\n",
      "\t partial train loss (single batch): 0.107985\n",
      "Epoch 639/1000 train loss: [array(0.10798512, dtype=float32)], val loss: 0.10724415630102158\n",
      "\t partial train loss (single batch): 0.107261\n",
      "Epoch 640/1000 train loss: [array(0.10726121, dtype=float32)], val loss: 0.10723302513360977\n",
      "\t partial train loss (single batch): 0.107372\n",
      "Epoch 641/1000 train loss: [array(0.10737155, dtype=float32)], val loss: 0.107475146651268\n",
      "\t partial train loss (single batch): 0.107029\n",
      "Epoch 642/1000 train loss: [array(0.10702851, dtype=float32)], val loss: 0.10800871253013611\n",
      "\t partial train loss (single batch): 0.108287\n",
      "Epoch 643/1000 train loss: [array(0.10828741, dtype=float32)], val loss: 0.10903078317642212\n",
      "\t partial train loss (single batch): 0.108211\n",
      "Epoch 644/1000 train loss: [array(0.10821122, dtype=float32)], val loss: 0.1084648072719574\n",
      "\t partial train loss (single batch): 0.107782\n",
      "Epoch 645/1000 train loss: [array(0.10778244, dtype=float32)], val loss: 0.10906987637281418\n",
      "\t partial train loss (single batch): 0.107576\n",
      "Epoch 646/1000 train loss: [array(0.10757643, dtype=float32)], val loss: 0.10843385010957718\n",
      "\t partial train loss (single batch): 0.106552\n",
      "Epoch 647/1000 train loss: [array(0.1065521, dtype=float32)], val loss: 0.10831190645694733\n",
      "\t partial train loss (single batch): 0.107240\n",
      "Epoch 648/1000 train loss: [array(0.10723993, dtype=float32)], val loss: 0.10852206498384476\n",
      "\t partial train loss (single batch): 0.107675\n",
      "Epoch 649/1000 train loss: [array(0.10767512, dtype=float32)], val loss: 0.10711560398340225\n",
      "\t partial train loss (single batch): 0.107693\n",
      "Epoch 650/1000 train loss: [array(0.10769344, dtype=float32)], val loss: 0.10841713845729828\n",
      "\t partial train loss (single batch): 0.107389\n",
      "Epoch 651/1000 train loss: [array(0.10738897, dtype=float32)], val loss: 0.10839316993951797\n",
      "\t partial train loss (single batch): 0.106286\n",
      "Epoch 652/1000 train loss: [array(0.10628609, dtype=float32)], val loss: 0.10654797405004501\n",
      "\t partial train loss (single batch): 0.108127\n",
      "Epoch 653/1000 train loss: [array(0.1081266, dtype=float32)], val loss: 0.10764394700527191\n",
      "\t partial train loss (single batch): 0.106817\n",
      "Epoch 654/1000 train loss: [array(0.10681735, dtype=float32)], val loss: 0.10725326836109161\n",
      "\t partial train loss (single batch): 0.108536\n",
      "Epoch 655/1000 train loss: [array(0.10853618, dtype=float32)], val loss: 0.10717295110225677\n",
      "\t partial train loss (single batch): 0.107262\n",
      "Epoch 656/1000 train loss: [array(0.10726172, dtype=float32)], val loss: 0.10706793516874313\n",
      "\t partial train loss (single batch): 0.107443\n",
      "Epoch 657/1000 train loss: [array(0.10744332, dtype=float32)], val loss: 0.10771781206130981\n",
      "\t partial train loss (single batch): 0.107066\n",
      "Epoch 658/1000 train loss: [array(0.10706618, dtype=float32)], val loss: 0.10725157707929611\n",
      "\t partial train loss (single batch): 0.106596\n",
      "Epoch 659/1000 train loss: [array(0.10659587, dtype=float32)], val loss: 0.10799647122621536\n",
      "\t partial train loss (single batch): 0.106592\n",
      "Epoch 660/1000 train loss: [array(0.10659234, dtype=float32)], val loss: 0.1084785982966423\n",
      "\t partial train loss (single batch): 0.106613\n",
      "Epoch 661/1000 train loss: [array(0.10661341, dtype=float32)], val loss: 0.10677958279848099\n",
      "\t partial train loss (single batch): 0.108045\n",
      "Epoch 662/1000 train loss: [array(0.10804535, dtype=float32)], val loss: 0.10707973688840866\n",
      "\t partial train loss (single batch): 0.107276\n",
      "Epoch 663/1000 train loss: [array(0.10727584, dtype=float32)], val loss: 0.10664097964763641\n",
      "\t partial train loss (single batch): 0.106065\n",
      "Epoch 664/1000 train loss: [array(0.10606547, dtype=float32)], val loss: 0.10788441449403763\n",
      "\t partial train loss (single batch): 0.106254\n",
      "Epoch 665/1000 train loss: [array(0.10625395, dtype=float32)], val loss: 0.10797082632780075\n",
      "\t partial train loss (single batch): 0.106257\n",
      "Epoch 666/1000 train loss: [array(0.1062569, dtype=float32)], val loss: 0.10775714367628098\n",
      "\t partial train loss (single batch): 0.106844\n",
      "Epoch 667/1000 train loss: [array(0.10684352, dtype=float32)], val loss: 0.10701386630535126\n",
      "\t partial train loss (single batch): 0.107084\n",
      "Epoch 668/1000 train loss: [array(0.1070838, dtype=float32)], val loss: 0.10804805904626846\n",
      "\t partial train loss (single batch): 0.106722\n",
      "Epoch 669/1000 train loss: [array(0.10672215, dtype=float32)], val loss: 0.1071588322520256\n",
      "\t partial train loss (single batch): 0.106879\n",
      "Epoch 670/1000 train loss: [array(0.10687915, dtype=float32)], val loss: 0.10655348747968674\n",
      "\t partial train loss (single batch): 0.106351\n",
      "Epoch 671/1000 train loss: [array(0.10635072, dtype=float32)], val loss: 0.10736243426799774\n",
      "\t partial train loss (single batch): 0.105701\n",
      "Epoch 672/1000 train loss: [array(0.10570125, dtype=float32)], val loss: 0.10731770843267441\n",
      "\t partial train loss (single batch): 0.105885\n",
      "Epoch 673/1000 train loss: [array(0.10588545, dtype=float32)], val loss: 0.10739947855472565\n",
      "\t partial train loss (single batch): 0.106119\n",
      "Epoch 674/1000 train loss: [array(0.10611896, dtype=float32)], val loss: 0.10636644810438156\n",
      "\t partial train loss (single batch): 0.106463\n",
      "Epoch 675/1000 train loss: [array(0.10646273, dtype=float32)], val loss: 0.10724974423646927\n",
      "\t partial train loss (single batch): 0.106108\n",
      "Epoch 676/1000 train loss: [array(0.10610835, dtype=float32)], val loss: 0.10702241212129593\n",
      "\t partial train loss (single batch): 0.106893\n",
      "Epoch 677/1000 train loss: [array(0.1068928, dtype=float32)], val loss: 0.10722702741622925\n",
      "\t partial train loss (single batch): 0.106109\n",
      "Epoch 678/1000 train loss: [array(0.10610926, dtype=float32)], val loss: 0.10656861215829849\n",
      "\t partial train loss (single batch): 0.106252\n",
      "Epoch 679/1000 train loss: [array(0.1062518, dtype=float32)], val loss: 0.10694576799869537\n",
      "\t partial train loss (single batch): 0.106946\n",
      "Epoch 680/1000 train loss: [array(0.10694567, dtype=float32)], val loss: 0.10705531388521194\n",
      "\t partial train loss (single batch): 0.106825\n",
      "Epoch 681/1000 train loss: [array(0.10682475, dtype=float32)], val loss: 0.10710443556308746\n",
      "\t partial train loss (single batch): 0.105539\n",
      "Epoch 682/1000 train loss: [array(0.1055394, dtype=float32)], val loss: 0.10700185596942902\n",
      "\t partial train loss (single batch): 0.105459\n",
      "Epoch 683/1000 train loss: [array(0.10545947, dtype=float32)], val loss: 0.1071416586637497\n",
      "\t partial train loss (single batch): 0.106606\n",
      "Epoch 684/1000 train loss: [array(0.10660601, dtype=float32)], val loss: 0.10667935758829117\n",
      "\t partial train loss (single batch): 0.104342\n",
      "Epoch 685/1000 train loss: [array(0.10434224, dtype=float32)], val loss: 0.1081034317612648\n",
      "\t partial train loss (single batch): 0.105631\n",
      "Epoch 686/1000 train loss: [array(0.10563108, dtype=float32)], val loss: 0.10766863077878952\n",
      "\t partial train loss (single batch): 0.106740\n",
      "Epoch 687/1000 train loss: [array(0.10673964, dtype=float32)], val loss: 0.10679727792739868\n",
      "\t partial train loss (single batch): 0.106646\n",
      "Epoch 688/1000 train loss: [array(0.10664596, dtype=float32)], val loss: 0.10709837824106216\n",
      "\t partial train loss (single batch): 0.106749\n",
      "Epoch 689/1000 train loss: [array(0.10674904, dtype=float32)], val loss: 0.10475673526525497\n",
      "\t partial train loss (single batch): 0.106372\n",
      "Epoch 690/1000 train loss: [array(0.10637227, dtype=float32)], val loss: 0.10549219697713852\n",
      "\t partial train loss (single batch): 0.106295\n",
      "Epoch 691/1000 train loss: [array(0.10629536, dtype=float32)], val loss: 0.10525303333997726\n",
      "\t partial train loss (single batch): 0.105991\n",
      "Epoch 692/1000 train loss: [array(0.10599094, dtype=float32)], val loss: 0.1061587855219841\n",
      "\t partial train loss (single batch): 0.105126\n",
      "Epoch 693/1000 train loss: [array(0.10512587, dtype=float32)], val loss: 0.10493442416191101\n",
      "\t partial train loss (single batch): 0.105134\n",
      "Epoch 694/1000 train loss: [array(0.10513407, dtype=float32)], val loss: 0.10600744932889938\n",
      "\t partial train loss (single batch): 0.104286\n",
      "Epoch 695/1000 train loss: [array(0.10428622, dtype=float32)], val loss: 0.10617830604314804\n",
      "\t partial train loss (single batch): 0.104615\n",
      "Epoch 696/1000 train loss: [array(0.10461477, dtype=float32)], val loss: 0.10683486610651016\n",
      "\t partial train loss (single batch): 0.105255\n",
      "Epoch 697/1000 train loss: [array(0.10525533, dtype=float32)], val loss: 0.10632588714361191\n",
      "\t partial train loss (single batch): 0.104681\n",
      "Epoch 698/1000 train loss: [array(0.10468054, dtype=float32)], val loss: 0.10556447505950928\n",
      "\t partial train loss (single batch): 0.104639\n",
      "Epoch 699/1000 train loss: [array(0.10463902, dtype=float32)], val loss: 0.10675491392612457\n",
      "\t partial train loss (single batch): 0.104836\n",
      "Epoch 700/1000 train loss: [array(0.10483593, dtype=float32)], val loss: 0.10609328746795654\n",
      "\t partial train loss (single batch): 0.104572\n",
      "Epoch 701/1000 train loss: [array(0.10457153, dtype=float32)], val loss: 0.10640766471624374\n",
      "\t partial train loss (single batch): 0.106026\n",
      "Epoch 702/1000 train loss: [array(0.10602605, dtype=float32)], val loss: 0.10545667260885239\n",
      "\t partial train loss (single batch): 0.105330\n",
      "Epoch 703/1000 train loss: [array(0.10533024, dtype=float32)], val loss: 0.10616409033536911\n",
      "\t partial train loss (single batch): 0.105703\n",
      "Epoch 704/1000 train loss: [array(0.1057035, dtype=float32)], val loss: 0.10703036934137344\n",
      "\t partial train loss (single batch): 0.105578\n",
      "Epoch 705/1000 train loss: [array(0.10557798, dtype=float32)], val loss: 0.10551908612251282\n",
      "\t partial train loss (single batch): 0.106160\n",
      "Epoch 706/1000 train loss: [array(0.10615966, dtype=float32)], val loss: 0.10593587160110474\n",
      "\t partial train loss (single batch): 0.104835\n",
      "Epoch 707/1000 train loss: [array(0.10483541, dtype=float32)], val loss: 0.10582691431045532\n",
      "\t partial train loss (single batch): 0.104423\n",
      "Epoch 708/1000 train loss: [array(0.10442264, dtype=float32)], val loss: 0.10556820034980774\n",
      "\t partial train loss (single batch): 0.104846\n",
      "Epoch 709/1000 train loss: [array(0.10484618, dtype=float32)], val loss: 0.10621634125709534\n",
      "\t partial train loss (single batch): 0.104848\n",
      "Epoch 710/1000 train loss: [array(0.10484768, dtype=float32)], val loss: 0.10478709638118744\n",
      "\t partial train loss (single batch): 0.105738\n",
      "Epoch 711/1000 train loss: [array(0.10573807, dtype=float32)], val loss: 0.10610039532184601\n",
      "\t partial train loss (single batch): 0.105604\n",
      "Epoch 712/1000 train loss: [array(0.10560448, dtype=float32)], val loss: 0.10638383030891418\n",
      "\t partial train loss (single batch): 0.104643\n",
      "Epoch 713/1000 train loss: [array(0.10464254, dtype=float32)], val loss: 0.1050122082233429\n",
      "\t partial train loss (single batch): 0.104647\n",
      "Epoch 714/1000 train loss: [array(0.10464741, dtype=float32)], val loss: 0.10509390383958817\n",
      "\t partial train loss (single batch): 0.105637\n",
      "Epoch 715/1000 train loss: [array(0.10563675, dtype=float32)], val loss: 0.10615810006856918\n",
      "\t partial train loss (single batch): 0.104884\n",
      "Epoch 716/1000 train loss: [array(0.10488418, dtype=float32)], val loss: 0.10489511489868164\n",
      "\t partial train loss (single batch): 0.104624\n",
      "Epoch 717/1000 train loss: [array(0.10462426, dtype=float32)], val loss: 0.1053042933344841\n",
      "\t partial train loss (single batch): 0.104866\n",
      "Epoch 718/1000 train loss: [array(0.10486642, dtype=float32)], val loss: 0.10558933764696121\n",
      "\t partial train loss (single batch): 0.105281\n",
      "Epoch 719/1000 train loss: [array(0.10528105, dtype=float32)], val loss: 0.1059931144118309\n",
      "\t partial train loss (single batch): 0.104848\n",
      "Epoch 720/1000 train loss: [array(0.1048478, dtype=float32)], val loss: 0.10502150654792786\n",
      "\t partial train loss (single batch): 0.104201\n",
      "Epoch 721/1000 train loss: [array(0.1042014, dtype=float32)], val loss: 0.10559562593698502\n",
      "\t partial train loss (single batch): 0.105255\n",
      "Epoch 722/1000 train loss: [array(0.1052554, dtype=float32)], val loss: 0.10552483797073364\n",
      "\t partial train loss (single batch): 0.103725\n",
      "Epoch 723/1000 train loss: [array(0.10372496, dtype=float32)], val loss: 0.10553824156522751\n",
      "\t partial train loss (single batch): 0.104513\n",
      "Epoch 724/1000 train loss: [array(0.10451315, dtype=float32)], val loss: 0.10532848536968231\n",
      "\t partial train loss (single batch): 0.105141\n",
      "Epoch 725/1000 train loss: [array(0.10514088, dtype=float32)], val loss: 0.10537585616111755\n",
      "\t partial train loss (single batch): 0.104682\n",
      "Epoch 726/1000 train loss: [array(0.10468193, dtype=float32)], val loss: 0.10536034405231476\n",
      "\t partial train loss (single batch): 0.104734\n",
      "Epoch 727/1000 train loss: [array(0.10473432, dtype=float32)], val loss: 0.10565412789583206\n",
      "\t partial train loss (single batch): 0.104764\n",
      "Epoch 728/1000 train loss: [array(0.10476403, dtype=float32)], val loss: 0.10412829369306564\n",
      "\t partial train loss (single batch): 0.105002\n",
      "Epoch 729/1000 train loss: [array(0.1050021, dtype=float32)], val loss: 0.10566316545009613\n",
      "\t partial train loss (single batch): 0.105224\n",
      "Epoch 730/1000 train loss: [array(0.10522449, dtype=float32)], val loss: 0.10518680512905121\n",
      "\t partial train loss (single batch): 0.103447\n",
      "Epoch 731/1000 train loss: [array(0.10344692, dtype=float32)], val loss: 0.1039312332868576\n",
      "\t partial train loss (single batch): 0.104492\n",
      "Epoch 732/1000 train loss: [array(0.10449242, dtype=float32)], val loss: 0.1051410436630249\n",
      "\t partial train loss (single batch): 0.103103\n",
      "Epoch 733/1000 train loss: [array(0.10310265, dtype=float32)], val loss: 0.10596258193254471\n",
      "\t partial train loss (single batch): 0.104379\n",
      "Epoch 734/1000 train loss: [array(0.1043792, dtype=float32)], val loss: 0.10588327050209045\n",
      "\t partial train loss (single batch): 0.103383\n",
      "Epoch 735/1000 train loss: [array(0.10338292, dtype=float32)], val loss: 0.10470934212207794\n",
      "\t partial train loss (single batch): 0.104072\n",
      "Epoch 736/1000 train loss: [array(0.1040717, dtype=float32)], val loss: 0.10630273818969727\n",
      "\t partial train loss (single batch): 0.104780\n",
      "Epoch 737/1000 train loss: [array(0.10477997, dtype=float32)], val loss: 0.1050933375954628\n",
      "\t partial train loss (single batch): 0.103180\n",
      "Epoch 738/1000 train loss: [array(0.10317989, dtype=float32)], val loss: 0.10574683547019958\n",
      "\t partial train loss (single batch): 0.104143\n",
      "Epoch 739/1000 train loss: [array(0.10414284, dtype=float32)], val loss: 0.10449880361557007\n",
      "\t partial train loss (single batch): 0.104021\n",
      "Epoch 740/1000 train loss: [array(0.1040214, dtype=float32)], val loss: 0.10502811521291733\n",
      "\t partial train loss (single batch): 0.104273\n",
      "Epoch 741/1000 train loss: [array(0.10427276, dtype=float32)], val loss: 0.10488036274909973\n",
      "\t partial train loss (single batch): 0.104197\n",
      "Epoch 742/1000 train loss: [array(0.10419664, dtype=float32)], val loss: 0.10357119888067245\n",
      "\t partial train loss (single batch): 0.104185\n",
      "Epoch 743/1000 train loss: [array(0.10418463, dtype=float32)], val loss: 0.10407809168100357\n",
      "\t partial train loss (single batch): 0.103929\n",
      "Epoch 744/1000 train loss: [array(0.10392938, dtype=float32)], val loss: 0.10423677414655685\n",
      "\t partial train loss (single batch): 0.104752\n",
      "Epoch 745/1000 train loss: [array(0.10475213, dtype=float32)], val loss: 0.10380282998085022\n",
      "\t partial train loss (single batch): 0.105393\n",
      "Epoch 746/1000 train loss: [array(0.10539349, dtype=float32)], val loss: 0.10419312864542007\n",
      "\t partial train loss (single batch): 0.105227\n",
      "Epoch 747/1000 train loss: [array(0.10522716, dtype=float32)], val loss: 0.10441803187131882\n",
      "\t partial train loss (single batch): 0.104031\n",
      "Epoch 748/1000 train loss: [array(0.10403066, dtype=float32)], val loss: 0.10539271682500839\n",
      "\t partial train loss (single batch): 0.103655\n",
      "Epoch 749/1000 train loss: [array(0.10365488, dtype=float32)], val loss: 0.10381270945072174\n",
      "\t partial train loss (single batch): 0.103630\n",
      "Epoch 750/1000 train loss: [array(0.10363013, dtype=float32)], val loss: 0.10504890233278275\n",
      "\t partial train loss (single batch): 0.103995\n",
      "Epoch 751/1000 train loss: [array(0.10399499, dtype=float32)], val loss: 0.10524997115135193\n",
      "\t partial train loss (single batch): 0.103720\n",
      "Epoch 752/1000 train loss: [array(0.10372008, dtype=float32)], val loss: 0.10516732186079025\n",
      "\t partial train loss (single batch): 0.104307\n",
      "Epoch 753/1000 train loss: [array(0.10430719, dtype=float32)], val loss: 0.10443957149982452\n",
      "\t partial train loss (single batch): 0.104130\n",
      "Epoch 754/1000 train loss: [array(0.10412963, dtype=float32)], val loss: 0.10415314137935638\n",
      "\t partial train loss (single batch): 0.104491\n",
      "Epoch 755/1000 train loss: [array(0.1044913, dtype=float32)], val loss: 0.10346736758947372\n",
      "\t partial train loss (single batch): 0.103795\n",
      "Epoch 756/1000 train loss: [array(0.10379507, dtype=float32)], val loss: 0.10403715819120407\n",
      "\t partial train loss (single batch): 0.104554\n",
      "Epoch 757/1000 train loss: [array(0.10455426, dtype=float32)], val loss: 0.10422054678201675\n",
      "\t partial train loss (single batch): 0.103887\n",
      "Epoch 758/1000 train loss: [array(0.10388736, dtype=float32)], val loss: 0.10331346094608307\n",
      "\t partial train loss (single batch): 0.103621\n",
      "Epoch 759/1000 train loss: [array(0.10362118, dtype=float32)], val loss: 0.10396075248718262\n",
      "\t partial train loss (single batch): 0.103601\n",
      "Epoch 760/1000 train loss: [array(0.10360086, dtype=float32)], val loss: 0.10528446733951569\n",
      "\t partial train loss (single batch): 0.103386\n",
      "Epoch 761/1000 train loss: [array(0.10338619, dtype=float32)], val loss: 0.10443682223558426\n",
      "\t partial train loss (single batch): 0.103899\n",
      "Epoch 762/1000 train loss: [array(0.10389926, dtype=float32)], val loss: 0.1039581224322319\n",
      "\t partial train loss (single batch): 0.103627\n",
      "Epoch 763/1000 train loss: [array(0.10362665, dtype=float32)], val loss: 0.10414853692054749\n",
      "\t partial train loss (single batch): 0.102793\n",
      "Epoch 764/1000 train loss: [array(0.10279345, dtype=float32)], val loss: 0.10443930327892303\n",
      "\t partial train loss (single batch): 0.103851\n",
      "Epoch 765/1000 train loss: [array(0.10385105, dtype=float32)], val loss: 0.10434963554143906\n",
      "\t partial train loss (single batch): 0.103481\n",
      "Epoch 766/1000 train loss: [array(0.10348088, dtype=float32)], val loss: 0.10387413948774338\n",
      "\t partial train loss (single batch): 0.103311\n",
      "Epoch 767/1000 train loss: [array(0.10331114, dtype=float32)], val loss: 0.10389264672994614\n",
      "\t partial train loss (single batch): 0.102838\n",
      "Epoch 768/1000 train loss: [array(0.10283773, dtype=float32)], val loss: 0.10360519587993622\n",
      "\t partial train loss (single batch): 0.103197\n",
      "Epoch 769/1000 train loss: [array(0.1031969, dtype=float32)], val loss: 0.1049460768699646\n",
      "\t partial train loss (single batch): 0.101727\n",
      "Epoch 770/1000 train loss: [array(0.1017269, dtype=float32)], val loss: 0.10300794988870621\n",
      "\t partial train loss (single batch): 0.103943\n",
      "Epoch 771/1000 train loss: [array(0.10394324, dtype=float32)], val loss: 0.10448125749826431\n",
      "\t partial train loss (single batch): 0.101752\n",
      "Epoch 772/1000 train loss: [array(0.10175157, dtype=float32)], val loss: 0.10353046655654907\n",
      "\t partial train loss (single batch): 0.102930\n",
      "Epoch 773/1000 train loss: [array(0.10293023, dtype=float32)], val loss: 0.10446979850530624\n",
      "\t partial train loss (single batch): 0.102544\n",
      "Epoch 774/1000 train loss: [array(0.10254356, dtype=float32)], val loss: 0.10405491292476654\n",
      "\t partial train loss (single batch): 0.103134\n",
      "Epoch 775/1000 train loss: [array(0.10313377, dtype=float32)], val loss: 0.10418731719255447\n",
      "\t partial train loss (single batch): 0.102249\n",
      "Epoch 776/1000 train loss: [array(0.10224937, dtype=float32)], val loss: 0.10298324376344681\n",
      "\t partial train loss (single batch): 0.102891\n",
      "Epoch 777/1000 train loss: [array(0.10289089, dtype=float32)], val loss: 0.10338298976421356\n",
      "\t partial train loss (single batch): 0.103254\n",
      "Epoch 778/1000 train loss: [array(0.10325442, dtype=float32)], val loss: 0.10419674217700958\n",
      "\t partial train loss (single batch): 0.102655\n",
      "Epoch 779/1000 train loss: [array(0.1026548, dtype=float32)], val loss: 0.10490407794713974\n",
      "\t partial train loss (single batch): 0.102202\n",
      "Epoch 780/1000 train loss: [array(0.10220244, dtype=float32)], val loss: 0.10329148173332214\n",
      "\t partial train loss (single batch): 0.104335\n",
      "Epoch 781/1000 train loss: [array(0.10433514, dtype=float32)], val loss: 0.10273586213588715\n",
      "\t partial train loss (single batch): 0.103602\n",
      "Epoch 782/1000 train loss: [array(0.10360175, dtype=float32)], val loss: 0.10481031984090805\n",
      "\t partial train loss (single batch): 0.104512\n",
      "Epoch 783/1000 train loss: [array(0.10451206, dtype=float32)], val loss: 0.10383905470371246\n",
      "\t partial train loss (single batch): 0.103198\n",
      "Epoch 784/1000 train loss: [array(0.10319756, dtype=float32)], val loss: 0.10325770080089569\n",
      "\t partial train loss (single batch): 0.102511\n",
      "Epoch 785/1000 train loss: [array(0.1025109, dtype=float32)], val loss: 0.10340365767478943\n",
      "\t partial train loss (single batch): 0.103126\n",
      "Epoch 786/1000 train loss: [array(0.1031257, dtype=float32)], val loss: 0.1032097190618515\n",
      "\t partial train loss (single batch): 0.102636\n",
      "Epoch 787/1000 train loss: [array(0.10263581, dtype=float32)], val loss: 0.1038363054394722\n",
      "\t partial train loss (single batch): 0.102321\n",
      "Epoch 788/1000 train loss: [array(0.10232134, dtype=float32)], val loss: 0.10392133891582489\n",
      "\t partial train loss (single batch): 0.103533\n",
      "Epoch 789/1000 train loss: [array(0.10353269, dtype=float32)], val loss: 0.10334420949220657\n",
      "\t partial train loss (single batch): 0.103541\n",
      "Epoch 790/1000 train loss: [array(0.10354147, dtype=float32)], val loss: 0.10245874524116516\n",
      "\t partial train loss (single batch): 0.103063\n",
      "Epoch 791/1000 train loss: [array(0.10306336, dtype=float32)], val loss: 0.1021893322467804\n",
      "\t partial train loss (single batch): 0.102277\n",
      "Epoch 792/1000 train loss: [array(0.10227694, dtype=float32)], val loss: 0.10333278775215149\n",
      "\t partial train loss (single batch): 0.101992\n",
      "Epoch 793/1000 train loss: [array(0.10199182, dtype=float32)], val loss: 0.1040695309638977\n",
      "\t partial train loss (single batch): 0.103523\n",
      "Epoch 794/1000 train loss: [array(0.10352307, dtype=float32)], val loss: 0.10292299091815948\n",
      "\t partial train loss (single batch): 0.102904\n",
      "Epoch 795/1000 train loss: [array(0.10290373, dtype=float32)], val loss: 0.10329863429069519\n",
      "\t partial train loss (single batch): 0.100809\n",
      "Epoch 796/1000 train loss: [array(0.10080867, dtype=float32)], val loss: 0.10189896076917648\n",
      "\t partial train loss (single batch): 0.102140\n",
      "Epoch 797/1000 train loss: [array(0.1021404, dtype=float32)], val loss: 0.10363951325416565\n",
      "\t partial train loss (single batch): 0.101789\n",
      "Epoch 798/1000 train loss: [array(0.10178884, dtype=float32)], val loss: 0.10293582081794739\n",
      "\t partial train loss (single batch): 0.102961\n",
      "Epoch 799/1000 train loss: [array(0.10296068, dtype=float32)], val loss: 0.1030387133359909\n",
      "\t partial train loss (single batch): 0.102432\n",
      "Epoch 800/1000 train loss: [array(0.10243163, dtype=float32)], val loss: 0.1037437692284584\n",
      "\t partial train loss (single batch): 0.102146\n",
      "Epoch 801/1000 train loss: [array(0.10214608, dtype=float32)], val loss: 0.1025395318865776\n",
      "\t partial train loss (single batch): 0.103008\n",
      "Epoch 802/1000 train loss: [array(0.10300782, dtype=float32)], val loss: 0.10371234267950058\n",
      "\t partial train loss (single batch): 0.102322\n",
      "Epoch 803/1000 train loss: [array(0.10232197, dtype=float32)], val loss: 0.10231220722198486\n",
      "\t partial train loss (single batch): 0.102358\n",
      "Epoch 804/1000 train loss: [array(0.10235795, dtype=float32)], val loss: 0.1023237332701683\n",
      "\t partial train loss (single batch): 0.103552\n",
      "Epoch 805/1000 train loss: [array(0.10355229, dtype=float32)], val loss: 0.10179470479488373\n",
      "\t partial train loss (single batch): 0.102569\n",
      "Epoch 806/1000 train loss: [array(0.10256948, dtype=float32)], val loss: 0.10275489836931229\n",
      "\t partial train loss (single batch): 0.103143\n",
      "Epoch 807/1000 train loss: [array(0.10314256, dtype=float32)], val loss: 0.10342108458280563\n",
      "\t partial train loss (single batch): 0.102111\n",
      "Epoch 808/1000 train loss: [array(0.10211135, dtype=float32)], val loss: 0.10277285426855087\n",
      "\t partial train loss (single batch): 0.102494\n",
      "Epoch 809/1000 train loss: [array(0.10249405, dtype=float32)], val loss: 0.10426605492830276\n",
      "\t partial train loss (single batch): 0.103006\n",
      "Epoch 810/1000 train loss: [array(0.10300582, dtype=float32)], val loss: 0.10291949659585953\n",
      "\t partial train loss (single batch): 0.102338\n",
      "Epoch 811/1000 train loss: [array(0.10233849, dtype=float32)], val loss: 0.10249321162700653\n",
      "\t partial train loss (single batch): 0.101632\n",
      "Epoch 812/1000 train loss: [array(0.10163163, dtype=float32)], val loss: 0.10207024961709976\n",
      "\t partial train loss (single batch): 0.100543\n",
      "Epoch 813/1000 train loss: [array(0.10054346, dtype=float32)], val loss: 0.10342445224523544\n",
      "\t partial train loss (single batch): 0.102903\n",
      "Epoch 814/1000 train loss: [array(0.10290284, dtype=float32)], val loss: 0.1029849499464035\n",
      "\t partial train loss (single batch): 0.102260\n",
      "Epoch 815/1000 train loss: [array(0.10225972, dtype=float32)], val loss: 0.10309328138828278\n",
      "\t partial train loss (single batch): 0.102183\n",
      "Epoch 816/1000 train loss: [array(0.10218289, dtype=float32)], val loss: 0.10227928310632706\n",
      "\t partial train loss (single batch): 0.102106\n",
      "Epoch 817/1000 train loss: [array(0.10210647, dtype=float32)], val loss: 0.10284600406885147\n",
      "\t partial train loss (single batch): 0.102693\n",
      "Epoch 818/1000 train loss: [array(0.10269308, dtype=float32)], val loss: 0.10271340608596802\n",
      "\t partial train loss (single batch): 0.102397\n",
      "Epoch 819/1000 train loss: [array(0.10239696, dtype=float32)], val loss: 0.10268311947584152\n",
      "\t partial train loss (single batch): 0.101637\n",
      "Epoch 820/1000 train loss: [array(0.10163707, dtype=float32)], val loss: 0.10314922034740448\n",
      "\t partial train loss (single batch): 0.102289\n",
      "Epoch 821/1000 train loss: [array(0.10228926, dtype=float32)], val loss: 0.10272519290447235\n",
      "\t partial train loss (single batch): 0.101163\n",
      "Epoch 822/1000 train loss: [array(0.10116348, dtype=float32)], val loss: 0.10319672524929047\n",
      "\t partial train loss (single batch): 0.101191\n",
      "Epoch 823/1000 train loss: [array(0.10119119, dtype=float32)], val loss: 0.10233768820762634\n",
      "\t partial train loss (single batch): 0.102357\n",
      "Epoch 824/1000 train loss: [array(0.10235664, dtype=float32)], val loss: 0.10315926373004913\n",
      "\t partial train loss (single batch): 0.101880\n",
      "Epoch 825/1000 train loss: [array(0.10188049, dtype=float32)], val loss: 0.10129643231630325\n",
      "\t partial train loss (single batch): 0.101463\n",
      "Epoch 826/1000 train loss: [array(0.10146292, dtype=float32)], val loss: 0.10240725427865982\n",
      "\t partial train loss (single batch): 0.102381\n",
      "Epoch 827/1000 train loss: [array(0.10238124, dtype=float32)], val loss: 0.10196467489004135\n",
      "\t partial train loss (single batch): 0.102056\n",
      "Epoch 828/1000 train loss: [array(0.10205568, dtype=float32)], val loss: 0.10294444859027863\n",
      "\t partial train loss (single batch): 0.102332\n",
      "Epoch 829/1000 train loss: [array(0.10233247, dtype=float32)], val loss: 0.10227635502815247\n",
      "\t partial train loss (single batch): 0.102559\n",
      "Epoch 830/1000 train loss: [array(0.10255887, dtype=float32)], val loss: 0.10310561209917068\n",
      "\t partial train loss (single batch): 0.102820\n",
      "Epoch 831/1000 train loss: [array(0.10282045, dtype=float32)], val loss: 0.10225026309490204\n",
      "\t partial train loss (single batch): 0.101810\n",
      "Epoch 832/1000 train loss: [array(0.10180954, dtype=float32)], val loss: 0.10196171700954437\n",
      "\t partial train loss (single batch): 0.102229\n",
      "Epoch 833/1000 train loss: [array(0.10222875, dtype=float32)], val loss: 0.10410493612289429\n",
      "\t partial train loss (single batch): 0.101843\n",
      "Epoch 834/1000 train loss: [array(0.10184288, dtype=float32)], val loss: 0.10227946937084198\n",
      "\t partial train loss (single batch): 0.102509\n",
      "Epoch 835/1000 train loss: [array(0.10250897, dtype=float32)], val loss: 0.10351712256669998\n",
      "\t partial train loss (single batch): 0.101967\n",
      "Epoch 836/1000 train loss: [array(0.10196696, dtype=float32)], val loss: 0.10283824801445007\n",
      "\t partial train loss (single batch): 0.102004\n",
      "Epoch 837/1000 train loss: [array(0.10200445, dtype=float32)], val loss: 0.10154521465301514\n",
      "\t partial train loss (single batch): 0.102187\n",
      "Epoch 838/1000 train loss: [array(0.10218662, dtype=float32)], val loss: 0.10187385231256485\n",
      "\t partial train loss (single batch): 0.101878\n",
      "Epoch 839/1000 train loss: [array(0.10187793, dtype=float32)], val loss: 0.10254497826099396\n",
      "\t partial train loss (single batch): 0.101685\n",
      "Epoch 840/1000 train loss: [array(0.10168457, dtype=float32)], val loss: 0.10262970626354218\n",
      "\t partial train loss (single batch): 0.101164\n",
      "Epoch 841/1000 train loss: [array(0.10116372, dtype=float32)], val loss: 0.10226603597402573\n",
      "\t partial train loss (single batch): 0.100908\n",
      "Epoch 842/1000 train loss: [array(0.10090818, dtype=float32)], val loss: 0.1023346483707428\n",
      "\t partial train loss (single batch): 0.102557\n",
      "Epoch 843/1000 train loss: [array(0.10255658, dtype=float32)], val loss: 0.10227934271097183\n",
      "\t partial train loss (single batch): 0.101311\n",
      "Epoch 844/1000 train loss: [array(0.10131056, dtype=float32)], val loss: 0.10207891464233398\n",
      "\t partial train loss (single batch): 0.102216\n",
      "Epoch 845/1000 train loss: [array(0.10221597, dtype=float32)], val loss: 0.10121119767427444\n",
      "\t partial train loss (single batch): 0.100736\n",
      "Epoch 846/1000 train loss: [array(0.10073627, dtype=float32)], val loss: 0.10157674551010132\n",
      "\t partial train loss (single batch): 0.100588\n",
      "Epoch 847/1000 train loss: [array(0.10058844, dtype=float32)], val loss: 0.10227064043283463\n",
      "\t partial train loss (single batch): 0.101558\n",
      "Epoch 848/1000 train loss: [array(0.10155808, dtype=float32)], val loss: 0.10067685693502426\n",
      "\t partial train loss (single batch): 0.101461\n",
      "Epoch 849/1000 train loss: [array(0.10146139, dtype=float32)], val loss: 0.10263614356517792\n",
      "\t partial train loss (single batch): 0.100823\n",
      "Epoch 850/1000 train loss: [array(0.10082292, dtype=float32)], val loss: 0.10251499712467194\n",
      "\t partial train loss (single batch): 0.101573\n",
      "Epoch 851/1000 train loss: [array(0.10157277, dtype=float32)], val loss: 0.10156040638685226\n",
      "\t partial train loss (single batch): 0.101749\n",
      "Epoch 852/1000 train loss: [array(0.1017492, dtype=float32)], val loss: 0.10246396064758301\n",
      "\t partial train loss (single batch): 0.101842\n",
      "Epoch 853/1000 train loss: [array(0.10184197, dtype=float32)], val loss: 0.10151619464159012\n",
      "\t partial train loss (single batch): 0.100966\n",
      "Epoch 854/1000 train loss: [array(0.10096616, dtype=float32)], val loss: 0.10215135663747787\n",
      "\t partial train loss (single batch): 0.100817\n",
      "Epoch 855/1000 train loss: [array(0.10081711, dtype=float32)], val loss: 0.10178228467702866\n",
      "\t partial train loss (single batch): 0.099716\n",
      "Epoch 856/1000 train loss: [array(0.09971585, dtype=float32)], val loss: 0.10193302482366562\n",
      "\t partial train loss (single batch): 0.101702\n",
      "Epoch 857/1000 train loss: [array(0.1017017, dtype=float32)], val loss: 0.10163603723049164\n",
      "\t partial train loss (single batch): 0.101506\n",
      "Epoch 858/1000 train loss: [array(0.10150564, dtype=float32)], val loss: 0.10029594600200653\n",
      "\t partial train loss (single batch): 0.101510\n",
      "Epoch 859/1000 train loss: [array(0.10150977, dtype=float32)], val loss: 0.10054688900709152\n",
      "\t partial train loss (single batch): 0.100961\n",
      "Epoch 860/1000 train loss: [array(0.1009607, dtype=float32)], val loss: 0.10278843343257904\n",
      "\t partial train loss (single batch): 0.101980\n",
      "Epoch 861/1000 train loss: [array(0.10197953, dtype=float32)], val loss: 0.10072343796491623\n",
      "\t partial train loss (single batch): 0.099585\n",
      "Epoch 862/1000 train loss: [array(0.09958537, dtype=float32)], val loss: 0.10075044631958008\n",
      "\t partial train loss (single batch): 0.101286\n",
      "Epoch 863/1000 train loss: [array(0.10128578, dtype=float32)], val loss: 0.10244191437959671\n",
      "\t partial train loss (single batch): 0.100681\n",
      "Epoch 864/1000 train loss: [array(0.10068106, dtype=float32)], val loss: 0.10030991584062576\n",
      "\t partial train loss (single batch): 0.101856\n",
      "Epoch 865/1000 train loss: [array(0.10185607, dtype=float32)], val loss: 0.1016046404838562\n",
      "\t partial train loss (single batch): 0.102063\n",
      "Epoch 866/1000 train loss: [array(0.10206293, dtype=float32)], val loss: 0.10166201740503311\n",
      "\t partial train loss (single batch): 0.100985\n",
      "Epoch 867/1000 train loss: [array(0.10098468, dtype=float32)], val loss: 0.10253605246543884\n",
      "\t partial train loss (single batch): 0.100235\n",
      "Epoch 868/1000 train loss: [array(0.10023546, dtype=float32)], val loss: 0.10237007588148117\n",
      "\t partial train loss (single batch): 0.100444\n",
      "Epoch 869/1000 train loss: [array(0.10044407, dtype=float32)], val loss: 0.10231146216392517\n",
      "\t partial train loss (single batch): 0.100719\n",
      "Epoch 870/1000 train loss: [array(0.10071915, dtype=float32)], val loss: 0.1019587516784668\n",
      "\t partial train loss (single batch): 0.099647\n",
      "Epoch 871/1000 train loss: [array(0.09964696, dtype=float32)], val loss: 0.1010148823261261\n",
      "\t partial train loss (single batch): 0.100873\n",
      "Epoch 872/1000 train loss: [array(0.10087332, dtype=float32)], val loss: 0.10051006823778152\n",
      "\t partial train loss (single batch): 0.101185\n",
      "Epoch 873/1000 train loss: [array(0.10118517, dtype=float32)], val loss: 0.10196923464536667\n",
      "\t partial train loss (single batch): 0.101261\n",
      "Epoch 874/1000 train loss: [array(0.10126097, dtype=float32)], val loss: 0.10123315453529358\n",
      "\t partial train loss (single batch): 0.099948\n",
      "Epoch 875/1000 train loss: [array(0.09994835, dtype=float32)], val loss: 0.1003202497959137\n",
      "\t partial train loss (single batch): 0.100797\n",
      "Epoch 876/1000 train loss: [array(0.10079746, dtype=float32)], val loss: 0.10035280138254166\n",
      "\t partial train loss (single batch): 0.100323\n",
      "Epoch 877/1000 train loss: [array(0.10032325, dtype=float32)], val loss: 0.10159630328416824\n",
      "\t partial train loss (single batch): 0.100720\n",
      "Epoch 878/1000 train loss: [array(0.10072049, dtype=float32)], val loss: 0.10072381049394608\n",
      "\t partial train loss (single batch): 0.099191\n",
      "Epoch 879/1000 train loss: [array(0.09919139, dtype=float32)], val loss: 0.1012304350733757\n",
      "\t partial train loss (single batch): 0.100130\n",
      "Epoch 880/1000 train loss: [array(0.10013013, dtype=float32)], val loss: 0.10128491371870041\n",
      "\t partial train loss (single batch): 0.100846\n",
      "Epoch 881/1000 train loss: [array(0.10084578, dtype=float32)], val loss: 0.10135648399591446\n",
      "\t partial train loss (single batch): 0.100615\n",
      "Epoch 882/1000 train loss: [array(0.10061534, dtype=float32)], val loss: 0.1014244332909584\n",
      "\t partial train loss (single batch): 0.099498\n",
      "Epoch 883/1000 train loss: [array(0.09949818, dtype=float32)], val loss: 0.10078585892915726\n",
      "\t partial train loss (single batch): 0.101530\n",
      "Epoch 884/1000 train loss: [array(0.10153014, dtype=float32)], val loss: 0.10193834453821182\n",
      "\t partial train loss (single batch): 0.099354\n",
      "Epoch 885/1000 train loss: [array(0.09935436, dtype=float32)], val loss: 0.10114006698131561\n",
      "\t partial train loss (single batch): 0.100230\n",
      "Epoch 886/1000 train loss: [array(0.10023049, dtype=float32)], val loss: 0.10183966159820557\n",
      "\t partial train loss (single batch): 0.100582\n",
      "Epoch 887/1000 train loss: [array(0.10058181, dtype=float32)], val loss: 0.101495161652565\n",
      "\t partial train loss (single batch): 0.100368\n",
      "Epoch 888/1000 train loss: [array(0.10036775, dtype=float32)], val loss: 0.10208899527788162\n",
      "\t partial train loss (single batch): 0.100195\n",
      "Epoch 889/1000 train loss: [array(0.1001953, dtype=float32)], val loss: 0.10142739862203598\n",
      "\t partial train loss (single batch): 0.099917\n",
      "Epoch 890/1000 train loss: [array(0.09991743, dtype=float32)], val loss: 0.09983547776937485\n",
      "\t partial train loss (single batch): 0.099448\n",
      "Epoch 891/1000 train loss: [array(0.09944828, dtype=float32)], val loss: 0.10085520893335342\n",
      "\t partial train loss (single batch): 0.100535\n",
      "Epoch 892/1000 train loss: [array(0.10053485, dtype=float32)], val loss: 0.10059933364391327\n",
      "\t partial train loss (single batch): 0.100338\n",
      "Epoch 893/1000 train loss: [array(0.10033831, dtype=float32)], val loss: 0.10099305957555771\n",
      "\t partial train loss (single batch): 0.099500\n",
      "Epoch 894/1000 train loss: [array(0.09950028, dtype=float32)], val loss: 0.10120794177055359\n",
      "\t partial train loss (single batch): 0.100567\n",
      "Epoch 895/1000 train loss: [array(0.10056714, dtype=float32)], val loss: 0.10054474323987961\n",
      "\t partial train loss (single batch): 0.100485\n",
      "Epoch 896/1000 train loss: [array(0.10048545, dtype=float32)], val loss: 0.1004548892378807\n",
      "\t partial train loss (single batch): 0.100752\n",
      "Epoch 897/1000 train loss: [array(0.10075229, dtype=float32)], val loss: 0.10144181549549103\n",
      "\t partial train loss (single batch): 0.099787\n",
      "Epoch 898/1000 train loss: [array(0.09978704, dtype=float32)], val loss: 0.10088283568620682\n",
      "\t partial train loss (single batch): 0.099011\n",
      "Epoch 899/1000 train loss: [array(0.09901104, dtype=float32)], val loss: 0.10147696733474731\n",
      "\t partial train loss (single batch): 0.099870\n",
      "Epoch 900/1000 train loss: [array(0.0998698, dtype=float32)], val loss: 0.1003475934267044\n",
      "\t partial train loss (single batch): 0.101072\n",
      "Epoch 901/1000 train loss: [array(0.10107167, dtype=float32)], val loss: 0.09944628924131393\n",
      "\t partial train loss (single batch): 0.099996\n",
      "Epoch 902/1000 train loss: [array(0.0999962, dtype=float32)], val loss: 0.10047760605812073\n",
      "\t partial train loss (single batch): 0.100846\n",
      "Epoch 903/1000 train loss: [array(0.10084625, dtype=float32)], val loss: 0.10105728358030319\n",
      "\t partial train loss (single batch): 0.099859\n",
      "Epoch 904/1000 train loss: [array(0.09985913, dtype=float32)], val loss: 0.1001543253660202\n",
      "\t partial train loss (single batch): 0.099405\n",
      "Epoch 905/1000 train loss: [array(0.09940522, dtype=float32)], val loss: 0.10028692334890366\n",
      "\t partial train loss (single batch): 0.100984\n",
      "Epoch 906/1000 train loss: [array(0.10098435, dtype=float32)], val loss: 0.09990077465772629\n",
      "\t partial train loss (single batch): 0.099693\n",
      "Epoch 907/1000 train loss: [array(0.09969293, dtype=float32)], val loss: 0.10064508765935898\n",
      "\t partial train loss (single batch): 0.099568\n",
      "Epoch 908/1000 train loss: [array(0.09956827, dtype=float32)], val loss: 0.10125739127397537\n",
      "\t partial train loss (single batch): 0.100237\n",
      "Epoch 909/1000 train loss: [array(0.10023668, dtype=float32)], val loss: 0.10009647905826569\n",
      "\t partial train loss (single batch): 0.100798\n",
      "Epoch 910/1000 train loss: [array(0.10079796, dtype=float32)], val loss: 0.10063711553812027\n",
      "\t partial train loss (single batch): 0.099239\n",
      "Epoch 911/1000 train loss: [array(0.09923908, dtype=float32)], val loss: 0.0995197519659996\n",
      "\t partial train loss (single batch): 0.101054\n",
      "Epoch 912/1000 train loss: [array(0.10105403, dtype=float32)], val loss: 0.10104554146528244\n",
      "\t partial train loss (single batch): 0.098918\n",
      "Epoch 913/1000 train loss: [array(0.09891769, dtype=float32)], val loss: 0.10106638073921204\n",
      "\t partial train loss (single batch): 0.099332\n",
      "Epoch 914/1000 train loss: [array(0.09933242, dtype=float32)], val loss: 0.0999637097120285\n",
      "\t partial train loss (single batch): 0.098092\n",
      "Epoch 915/1000 train loss: [array(0.09809215, dtype=float32)], val loss: 0.09938984364271164\n",
      "\t partial train loss (single batch): 0.099506\n",
      "Epoch 916/1000 train loss: [array(0.0995063, dtype=float32)], val loss: 0.10088975727558136\n",
      "\t partial train loss (single batch): 0.099505\n",
      "Epoch 917/1000 train loss: [array(0.09950473, dtype=float32)], val loss: 0.10011685639619827\n",
      "\t partial train loss (single batch): 0.098984\n",
      "Epoch 918/1000 train loss: [array(0.09898427, dtype=float32)], val loss: 0.09835386276245117\n",
      "\t partial train loss (single batch): 0.099521\n",
      "Epoch 919/1000 train loss: [array(0.09952094, dtype=float32)], val loss: 0.10022643208503723\n",
      "\t partial train loss (single batch): 0.099105\n",
      "Epoch 920/1000 train loss: [array(0.09910502, dtype=float32)], val loss: 0.10098723322153091\n",
      "\t partial train loss (single batch): 0.100220\n",
      "Epoch 921/1000 train loss: [array(0.10022014, dtype=float32)], val loss: 0.10119035840034485\n",
      "\t partial train loss (single batch): 0.099323\n",
      "Epoch 922/1000 train loss: [array(0.09932292, dtype=float32)], val loss: 0.10104652494192123\n",
      "\t partial train loss (single batch): 0.099454\n",
      "Epoch 923/1000 train loss: [array(0.0994542, dtype=float32)], val loss: 0.10096371918916702\n",
      "\t partial train loss (single batch): 0.099688\n",
      "Epoch 924/1000 train loss: [array(0.09968787, dtype=float32)], val loss: 0.09846030920743942\n",
      "\t partial train loss (single batch): 0.098656\n",
      "Epoch 925/1000 train loss: [array(0.09865558, dtype=float32)], val loss: 0.10071541368961334\n",
      "\t partial train loss (single batch): 0.099921\n",
      "Epoch 926/1000 train loss: [array(0.09992059, dtype=float32)], val loss: 0.100681833922863\n",
      "\t partial train loss (single batch): 0.098803\n",
      "Epoch 927/1000 train loss: [array(0.09880302, dtype=float32)], val loss: 0.09922382980585098\n",
      "\t partial train loss (single batch): 0.099513\n",
      "Epoch 928/1000 train loss: [array(0.09951339, dtype=float32)], val loss: 0.10061127692461014\n",
      "\t partial train loss (single batch): 0.100339\n",
      "Epoch 929/1000 train loss: [array(0.10033942, dtype=float32)], val loss: 0.099135622382164\n",
      "\t partial train loss (single batch): 0.098941\n",
      "Epoch 930/1000 train loss: [array(0.09894072, dtype=float32)], val loss: 0.10002285242080688\n",
      "\t partial train loss (single batch): 0.099894\n",
      "Epoch 931/1000 train loss: [array(0.09989414, dtype=float32)], val loss: 0.10002294182777405\n",
      "\t partial train loss (single batch): 0.098605\n",
      "Epoch 932/1000 train loss: [array(0.09860504, dtype=float32)], val loss: 0.10057245939970016\n",
      "\t partial train loss (single batch): 0.098862\n",
      "Epoch 933/1000 train loss: [array(0.09886162, dtype=float32)], val loss: 0.10052653402090073\n",
      "\t partial train loss (single batch): 0.099174\n",
      "Epoch 934/1000 train loss: [array(0.09917396, dtype=float32)], val loss: 0.09929200261831284\n",
      "\t partial train loss (single batch): 0.099740\n",
      "Epoch 935/1000 train loss: [array(0.09974013, dtype=float32)], val loss: 0.09983602166175842\n",
      "\t partial train loss (single batch): 0.099032\n",
      "Epoch 936/1000 train loss: [array(0.09903211, dtype=float32)], val loss: 0.10051529109477997\n",
      "\t partial train loss (single batch): 0.100032\n",
      "Epoch 937/1000 train loss: [array(0.10003233, dtype=float32)], val loss: 0.10066526383161545\n",
      "\t partial train loss (single batch): 0.100726\n",
      "Epoch 938/1000 train loss: [array(0.10072608, dtype=float32)], val loss: 0.10019635409116745\n",
      "\t partial train loss (single batch): 0.098924\n",
      "Epoch 939/1000 train loss: [array(0.09892392, dtype=float32)], val loss: 0.09951949119567871\n",
      "\t partial train loss (single batch): 0.096883\n",
      "Epoch 940/1000 train loss: [array(0.09688338, dtype=float32)], val loss: 0.0987032875418663\n",
      "\t partial train loss (single batch): 0.098721\n",
      "Epoch 941/1000 train loss: [array(0.09872072, dtype=float32)], val loss: 0.09980972111225128\n",
      "\t partial train loss (single batch): 0.098983\n",
      "Epoch 942/1000 train loss: [array(0.09898336, dtype=float32)], val loss: 0.10004335641860962\n",
      "\t partial train loss (single batch): 0.099482\n",
      "Epoch 943/1000 train loss: [array(0.09948239, dtype=float32)], val loss: 0.099797323346138\n",
      "\t partial train loss (single batch): 0.099461\n",
      "Epoch 944/1000 train loss: [array(0.09946107, dtype=float32)], val loss: 0.10022251307964325\n",
      "\t partial train loss (single batch): 0.098824\n",
      "Epoch 945/1000 train loss: [array(0.09882439, dtype=float32)], val loss: 0.10028118640184402\n",
      "\t partial train loss (single batch): 0.099535\n",
      "Epoch 946/1000 train loss: [array(0.09953506, dtype=float32)], val loss: 0.09973588585853577\n",
      "\t partial train loss (single batch): 0.099130\n",
      "Epoch 947/1000 train loss: [array(0.09913019, dtype=float32)], val loss: 0.0990089401602745\n",
      "\t partial train loss (single batch): 0.098792\n",
      "Epoch 948/1000 train loss: [array(0.09879217, dtype=float32)], val loss: 0.10115867108106613\n",
      "\t partial train loss (single batch): 0.098941\n",
      "Epoch 949/1000 train loss: [array(0.09894117, dtype=float32)], val loss: 0.1009531319141388\n",
      "\t partial train loss (single batch): 0.098407\n",
      "Epoch 950/1000 train loss: [array(0.09840722, dtype=float32)], val loss: 0.1001378744840622\n",
      "\t partial train loss (single batch): 0.098769\n",
      "Epoch 951/1000 train loss: [array(0.09876899, dtype=float32)], val loss: 0.0999167189002037\n",
      "\t partial train loss (single batch): 0.098998\n",
      "Epoch 952/1000 train loss: [array(0.09899792, dtype=float32)], val loss: 0.09937941282987595\n",
      "\t partial train loss (single batch): 0.097737\n",
      "Epoch 953/1000 train loss: [array(0.09773671, dtype=float32)], val loss: 0.09920177608728409\n",
      "\t partial train loss (single batch): 0.098900\n",
      "Epoch 954/1000 train loss: [array(0.0988998, dtype=float32)], val loss: 0.09873636811971664\n",
      "\t partial train loss (single batch): 0.098545\n",
      "Epoch 955/1000 train loss: [array(0.09854469, dtype=float32)], val loss: 0.09984761476516724\n",
      "\t partial train loss (single batch): 0.099126\n",
      "Epoch 956/1000 train loss: [array(0.0991258, dtype=float32)], val loss: 0.09880601614713669\n",
      "\t partial train loss (single batch): 0.099183\n",
      "Epoch 957/1000 train loss: [array(0.09918308, dtype=float32)], val loss: 0.0998663604259491\n",
      "\t partial train loss (single batch): 0.099643\n",
      "Epoch 958/1000 train loss: [array(0.09964259, dtype=float32)], val loss: 0.10037803649902344\n",
      "\t partial train loss (single batch): 0.098594\n",
      "Epoch 959/1000 train loss: [array(0.09859353, dtype=float32)], val loss: 0.09886493533849716\n",
      "\t partial train loss (single batch): 0.098556\n",
      "Epoch 960/1000 train loss: [array(0.09855644, dtype=float32)], val loss: 0.10016071796417236\n",
      "\t partial train loss (single batch): 0.098747\n",
      "Epoch 961/1000 train loss: [array(0.09874683, dtype=float32)], val loss: 0.10043632984161377\n",
      "\t partial train loss (single batch): 0.097879\n",
      "Epoch 962/1000 train loss: [array(0.09787899, dtype=float32)], val loss: 0.09928605705499649\n",
      "\t partial train loss (single batch): 0.099753\n",
      "Epoch 963/1000 train loss: [array(0.09975335, dtype=float32)], val loss: 0.10018710792064667\n",
      "\t partial train loss (single batch): 0.098153\n",
      "Epoch 964/1000 train loss: [array(0.09815286, dtype=float32)], val loss: 0.09905143082141876\n",
      "\t partial train loss (single batch): 0.099261\n",
      "Epoch 965/1000 train loss: [array(0.09926115, dtype=float32)], val loss: 0.09839969873428345\n",
      "\t partial train loss (single batch): 0.099522\n",
      "Epoch 966/1000 train loss: [array(0.09952244, dtype=float32)], val loss: 0.09896884113550186\n",
      "\t partial train loss (single batch): 0.098247\n",
      "Epoch 967/1000 train loss: [array(0.09824695, dtype=float32)], val loss: 0.10023914277553558\n",
      "\t partial train loss (single batch): 0.099701\n",
      "Epoch 968/1000 train loss: [array(0.09970105, dtype=float32)], val loss: 0.0995456650853157\n",
      "\t partial train loss (single batch): 0.098670\n",
      "Epoch 969/1000 train loss: [array(0.09867015, dtype=float32)], val loss: 0.10050275921821594\n",
      "\t partial train loss (single batch): 0.097993\n",
      "Epoch 970/1000 train loss: [array(0.09799272, dtype=float32)], val loss: 0.0989643856883049\n",
      "\t partial train loss (single batch): 0.098672\n",
      "Epoch 971/1000 train loss: [array(0.09867202, dtype=float32)], val loss: 0.09842239320278168\n",
      "\t partial train loss (single batch): 0.098743\n",
      "Epoch 972/1000 train loss: [array(0.09874265, dtype=float32)], val loss: 0.0996415764093399\n",
      "\t partial train loss (single batch): 0.099550\n",
      "Epoch 973/1000 train loss: [array(0.09954959, dtype=float32)], val loss: 0.09769966453313828\n",
      "\t partial train loss (single batch): 0.098162\n",
      "Epoch 974/1000 train loss: [array(0.09816227, dtype=float32)], val loss: 0.09998930990695953\n",
      "\t partial train loss (single batch): 0.097431\n",
      "Epoch 975/1000 train loss: [array(0.09743069, dtype=float32)], val loss: 0.09975776076316833\n",
      "\t partial train loss (single batch): 0.098829\n",
      "Epoch 976/1000 train loss: [array(0.0988289, dtype=float32)], val loss: 0.09852961450815201\n",
      "\t partial train loss (single batch): 0.098151\n",
      "Epoch 977/1000 train loss: [array(0.09815083, dtype=float32)], val loss: 0.09923716634511948\n",
      "\t partial train loss (single batch): 0.098567\n",
      "Epoch 978/1000 train loss: [array(0.09856702, dtype=float32)], val loss: 0.0997215211391449\n",
      "\t partial train loss (single batch): 0.098037\n",
      "Epoch 979/1000 train loss: [array(0.09803682, dtype=float32)], val loss: 0.0988960713148117\n",
      "\t partial train loss (single batch): 0.098403\n",
      "Epoch 980/1000 train loss: [array(0.09840313, dtype=float32)], val loss: 0.09912476688623428\n",
      "\t partial train loss (single batch): 0.098719\n",
      "Epoch 981/1000 train loss: [array(0.09871904, dtype=float32)], val loss: 0.09953885525465012\n",
      "\t partial train loss (single batch): 0.099173\n",
      "Epoch 982/1000 train loss: [array(0.09917346, dtype=float32)], val loss: 0.09848833084106445\n",
      "\t partial train loss (single batch): 0.098026\n",
      "Epoch 983/1000 train loss: [array(0.09802587, dtype=float32)], val loss: 0.09870868176221848\n",
      "\t partial train loss (single batch): 0.099684\n",
      "Epoch 984/1000 train loss: [array(0.09968374, dtype=float32)], val loss: 0.09925217181444168\n",
      "\t partial train loss (single batch): 0.098919\n",
      "Epoch 985/1000 train loss: [array(0.0989186, dtype=float32)], val loss: 0.09968504309654236\n",
      "\t partial train loss (single batch): 0.098242\n",
      "Epoch 986/1000 train loss: [array(0.09824236, dtype=float32)], val loss: 0.09860948473215103\n",
      "\t partial train loss (single batch): 0.097907\n",
      "Epoch 987/1000 train loss: [array(0.09790657, dtype=float32)], val loss: 0.09871106594800949\n",
      "\t partial train loss (single batch): 0.098349\n",
      "Epoch 988/1000 train loss: [array(0.09834854, dtype=float32)], val loss: 0.09989745169878006\n",
      "\t partial train loss (single batch): 0.099116\n",
      "Epoch 989/1000 train loss: [array(0.0991156, dtype=float32)], val loss: 0.09882257878780365\n",
      "\t partial train loss (single batch): 0.098263\n",
      "Epoch 990/1000 train loss: [array(0.09826336, dtype=float32)], val loss: 0.09889422357082367\n",
      "\t partial train loss (single batch): 0.098224\n",
      "Epoch 991/1000 train loss: [array(0.09822424, dtype=float32)], val loss: 0.09864950180053711\n",
      "\t partial train loss (single batch): 0.097968\n",
      "Epoch 992/1000 train loss: [array(0.09796804, dtype=float32)], val loss: 0.09879078716039658\n",
      "\t partial train loss (single batch): 0.097123\n",
      "Epoch 993/1000 train loss: [array(0.09712343, dtype=float32)], val loss: 0.09823683649301529\n",
      "\t partial train loss (single batch): 0.096940\n",
      "Epoch 994/1000 train loss: [array(0.09694014, dtype=float32)], val loss: 0.09846657514572144\n",
      "\t partial train loss (single batch): 0.098716\n",
      "Epoch 995/1000 train loss: [array(0.09871642, dtype=float32)], val loss: 0.09760623425245285\n",
      "\t partial train loss (single batch): 0.099567\n",
      "Epoch 996/1000 train loss: [array(0.09956688, dtype=float32)], val loss: 0.09884417802095413\n",
      "\t partial train loss (single batch): 0.098763\n",
      "Epoch 997/1000 train loss: [array(0.09876312, dtype=float32)], val loss: 0.09857306629419327\n",
      "\t partial train loss (single batch): 0.098944\n",
      "Epoch 998/1000 train loss: [array(0.09894366, dtype=float32)], val loss: 0.09761134535074234\n",
      "\t partial train loss (single batch): 0.098282\n",
      "Epoch 999/1000 train loss: [array(0.09828224, dtype=float32)], val loss: 0.09947266429662704\n",
      "\t partial train loss (single batch): 0.098734\n",
      "Epoch 1000/1000 train loss: [array(0.0987343, dtype=float32)], val loss: 0.09841986745595932\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import random \n",
    "num_epochs = 1000\n",
    "history={'train_loss':[],'val_loss':[]}\n",
    "for epoch in range(num_epochs):\n",
    "   img_train, labels_train = generator.get_random_batch(training=True)\n",
    "   img_test, labels_test = generator.get_random_batch(training=False)\n",
    "\n",
    "   img_train = np.expand_dims(img_train, axis=-1)\n",
    "   img_train = np.swapaxes(img_train, 1, -1)\n",
    "   img_test = np.expand_dims(img_test, axis=-1)\n",
    "   img_test = np.swapaxes(img_test, 1, -1)\n",
    "   img_train = torch.tensor(img_train)\n",
    "   img_train = img_train.type(torch.FloatTensor)\n",
    "   img_test = torch.tensor(img_test)\n",
    "   img_test = img_test.type(torch.FloatTensor)\n",
    "   train_loss = train_epoch(encoder,decoder,loss_fn,optim, img_train)\n",
    "   val_loss = test_epoch(encoder,decoder,loss_fn, img_test)\n",
    "   print(f\"Epoch {epoch+1}/{num_epochs} train loss: {train_loss}, val loss: {val_loss}\")\n",
    "   #print('\\n EPOCH {}/{} \\t train loss {:.3f} \\t val loss {:.3f}'.format(epoch + 1, num_epochs,train_loss,val_loss))\n",
    "   history['train_loss'].append(train_loss)\n",
    "   history['val_loss'].append(val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "plt.semilogy(history['train_loss'], label='Train')\n",
    "plt.semilogy(history['val_loss'], label='Valid')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Average Loss')\n",
    "#plt.grid()\n",
    "plt.legend()\n",
    "#plt.title('loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 1, 28, 28, 3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fadd473e370>"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8/fFQqAAAACXBIWXMAAAsTAAALEwEAmpwYAAAKW0lEQVR4nO3dT4ic933H8fentqyAkoLU1EY4pkmDKTWFKmVRCy4lxTg4vsg5tESHoIJBOcSQQA416aE+mtIk9FACSi2iltShkBjrYNoIETCBYrw2qi1XbeUaJVEkpAYf4hQqy863h31cNvKudjzzzB/6fb9gmJlnnt3ny7Dvnb/wS1Uh6f+/X1r2AJIWw9ilJoxdasLYpSaMXWri1kUe7LbsrvexZ5GHlFr5H/6bN+tatrptptiTPAD8FXAL8DdV9fjN9n8fe/jd3DfLISXdxHN1etvbpn4an+QW4K+BTwL3AIeT3DPt75M0X7O8Zj8IvFpVr1XVm8C3gEPjjCVpbLPEfifwo03XLw7bfkGSo0nWk6xf59oMh5M0i1li3+pNgHd997aqjlXVWlWt7WL3DIeTNItZYr8I3LXp+oeAS7ONI2leZon9eeDuJB9JchvwaeDkOGNJGtvUH71V1VtJHgH+iY2P3o5X1SujTSZpVDN9zl5VzwDPjDSLpDny67JSE8YuNWHsUhPGLjVh7FITxi41YexSE8YuNWHsUhPGLjVh7FITxi41YexSE8YuNWHsUhPGLjVh7FITxi41YexSE8YuNWHsUhPGLjVh7FITxi41YexSE8YuNWHsUhPGLjVh7FITxi41MdOSzUkuAG8AbwNvVdXaGENJGt9MsQ/+sKp+MsLvkTRHPo2Xmpg19gK+m+SFJEe32iHJ0STrSdavc23Gw0ma1qxP4++tqktJbgdOJfm3qnp28w5VdQw4BvDL2VczHk/SlGZ6ZK+qS8P5VeAp4OAYQ0ka39SxJ9mT5APvXAY+AZwdazBJ45rlafwdwFNJ3vk9f19V/zjKVJJGN3XsVfUa8NsjziJpjvzoTWrC2KUmjF1qwtilJoxdasLYpSaMXWrC2KUmjF1qwtilJoxdasLYpSaMXWrC2KUmjF1qwtilJoxdasLYpSaMXWrC2KUmjF1qwtilJoxdasLYpSaMXWrC2KUmjF1qwtilJoxdasLYpSZ2jD3J8SRXk5zdtG1fklNJzg/ne+c7pqRZTfLI/g3ggRu2PQqcrqq7gdPDdUkrbMfYq+pZ4PUbNh8CTgyXTwAPjTuWpLFN+5r9jqq6DDCc377djkmOJllPsn6da1MeTtKs5v4GXVUdq6q1qlrbxe55H07SNqaN/UqS/QDD+dXxRpI0D9PGfhI4Mlw+Ajw9zjiS5mWSj96eBP4Z+I0kF5M8DDwO3J/kPHD/cF3SCrt1px2q6vA2N9038iyS5shv0ElNGLvUhLFLTRi71ISxS00Yu9SEsUtNGLvUhLFLTRi71ISxS00Yu9SEsUtNGLvUhLFLTRi71ISxS00Yu9SEsUtNGLvUhLFLTRi71ISxS00Yu9SEsUtNGLvUhLFLTRi71ISxS00Yu9TEJOuzH09yNcnZTdseS/LjJGeG04PzHVPSrCZ5ZP8G8MAW279aVQeG0zPjjiVpbDvGXlXPAq8vYBZJczTLa/ZHkrw0PM3fu91OSY4mWU+yfp1rMxxO0iymjf1rwEeBA8Bl4Mvb7VhVx6pqrarWdrF7ysNJmtVUsVfVlap6u6p+DnwdODjuWJLGNlXsSfZvuvop4Ox2+0paDbfutEOSJ4GPAx9MchH4c+DjSQ4ABVwAPju/ESWNYcfYq+rwFpufmMMskubIb9BJTRi71ISxS00Yu9SEsUtNGLvUhLFLTRi71ISxS00Yu9SEsUtNGLvUhLFLTRi71ISxS00Yu9SEsUtNGLvUhLFLTRi71ISxS00Yu9SEsUtNGLvUhLFLTRi71ISxS00Yu9SEsUtNGLvUxI6xJ7kryfeSnEvySpLPD9v3JTmV5Pxwvnf+40qa1iSP7G8BX6yq3wR+D/hcknuAR4HTVXU3cHq4LmlF7Rh7VV2uqheHy28A54A7gUPAiWG3E8BDc5pR0gje02v2JB8GPgY8B9xRVZdh4x8CcPs2P3M0yXqS9etcm3FcSdOaOPYk7we+DXyhqn466c9V1bGqWquqtV3snmZGSSOYKPYku9gI/ZtV9Z1h85Uk+4fb9wNX5zOipDFM8m58gCeAc1X1lU03nQSODJePAE+PP56ksdw6wT73Ap8BXk5yZtj2JeBx4B+SPAz8EPijuUwoaRQ7xl5V3weyzc33jTuOpHnxG3RSE8YuNWHsUhPGLjVh7FITxi41YexSE8YuNWHsUhPGLjVh7FITxi41YexSE8YuNWHsUhPGLjVh7FITxi41YexSE8YuNWHsUhPGLjVh7FITxi41YexSE8YuNWHsUhPGLjVh7FITk6zPfleS7yU5l+SVJJ8ftj+W5MdJzgynB+c/rqRpTbI++1vAF6vqxSQfAF5Icmq47atV9ZfzG0/SWCZZn/0ycHm4/EaSc8Cd8x5M0rje02v2JB8GPgY8N2x6JMlLSY4n2bvNzxxNsp5k/TrXZptW0tQmjj3J+4FvA1+oqp8CXwM+Chxg45H/y1v9XFUdq6q1qlrbxe7ZJ5Y0lYliT7KLjdC/WVXfAaiqK1X1dlX9HPg6cHB+Y0qa1STvxgd4AjhXVV/ZtH3/pt0+BZwdfzxJY5nk3fh7gc8ALyc5M2z7EnA4yQGggAvAZ+cwn6SRTPJu/PeBbHHTM+OPI2le/Aad1ISxS00Yu9SEsUtNGLvUhLFLTRi71ISxS00Yu9SEsUtNGLvUhLFLTRi71ISxS02kqhZ3sOS/gB9s2vRB4CcLG+C9WdXZVnUucLZpjTnbr1XVr251w0Jjf9fBk/WqWlvaADexqrOt6lzgbNNa1Gw+jZeaMHapiWXHfmzJx7+ZVZ1tVecCZ5vWQmZb6mt2SYuz7Ed2SQti7FITS4k9yQNJ/j3Jq0keXcYM20lyIcnLwzLU60ue5XiSq0nObtq2L8mpJOeH8y3X2FvSbCuxjPdNlhlf6n237OXPF/6aPcktwH8A9wMXgeeBw1X1rwsdZBtJLgBrVbX0L2Ak+QPgZ8DfVtVvDdv+Ani9qh4f/lHurao/XZHZHgN+tuxlvIfVivZvXmYceAj4E5Z4391krj9mAffbMh7ZDwKvVtVrVfUm8C3g0BLmWHlV9Szw+g2bDwEnhssn2PhjWbhtZlsJVXW5ql4cLr8BvLPM+FLvu5vMtRDLiP1O4Eebrl9ktdZ7L+C7SV5IcnTZw2zhjqq6DBt/PMDtS57nRjsu471INywzvjL33TTLn89qGbFvtZTUKn3+d29V/Q7wSeBzw9NVTWaiZbwXZYtlxlfCtMufz2oZsV8E7tp0/UPApSXMsaWqujScXwWeYvWWor7yzgq6w/nVJc/zf1ZpGe+tlhlnBe67ZS5/vozYnwfuTvKRJLcBnwZOLmGOd0myZ3jjhCR7gE+wektRnwSODJePAE8vcZZfsCrLeG+3zDhLvu+Wvvx5VS38BDzIxjvy/wn82TJm2GauXwf+ZTi9suzZgCfZeFp3nY1nRA8DvwKcBs4P5/tWaLa/A14GXmIjrP1Lmu332Xhp+BJwZjg9uOz77iZzLeR+8+uyUhN+g05qwtilJoxdasLYpSaMXWrC2KUmjF1q4n8Bnt88ZNzdTi4AAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "z = np.random.randn(100, d)\n",
    "z = torch.Tensor(z)\n",
    "with torch.no_grad():\n",
    "    x_hat = decoder(z)\n",
    "print(x_hat.shape)\n",
    "x = np.array(x_hat[0])\n",
    "x = x[:, :, :, 2]\n",
    "x = x.reshape((28, 28))\n",
    "#x = np.swapaxes(x, 0, 1)\n",
    "#x = np.swapaxes(x, 1, 2)\n",
    "\n",
    "plt.imshow(x)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "49ff9a1d55cbad36b515c3ded8837e12145fab330794be4b4ac6e95d3772d975"
  },
  "kernelspec": {
   "display_name": "Python 3.8.0 ('tensorflow')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "from stacked_mnist import StackedMNISTData, DataMode\n",
    "from tensorflow import keras\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, encoded_space_dim, fc2_input_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        ### Convolutional section\n",
    "        self.encoder_cnn = nn.Sequential(\n",
    "            # First convolutional layer\n",
    "            nn.Conv2d(3, 9, 3, stride=2, padding=1, groups=3),\n",
    "            # nn.BatchNorm2d(8),\n",
    "            nn.ReLU(True),\n",
    "            # Second convolutional layer\n",
    "            nn.Conv2d(9, 18, 3, stride=2, padding=1, groups=3),\n",
    "            nn.BatchNorm2d(18),\n",
    "            nn.ReLU(True),\n",
    "            # Third convolutional layer\n",
    "            nn.Conv2d(18, 36, 3, stride=2, padding=0, groups=3),\n",
    "            # nn.BatchNorm2d(32),\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "\n",
    "        ### Flatten layer\n",
    "        self.flatten = nn.Flatten(start_dim=1)\n",
    "\n",
    "        ### Linear section\n",
    "        self.encoder_lin = nn.Sequential(\n",
    "            # First linear layer\n",
    "            nn.Linear(36*3*3, 128),\n",
    "            nn.ReLU(True),\n",
    "            # Second linear layer\n",
    "            nn.Linear(128, encoded_space_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply convolutions\n",
    "        x = self.encoder_cnn(x)\n",
    "        # Flatten\n",
    "        x = self.flatten(x)\n",
    "        # # Apply linear layers\n",
    "        x = self.encoder_lin(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, encoded_space_dim,fc2_input_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        ### Linear section\n",
    "        self.decoder_lin = nn.Sequential(\n",
    "            # First linear layer\n",
    "            nn.Linear(encoded_space_dim, 128),\n",
    "            nn.ReLU(True),\n",
    "            # Second linear layer\n",
    "            nn.Linear(128, 36*3*3),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "\n",
    "        ### Unflatten\n",
    "        self.unflatten = nn.Unflatten(dim=1, unflattened_size=(36, 3, 3))\n",
    "\n",
    "        ### Convolutional section\n",
    "        self.decoder_conv = nn.Sequential(\n",
    "            # First transposed convolution\n",
    "            nn.ConvTranspose2d(36, 18, 3, stride=2, padding=0, output_padding=0, groups=3),\n",
    "            nn.BatchNorm2d(18),\n",
    "            nn.ReLU(True),\n",
    "            # Second transposed convolution\n",
    "            nn.ConvTranspose2d(18, 9, 3, stride=2, padding=1, output_padding=1, groups=3),\n",
    "            nn.BatchNorm2d(9),\n",
    "            nn.ReLU(True),\n",
    "            # Third transposed convolution\n",
    "            nn.ConvTranspose2d(9, 3, 3, stride=2, padding=1, output_padding=1, groups=3)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Apply linear layers\n",
    "        x = self.decoder_lin(x)\n",
    "        # Unflatten\n",
    "        x = self.unflatten(x)\n",
    "        # Apply transposed convolutions\n",
    "        x = self.decoder_conv(x)\n",
    "        # Apply a sigmoid to force the output to be between 0 and 1 (valid pixel values)\n",
    "        x = torch.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1            [-1, 9, 14, 14]              90\n",
      "              ReLU-2            [-1, 9, 14, 14]               0\n",
      "            Conv2d-3             [-1, 18, 7, 7]             504\n",
      "       BatchNorm2d-4             [-1, 18, 7, 7]              36\n",
      "              ReLU-5             [-1, 18, 7, 7]               0\n",
      "            Conv2d-6             [-1, 36, 3, 3]           1,980\n",
      "              ReLU-7             [-1, 36, 3, 3]               0\n",
      "           Flatten-8                  [-1, 324]               0\n",
      "            Linear-9                  [-1, 128]          41,600\n",
      "             ReLU-10                  [-1, 128]               0\n",
      "           Linear-11                   [-1, 24]           3,096\n",
      "================================================================\n",
      "Total params: 47,306\n",
      "Trainable params: 47,306\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 0.06\n",
      "Params size (MB): 0.18\n",
      "Estimated Total Size (MB): 0.25\n",
      "----------------------------------------------------------------\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1                  [-1, 128]           3,200\n",
      "              ReLU-2                  [-1, 128]               0\n",
      "            Linear-3                  [-1, 324]          41,796\n",
      "              ReLU-4                  [-1, 324]               0\n",
      "         Unflatten-5             [-1, 36, 3, 3]               0\n",
      "   ConvTranspose2d-6             [-1, 18, 7, 7]           1,962\n",
      "       BatchNorm2d-7             [-1, 18, 7, 7]              36\n",
      "              ReLU-8             [-1, 18, 7, 7]               0\n",
      "   ConvTranspose2d-9            [-1, 9, 14, 14]             495\n",
      "      BatchNorm2d-10            [-1, 9, 14, 14]              18\n",
      "             ReLU-11            [-1, 9, 14, 14]               0\n",
      "  ConvTranspose2d-12            [-1, 3, 28, 28]              84\n",
      "================================================================\n",
      "Total params: 47,591\n",
      "Trainable params: 47,591\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.09\n",
      "Params size (MB): 0.18\n",
      "Estimated Total Size (MB): 0.27\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "d = 24\n",
    "encoder = Encoder(encoded_space_dim=d,fc2_input_dim=128)\n",
    "decoder = Decoder(encoded_space_dim=d,fc2_input_dim=128)\n",
    "#print(encoder)\n",
    "#print(decoder)\n",
    "summary(encoder, (3, 28, 28))\n",
    "summary(decoder, (24,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.MSELoss()\n",
    "lr= 0.001\n",
    "params_to_optimize = [\n",
    "    {'params': encoder.parameters()},\n",
    "    {'params': decoder.parameters()}\n",
    "]\n",
    "optim = torch.optim.Adam(params_to_optimize, lr=lr, weight_decay=1e-05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Training function\n",
    "def train_epoch(encoder, decoder, loss_fn, optimizer, x):\n",
    "    # Set train mode for both the encoder and the decoder\n",
    "    encoder.train()\n",
    "    decoder.train()\n",
    "    train_loss = []\n",
    "\n",
    "    # Encode data\n",
    "    encoded_data = encoder(x)\n",
    "    # Decode data\n",
    "    decoded_data = decoder(encoded_data)\n",
    "    # Evaluate loss\n",
    "    loss = loss_fn(decoded_data, x)\n",
    "    # Backward pass\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    # Print batch loss\n",
    "    print('\\t partial train loss (single batch): %f' % (loss.data))\n",
    "    train_loss.append(loss.detach().numpy())\n",
    "\n",
    "    return train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Testing function\n",
    "def test_epoch(encoder, decoder, loss_fn, x):\n",
    "    # Set evaluation mode for encoder and decoder\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    with torch.no_grad(): # No need to track the gradients\n",
    "        # Define the lists to store the outputs for each batch\n",
    "        conc_out = []\n",
    "        conc_label = []\n",
    "        # Encode data\n",
    "        encoded_data = encoder(x)\n",
    "        # Decode data\n",
    "        decoded_data = decoder(encoded_data)\n",
    "        # Append the network output and the original image to the lists\n",
    "        conc_out.append(decoded_data)\n",
    "        conc_label.append(x)\n",
    "        # Create a single tensor with all the values in the lists\n",
    "        conc_out = torch.cat(conc_out)\n",
    "        conc_label = torch.cat(conc_label) \n",
    "        # Evaluate global loss\n",
    "        val_loss = loss_fn(conc_out, conc_label)\n",
    "    return val_loss.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 3)\n",
      "(10000, 3)\n"
     ]
    }
   ],
   "source": [
    "generator = StackedMNISTData(mode=DataMode.COLOR_BINARY_COMPLETE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t partial train loss (single batch): 0.253998\n",
      "Epoch 1/1000 train loss: [array(0.2539975, dtype=float32)], val loss: 0.2581236660480499\n",
      "\t partial train loss (single batch): 0.251231\n",
      "Epoch 2/1000 train loss: [array(0.25123066, dtype=float32)], val loss: 0.25819259881973267\n",
      "\t partial train loss (single batch): 0.248914\n",
      "Epoch 3/1000 train loss: [array(0.24891435, dtype=float32)], val loss: 0.2581523358821869\n",
      "\t partial train loss (single batch): 0.247481\n",
      "Epoch 4/1000 train loss: [array(0.24748072, dtype=float32)], val loss: 0.2580978274345398\n",
      "\t partial train loss (single batch): 0.245662\n",
      "Epoch 5/1000 train loss: [array(0.24566217, dtype=float32)], val loss: 0.2586856186389923\n",
      "\t partial train loss (single batch): 0.244046\n",
      "Epoch 6/1000 train loss: [array(0.2440463, dtype=float32)], val loss: 0.2579525113105774\n",
      "\t partial train loss (single batch): 0.242697\n",
      "Epoch 7/1000 train loss: [array(0.24269661, dtype=float32)], val loss: 0.25791648030281067\n",
      "\t partial train loss (single batch): 0.241242\n",
      "Epoch 8/1000 train loss: [array(0.24124181, dtype=float32)], val loss: 0.25803348422050476\n",
      "\t partial train loss (single batch): 0.240212\n",
      "Epoch 9/1000 train loss: [array(0.24021211, dtype=float32)], val loss: 0.2578805983066559\n",
      "\t partial train loss (single batch): 0.238638\n",
      "Epoch 10/1000 train loss: [array(0.23863843, dtype=float32)], val loss: 0.257747620344162\n",
      "\t partial train loss (single batch): 0.237644\n",
      "Epoch 11/1000 train loss: [array(0.23764381, dtype=float32)], val loss: 0.257533997297287\n",
      "\t partial train loss (single batch): 0.236312\n",
      "Epoch 12/1000 train loss: [array(0.23631227, dtype=float32)], val loss: 0.2572304904460907\n",
      "\t partial train loss (single batch): 0.235276\n",
      "Epoch 13/1000 train loss: [array(0.23527618, dtype=float32)], val loss: 0.25696927309036255\n",
      "\t partial train loss (single batch): 0.234288\n",
      "Epoch 14/1000 train loss: [array(0.23428765, dtype=float32)], val loss: 0.25656136870384216\n",
      "\t partial train loss (single batch): 0.233352\n",
      "Epoch 15/1000 train loss: [array(0.23335199, dtype=float32)], val loss: 0.25622349977493286\n",
      "\t partial train loss (single batch): 0.232635\n",
      "Epoch 16/1000 train loss: [array(0.23263472, dtype=float32)], val loss: 0.25580206513404846\n",
      "\t partial train loss (single batch): 0.231539\n",
      "Epoch 17/1000 train loss: [array(0.23153931, dtype=float32)], val loss: 0.25596848130226135\n",
      "\t partial train loss (single batch): 0.230354\n",
      "Epoch 18/1000 train loss: [array(0.23035358, dtype=float32)], val loss: 0.2560153901576996\n",
      "\t partial train loss (single batch): 0.229376\n",
      "Epoch 19/1000 train loss: [array(0.22937629, dtype=float32)], val loss: 0.25531014800071716\n",
      "\t partial train loss (single batch): 0.228741\n",
      "Epoch 20/1000 train loss: [array(0.22874093, dtype=float32)], val loss: 0.2549513280391693\n",
      "\t partial train loss (single batch): 0.227634\n",
      "Epoch 21/1000 train loss: [array(0.22763447, dtype=float32)], val loss: 0.2545503079891205\n",
      "\t partial train loss (single batch): 0.226759\n",
      "Epoch 22/1000 train loss: [array(0.22675925, dtype=float32)], val loss: 0.2541363835334778\n",
      "\t partial train loss (single batch): 0.225670\n",
      "Epoch 23/1000 train loss: [array(0.22566962, dtype=float32)], val loss: 0.2538491487503052\n",
      "\t partial train loss (single batch): 0.224649\n",
      "Epoch 24/1000 train loss: [array(0.22464944, dtype=float32)], val loss: 0.2536296546459198\n",
      "\t partial train loss (single batch): 0.223982\n",
      "Epoch 25/1000 train loss: [array(0.2239819, dtype=float32)], val loss: 0.25336751341819763\n",
      "\t partial train loss (single batch): 0.223004\n",
      "Epoch 26/1000 train loss: [array(0.22300352, dtype=float32)], val loss: 0.25258350372314453\n",
      "\t partial train loss (single batch): 0.222181\n",
      "Epoch 27/1000 train loss: [array(0.22218078, dtype=float32)], val loss: 0.25196412205696106\n",
      "\t partial train loss (single batch): 0.221409\n",
      "Epoch 28/1000 train loss: [array(0.22140944, dtype=float32)], val loss: 0.2520791292190552\n",
      "\t partial train loss (single batch): 0.220417\n",
      "Epoch 29/1000 train loss: [array(0.22041744, dtype=float32)], val loss: 0.25104260444641113\n",
      "\t partial train loss (single batch): 0.219567\n",
      "Epoch 30/1000 train loss: [array(0.21956702, dtype=float32)], val loss: 0.2509649693965912\n",
      "\t partial train loss (single batch): 0.218674\n",
      "Epoch 31/1000 train loss: [array(0.2186743, dtype=float32)], val loss: 0.2501581013202667\n",
      "\t partial train loss (single batch): 0.218096\n",
      "Epoch 32/1000 train loss: [array(0.21809576, dtype=float32)], val loss: 0.24984712898731232\n",
      "\t partial train loss (single batch): 0.217450\n",
      "Epoch 33/1000 train loss: [array(0.21745004, dtype=float32)], val loss: 0.24950622022151947\n",
      "\t partial train loss (single batch): 0.216601\n",
      "Epoch 34/1000 train loss: [array(0.21660082, dtype=float32)], val loss: 0.2485981583595276\n",
      "\t partial train loss (single batch): 0.215749\n",
      "Epoch 35/1000 train loss: [array(0.21574868, dtype=float32)], val loss: 0.24811232089996338\n",
      "\t partial train loss (single batch): 0.214569\n",
      "Epoch 36/1000 train loss: [array(0.21456896, dtype=float32)], val loss: 0.24718797206878662\n",
      "\t partial train loss (single batch): 0.214283\n",
      "Epoch 37/1000 train loss: [array(0.21428348, dtype=float32)], val loss: 0.24648411571979523\n",
      "\t partial train loss (single batch): 0.213095\n",
      "Epoch 38/1000 train loss: [array(0.21309496, dtype=float32)], val loss: 0.24547363817691803\n",
      "\t partial train loss (single batch): 0.212534\n",
      "Epoch 39/1000 train loss: [array(0.21253407, dtype=float32)], val loss: 0.2448342740535736\n",
      "\t partial train loss (single batch): 0.211468\n",
      "Epoch 40/1000 train loss: [array(0.21146776, dtype=float32)], val loss: 0.2440219223499298\n",
      "\t partial train loss (single batch): 0.211501\n",
      "Epoch 41/1000 train loss: [array(0.21150088, dtype=float32)], val loss: 0.2430889904499054\n",
      "\t partial train loss (single batch): 0.210230\n",
      "Epoch 42/1000 train loss: [array(0.2102301, dtype=float32)], val loss: 0.24190817773342133\n",
      "\t partial train loss (single batch): 0.209462\n",
      "Epoch 43/1000 train loss: [array(0.20946223, dtype=float32)], val loss: 0.2411414384841919\n",
      "\t partial train loss (single batch): 0.208932\n",
      "Epoch 44/1000 train loss: [array(0.2089323, dtype=float32)], val loss: 0.24017144739627838\n",
      "\t partial train loss (single batch): 0.207813\n",
      "Epoch 45/1000 train loss: [array(0.20781302, dtype=float32)], val loss: 0.23877155780792236\n",
      "\t partial train loss (single batch): 0.207154\n",
      "Epoch 46/1000 train loss: [array(0.20715414, dtype=float32)], val loss: 0.23780570924282074\n",
      "\t partial train loss (single batch): 0.206568\n",
      "Epoch 47/1000 train loss: [array(0.20656803, dtype=float32)], val loss: 0.23661257326602936\n",
      "\t partial train loss (single batch): 0.205526\n",
      "Epoch 48/1000 train loss: [array(0.20552617, dtype=float32)], val loss: 0.23513005673885345\n",
      "\t partial train loss (single batch): 0.204895\n",
      "Epoch 49/1000 train loss: [array(0.20489472, dtype=float32)], val loss: 0.23401205241680145\n",
      "\t partial train loss (single batch): 0.204638\n",
      "Epoch 50/1000 train loss: [array(0.20463833, dtype=float32)], val loss: 0.23239009082317352\n",
      "\t partial train loss (single batch): 0.203618\n",
      "Epoch 51/1000 train loss: [array(0.20361759, dtype=float32)], val loss: 0.23148390650749207\n",
      "\t partial train loss (single batch): 0.202549\n",
      "Epoch 52/1000 train loss: [array(0.20254882, dtype=float32)], val loss: 0.22969898581504822\n",
      "\t partial train loss (single batch): 0.202374\n",
      "Epoch 53/1000 train loss: [array(0.20237353, dtype=float32)], val loss: 0.22819805145263672\n",
      "\t partial train loss (single batch): 0.201334\n",
      "Epoch 54/1000 train loss: [array(0.20133449, dtype=float32)], val loss: 0.22731231153011322\n",
      "\t partial train loss (single batch): 0.200835\n",
      "Epoch 55/1000 train loss: [array(0.20083453, dtype=float32)], val loss: 0.22528330981731415\n",
      "\t partial train loss (single batch): 0.200211\n",
      "Epoch 56/1000 train loss: [array(0.20021087, dtype=float32)], val loss: 0.22330349683761597\n",
      "\t partial train loss (single batch): 0.199520\n",
      "Epoch 57/1000 train loss: [array(0.19952026, dtype=float32)], val loss: 0.22253698110580444\n",
      "\t partial train loss (single batch): 0.198600\n",
      "Epoch 58/1000 train loss: [array(0.19860032, dtype=float32)], val loss: 0.221069797873497\n",
      "\t partial train loss (single batch): 0.198152\n",
      "Epoch 59/1000 train loss: [array(0.19815204, dtype=float32)], val loss: 0.21922466158866882\n",
      "\t partial train loss (single batch): 0.197122\n",
      "Epoch 60/1000 train loss: [array(0.19712235, dtype=float32)], val loss: 0.21710874140262604\n",
      "\t partial train loss (single batch): 0.196370\n",
      "Epoch 61/1000 train loss: [array(0.19636959, dtype=float32)], val loss: 0.21558193862438202\n",
      "\t partial train loss (single batch): 0.195563\n",
      "Epoch 62/1000 train loss: [array(0.19556312, dtype=float32)], val loss: 0.21394766867160797\n",
      "\t partial train loss (single batch): 0.195060\n",
      "Epoch 63/1000 train loss: [array(0.19505964, dtype=float32)], val loss: 0.21177512407302856\n",
      "\t partial train loss (single batch): 0.194252\n",
      "Epoch 64/1000 train loss: [array(0.19425231, dtype=float32)], val loss: 0.2108766734600067\n",
      "\t partial train loss (single batch): 0.193733\n",
      "Epoch 65/1000 train loss: [array(0.19373304, dtype=float32)], val loss: 0.20904776453971863\n",
      "\t partial train loss (single batch): 0.192790\n",
      "Epoch 66/1000 train loss: [array(0.1927905, dtype=float32)], val loss: 0.2076026201248169\n",
      "\t partial train loss (single batch): 0.192080\n",
      "Epoch 67/1000 train loss: [array(0.1920799, dtype=float32)], val loss: 0.20593713223934174\n",
      "\t partial train loss (single batch): 0.191699\n",
      "Epoch 68/1000 train loss: [array(0.19169863, dtype=float32)], val loss: 0.20485441386699677\n",
      "\t partial train loss (single batch): 0.190779\n",
      "Epoch 69/1000 train loss: [array(0.19077891, dtype=float32)], val loss: 0.20248594880104065\n",
      "\t partial train loss (single batch): 0.190268\n",
      "Epoch 70/1000 train loss: [array(0.19026849, dtype=float32)], val loss: 0.20134712755680084\n",
      "\t partial train loss (single batch): 0.189196\n",
      "Epoch 71/1000 train loss: [array(0.18919575, dtype=float32)], val loss: 0.199493870139122\n",
      "\t partial train loss (single batch): 0.188502\n",
      "Epoch 72/1000 train loss: [array(0.18850225, dtype=float32)], val loss: 0.19826087355613708\n",
      "\t partial train loss (single batch): 0.188274\n",
      "Epoch 73/1000 train loss: [array(0.18827358, dtype=float32)], val loss: 0.1974843591451645\n",
      "\t partial train loss (single batch): 0.187546\n",
      "Epoch 74/1000 train loss: [array(0.18754622, dtype=float32)], val loss: 0.19526143372058868\n",
      "\t partial train loss (single batch): 0.187204\n",
      "Epoch 75/1000 train loss: [array(0.18720357, dtype=float32)], val loss: 0.1939469426870346\n",
      "\t partial train loss (single batch): 0.186378\n",
      "Epoch 76/1000 train loss: [array(0.18637772, dtype=float32)], val loss: 0.19271311163902283\n",
      "\t partial train loss (single batch): 0.185880\n",
      "Epoch 77/1000 train loss: [array(0.18587992, dtype=float32)], val loss: 0.1911606788635254\n",
      "\t partial train loss (single batch): 0.184340\n",
      "Epoch 78/1000 train loss: [array(0.18433985, dtype=float32)], val loss: 0.1905350387096405\n",
      "\t partial train loss (single batch): 0.183583\n",
      "Epoch 79/1000 train loss: [array(0.18358316, dtype=float32)], val loss: 0.18902578949928284\n",
      "\t partial train loss (single batch): 0.183630\n",
      "Epoch 80/1000 train loss: [array(0.18362959, dtype=float32)], val loss: 0.18818043172359467\n",
      "\t partial train loss (single batch): 0.182815\n",
      "Epoch 81/1000 train loss: [array(0.18281521, dtype=float32)], val loss: 0.18669000267982483\n",
      "\t partial train loss (single batch): 0.182245\n",
      "Epoch 82/1000 train loss: [array(0.1822455, dtype=float32)], val loss: 0.18568743765354156\n",
      "\t partial train loss (single batch): 0.181194\n",
      "Epoch 83/1000 train loss: [array(0.18119399, dtype=float32)], val loss: 0.18460023403167725\n",
      "\t partial train loss (single batch): 0.180866\n",
      "Epoch 84/1000 train loss: [array(0.18086632, dtype=float32)], val loss: 0.18335305154323578\n",
      "\t partial train loss (single batch): 0.179740\n",
      "Epoch 85/1000 train loss: [array(0.17973965, dtype=float32)], val loss: 0.18243052065372467\n",
      "\t partial train loss (single batch): 0.179255\n",
      "Epoch 86/1000 train loss: [array(0.17925487, dtype=float32)], val loss: 0.1815692037343979\n",
      "\t partial train loss (single batch): 0.178923\n",
      "Epoch 87/1000 train loss: [array(0.1789229, dtype=float32)], val loss: 0.18030405044555664\n",
      "\t partial train loss (single batch): 0.178301\n",
      "Epoch 88/1000 train loss: [array(0.17830084, dtype=float32)], val loss: 0.17939917743206024\n",
      "\t partial train loss (single batch): 0.177792\n",
      "Epoch 89/1000 train loss: [array(0.17779201, dtype=float32)], val loss: 0.17814286053180695\n",
      "\t partial train loss (single batch): 0.177253\n",
      "Epoch 90/1000 train loss: [array(0.17725272, dtype=float32)], val loss: 0.17744168639183044\n",
      "\t partial train loss (single batch): 0.176256\n",
      "Epoch 91/1000 train loss: [array(0.17625576, dtype=float32)], val loss: 0.17765425145626068\n",
      "\t partial train loss (single batch): 0.175438\n",
      "Epoch 92/1000 train loss: [array(0.1754379, dtype=float32)], val loss: 0.1765139251947403\n",
      "\t partial train loss (single batch): 0.175124\n",
      "Epoch 93/1000 train loss: [array(0.1751239, dtype=float32)], val loss: 0.17574316263198853\n",
      "\t partial train loss (single batch): 0.174246\n",
      "Epoch 94/1000 train loss: [array(0.17424563, dtype=float32)], val loss: 0.17490462958812714\n",
      "\t partial train loss (single batch): 0.174133\n",
      "Epoch 95/1000 train loss: [array(0.17413284, dtype=float32)], val loss: 0.17392852902412415\n",
      "\t partial train loss (single batch): 0.172616\n",
      "Epoch 96/1000 train loss: [array(0.17261606, dtype=float32)], val loss: 0.17390842735767365\n",
      "\t partial train loss (single batch): 0.172631\n",
      "Epoch 97/1000 train loss: [array(0.17263141, dtype=float32)], val loss: 0.173079252243042\n",
      "\t partial train loss (single batch): 0.171903\n",
      "Epoch 98/1000 train loss: [array(0.1719034, dtype=float32)], val loss: 0.1726764291524887\n",
      "\t partial train loss (single batch): 0.171312\n",
      "Epoch 99/1000 train loss: [array(0.17131197, dtype=float32)], val loss: 0.17167559266090393\n",
      "\t partial train loss (single batch): 0.170136\n",
      "Epoch 100/1000 train loss: [array(0.17013636, dtype=float32)], val loss: 0.17125678062438965\n",
      "\t partial train loss (single batch): 0.170574\n",
      "Epoch 101/1000 train loss: [array(0.17057359, dtype=float32)], val loss: 0.17012545466423035\n",
      "\t partial train loss (single batch): 0.169958\n",
      "Epoch 102/1000 train loss: [array(0.16995786, dtype=float32)], val loss: 0.1688184142112732\n",
      "\t partial train loss (single batch): 0.168795\n",
      "Epoch 103/1000 train loss: [array(0.16879459, dtype=float32)], val loss: 0.1689041256904602\n",
      "\t partial train loss (single batch): 0.168336\n",
      "Epoch 104/1000 train loss: [array(0.16833623, dtype=float32)], val loss: 0.16828303039073944\n",
      "\t partial train loss (single batch): 0.167989\n",
      "Epoch 105/1000 train loss: [array(0.16798888, dtype=float32)], val loss: 0.1671678125858307\n",
      "\t partial train loss (single batch): 0.167507\n",
      "Epoch 106/1000 train loss: [array(0.16750705, dtype=float32)], val loss: 0.16706225275993347\n",
      "\t partial train loss (single batch): 0.166459\n",
      "Epoch 107/1000 train loss: [array(0.16645868, dtype=float32)], val loss: 0.16667413711547852\n",
      "\t partial train loss (single batch): 0.166187\n",
      "Epoch 108/1000 train loss: [array(0.16618682, dtype=float32)], val loss: 0.16625545918941498\n",
      "\t partial train loss (single batch): 0.165436\n",
      "Epoch 109/1000 train loss: [array(0.16543603, dtype=float32)], val loss: 0.165509432554245\n",
      "\t partial train loss (single batch): 0.164813\n",
      "Epoch 110/1000 train loss: [array(0.16481315, dtype=float32)], val loss: 0.16423910856246948\n",
      "\t partial train loss (single batch): 0.164166\n",
      "Epoch 111/1000 train loss: [array(0.16416594, dtype=float32)], val loss: 0.16336272656917572\n",
      "\t partial train loss (single batch): 0.164285\n",
      "Epoch 112/1000 train loss: [array(0.16428545, dtype=float32)], val loss: 0.16367468237876892\n",
      "\t partial train loss (single batch): 0.162696\n",
      "Epoch 113/1000 train loss: [array(0.16269599, dtype=float32)], val loss: 0.16203375160694122\n",
      "\t partial train loss (single batch): 0.162667\n",
      "Epoch 114/1000 train loss: [array(0.16266678, dtype=float32)], val loss: 0.16187459230422974\n",
      "\t partial train loss (single batch): 0.161848\n",
      "Epoch 115/1000 train loss: [array(0.16184793, dtype=float32)], val loss: 0.16088446974754333\n",
      "\t partial train loss (single batch): 0.161961\n",
      "Epoch 116/1000 train loss: [array(0.16196066, dtype=float32)], val loss: 0.16033680737018585\n",
      "\t partial train loss (single batch): 0.160806\n",
      "Epoch 117/1000 train loss: [array(0.16080642, dtype=float32)], val loss: 0.16136522591114044\n",
      "\t partial train loss (single batch): 0.160170\n",
      "Epoch 118/1000 train loss: [array(0.16017, dtype=float32)], val loss: 0.15955138206481934\n",
      "\t partial train loss (single batch): 0.159327\n",
      "Epoch 119/1000 train loss: [array(0.15932654, dtype=float32)], val loss: 0.15935784578323364\n",
      "\t partial train loss (single batch): 0.158660\n",
      "Epoch 120/1000 train loss: [array(0.15865958, dtype=float32)], val loss: 0.15904995799064636\n",
      "\t partial train loss (single batch): 0.158227\n",
      "Epoch 121/1000 train loss: [array(0.15822737, dtype=float32)], val loss: 0.1581946462392807\n",
      "\t partial train loss (single batch): 0.158029\n",
      "Epoch 122/1000 train loss: [array(0.15802857, dtype=float32)], val loss: 0.1577123999595642\n",
      "\t partial train loss (single batch): 0.157629\n",
      "Epoch 123/1000 train loss: [array(0.15762934, dtype=float32)], val loss: 0.1565345972776413\n",
      "\t partial train loss (single batch): 0.156962\n",
      "Epoch 124/1000 train loss: [array(0.15696174, dtype=float32)], val loss: 0.15595485270023346\n",
      "\t partial train loss (single batch): 0.156351\n",
      "Epoch 125/1000 train loss: [array(0.15635093, dtype=float32)], val loss: 0.15667101740837097\n",
      "\t partial train loss (single batch): 0.155679\n",
      "Epoch 126/1000 train loss: [array(0.15567908, dtype=float32)], val loss: 0.15656925737857819\n",
      "\t partial train loss (single batch): 0.155309\n",
      "Epoch 127/1000 train loss: [array(0.15530893, dtype=float32)], val loss: 0.15534448623657227\n",
      "\t partial train loss (single batch): 0.154584\n",
      "Epoch 128/1000 train loss: [array(0.15458363, dtype=float32)], val loss: 0.1549081951379776\n",
      "\t partial train loss (single batch): 0.153854\n",
      "Epoch 129/1000 train loss: [array(0.15385394, dtype=float32)], val loss: 0.1543334722518921\n",
      "\t partial train loss (single batch): 0.153630\n",
      "Epoch 130/1000 train loss: [array(0.15362966, dtype=float32)], val loss: 0.15401414036750793\n",
      "\t partial train loss (single batch): 0.153351\n",
      "Epoch 131/1000 train loss: [array(0.15335101, dtype=float32)], val loss: 0.1539389193058014\n",
      "\t partial train loss (single batch): 0.152035\n",
      "Epoch 132/1000 train loss: [array(0.15203497, dtype=float32)], val loss: 0.153306245803833\n",
      "\t partial train loss (single batch): 0.151972\n",
      "Epoch 133/1000 train loss: [array(0.15197207, dtype=float32)], val loss: 0.15231025218963623\n",
      "\t partial train loss (single batch): 0.151404\n",
      "Epoch 134/1000 train loss: [array(0.151404, dtype=float32)], val loss: 0.15092533826828003\n",
      "\t partial train loss (single batch): 0.150902\n",
      "Epoch 135/1000 train loss: [array(0.1509018, dtype=float32)], val loss: 0.15013989806175232\n",
      "\t partial train loss (single batch): 0.150086\n",
      "Epoch 136/1000 train loss: [array(0.15008612, dtype=float32)], val loss: 0.14997528493404388\n",
      "\t partial train loss (single batch): 0.149594\n",
      "Epoch 137/1000 train loss: [array(0.14959429, dtype=float32)], val loss: 0.1498352289199829\n",
      "\t partial train loss (single batch): 0.149672\n",
      "Epoch 138/1000 train loss: [array(0.1496718, dtype=float32)], val loss: 0.14919927716255188\n",
      "\t partial train loss (single batch): 0.149230\n",
      "Epoch 139/1000 train loss: [array(0.1492297, dtype=float32)], val loss: 0.14892111718654633\n",
      "\t partial train loss (single batch): 0.148299\n",
      "Epoch 140/1000 train loss: [array(0.14829929, dtype=float32)], val loss: 0.14851810038089752\n",
      "\t partial train loss (single batch): 0.148509\n",
      "Epoch 141/1000 train loss: [array(0.14850932, dtype=float32)], val loss: 0.14743934571743011\n",
      "\t partial train loss (single batch): 0.147419\n",
      "Epoch 142/1000 train loss: [array(0.14741887, dtype=float32)], val loss: 0.1465402990579605\n",
      "\t partial train loss (single batch): 0.146968\n",
      "Epoch 143/1000 train loss: [array(0.14696756, dtype=float32)], val loss: 0.14657406508922577\n",
      "\t partial train loss (single batch): 0.146145\n",
      "Epoch 144/1000 train loss: [array(0.14614491, dtype=float32)], val loss: 0.1466544270515442\n",
      "\t partial train loss (single batch): 0.145826\n",
      "Epoch 145/1000 train loss: [array(0.14582625, dtype=float32)], val loss: 0.14558476209640503\n",
      "\t partial train loss (single batch): 0.144873\n",
      "Epoch 146/1000 train loss: [array(0.1448725, dtype=float32)], val loss: 0.14505299925804138\n",
      "\t partial train loss (single batch): 0.144624\n",
      "Epoch 147/1000 train loss: [array(0.1446239, dtype=float32)], val loss: 0.14534977078437805\n",
      "\t partial train loss (single batch): 0.144092\n",
      "Epoch 148/1000 train loss: [array(0.14409186, dtype=float32)], val loss: 0.14481016993522644\n",
      "\t partial train loss (single batch): 0.144096\n",
      "Epoch 149/1000 train loss: [array(0.14409585, dtype=float32)], val loss: 0.14408297836780548\n",
      "\t partial train loss (single batch): 0.143296\n",
      "Epoch 150/1000 train loss: [array(0.14329581, dtype=float32)], val loss: 0.14278629422187805\n",
      "\t partial train loss (single batch): 0.142946\n",
      "Epoch 151/1000 train loss: [array(0.1429465, dtype=float32)], val loss: 0.14355768263339996\n",
      "\t partial train loss (single batch): 0.142888\n",
      "Epoch 152/1000 train loss: [array(0.14288767, dtype=float32)], val loss: 0.14217348396778107\n",
      "\t partial train loss (single batch): 0.141904\n",
      "Epoch 153/1000 train loss: [array(0.14190432, dtype=float32)], val loss: 0.14114126563072205\n",
      "\t partial train loss (single batch): 0.141662\n",
      "Epoch 154/1000 train loss: [array(0.14166228, dtype=float32)], val loss: 0.14020715653896332\n",
      "\t partial train loss (single batch): 0.140964\n",
      "Epoch 155/1000 train loss: [array(0.14096375, dtype=float32)], val loss: 0.14049407839775085\n",
      "\t partial train loss (single batch): 0.140330\n",
      "Epoch 156/1000 train loss: [array(0.14032951, dtype=float32)], val loss: 0.13976195454597473\n",
      "\t partial train loss (single batch): 0.140333\n",
      "Epoch 157/1000 train loss: [array(0.14033282, dtype=float32)], val loss: 0.1398608386516571\n",
      "\t partial train loss (single batch): 0.140266\n",
      "Epoch 158/1000 train loss: [array(0.14026561, dtype=float32)], val loss: 0.13810144364833832\n",
      "\t partial train loss (single batch): 0.139225\n",
      "Epoch 159/1000 train loss: [array(0.13922513, dtype=float32)], val loss: 0.138658806681633\n",
      "\t partial train loss (single batch): 0.138405\n",
      "Epoch 160/1000 train loss: [array(0.1384053, dtype=float32)], val loss: 0.13905921578407288\n",
      "\t partial train loss (single batch): 0.138765\n",
      "Epoch 161/1000 train loss: [array(0.13876548, dtype=float32)], val loss: 0.13819527626037598\n",
      "\t partial train loss (single batch): 0.137218\n",
      "Epoch 162/1000 train loss: [array(0.13721752, dtype=float32)], val loss: 0.13726207613945007\n",
      "\t partial train loss (single batch): 0.136670\n",
      "Epoch 163/1000 train loss: [array(0.13667022, dtype=float32)], val loss: 0.13689503073692322\n",
      "\t partial train loss (single batch): 0.136871\n",
      "Epoch 164/1000 train loss: [array(0.13687056, dtype=float32)], val loss: 0.1369241178035736\n",
      "\t partial train loss (single batch): 0.136358\n",
      "Epoch 165/1000 train loss: [array(0.1363578, dtype=float32)], val loss: 0.1357877552509308\n",
      "\t partial train loss (single batch): 0.135490\n",
      "Epoch 166/1000 train loss: [array(0.13548966, dtype=float32)], val loss: 0.1355595737695694\n",
      "\t partial train loss (single batch): 0.135301\n",
      "Epoch 167/1000 train loss: [array(0.13530137, dtype=float32)], val loss: 0.13450050354003906\n",
      "\t partial train loss (single batch): 0.134729\n",
      "Epoch 168/1000 train loss: [array(0.13472891, dtype=float32)], val loss: 0.13588932156562805\n",
      "\t partial train loss (single batch): 0.135105\n",
      "Epoch 169/1000 train loss: [array(0.13510495, dtype=float32)], val loss: 0.13509458303451538\n",
      "\t partial train loss (single batch): 0.133909\n",
      "Epoch 170/1000 train loss: [array(0.1339087, dtype=float32)], val loss: 0.1336325854063034\n",
      "\t partial train loss (single batch): 0.133304\n",
      "Epoch 171/1000 train loss: [array(0.13330393, dtype=float32)], val loss: 0.13312241435050964\n",
      "\t partial train loss (single batch): 0.133670\n",
      "Epoch 172/1000 train loss: [array(0.13366969, dtype=float32)], val loss: 0.1333182156085968\n",
      "\t partial train loss (single batch): 0.132598\n",
      "Epoch 173/1000 train loss: [array(0.1325977, dtype=float32)], val loss: 0.13246497511863708\n",
      "\t partial train loss (single batch): 0.133376\n",
      "Epoch 174/1000 train loss: [array(0.13337578, dtype=float32)], val loss: 0.13168968260288239\n",
      "\t partial train loss (single batch): 0.132569\n",
      "Epoch 175/1000 train loss: [array(0.13256902, dtype=float32)], val loss: 0.131391242146492\n",
      "\t partial train loss (single batch): 0.131773\n",
      "Epoch 176/1000 train loss: [array(0.13177319, dtype=float32)], val loss: 0.13091151416301727\n",
      "\t partial train loss (single batch): 0.131467\n",
      "Epoch 177/1000 train loss: [array(0.13146718, dtype=float32)], val loss: 0.13119448721408844\n",
      "\t partial train loss (single batch): 0.130970\n",
      "Epoch 178/1000 train loss: [array(0.13096966, dtype=float32)], val loss: 0.13073642551898956\n",
      "\t partial train loss (single batch): 0.130863\n",
      "Epoch 179/1000 train loss: [array(0.13086322, dtype=float32)], val loss: 0.13045071065425873\n",
      "\t partial train loss (single batch): 0.130786\n",
      "Epoch 180/1000 train loss: [array(0.13078576, dtype=float32)], val loss: 0.12969495356082916\n",
      "\t partial train loss (single batch): 0.130544\n",
      "Epoch 181/1000 train loss: [array(0.13054398, dtype=float32)], val loss: 0.129392609000206\n",
      "\t partial train loss (single batch): 0.130075\n",
      "Epoch 182/1000 train loss: [array(0.13007501, dtype=float32)], val loss: 0.12906499207019806\n",
      "\t partial train loss (single batch): 0.129114\n",
      "Epoch 183/1000 train loss: [array(0.129114, dtype=float32)], val loss: 0.1287938803434372\n",
      "\t partial train loss (single batch): 0.128950\n",
      "Epoch 184/1000 train loss: [array(0.12894987, dtype=float32)], val loss: 0.12835080921649933\n",
      "\t partial train loss (single batch): 0.128442\n",
      "Epoch 185/1000 train loss: [array(0.12844153, dtype=float32)], val loss: 0.12833666801452637\n",
      "\t partial train loss (single batch): 0.127976\n",
      "Epoch 186/1000 train loss: [array(0.12797642, dtype=float32)], val loss: 0.12699908018112183\n",
      "\t partial train loss (single batch): 0.126827\n",
      "Epoch 187/1000 train loss: [array(0.12682669, dtype=float32)], val loss: 0.1271636039018631\n",
      "\t partial train loss (single batch): 0.127224\n",
      "Epoch 188/1000 train loss: [array(0.12722424, dtype=float32)], val loss: 0.12689748406410217\n",
      "\t partial train loss (single batch): 0.126925\n",
      "Epoch 189/1000 train loss: [array(0.12692517, dtype=float32)], val loss: 0.1268247812986374\n",
      "\t partial train loss (single batch): 0.126604\n",
      "Epoch 190/1000 train loss: [array(0.12660433, dtype=float32)], val loss: 0.12653595209121704\n",
      "\t partial train loss (single batch): 0.126320\n",
      "Epoch 191/1000 train loss: [array(0.12631974, dtype=float32)], val loss: 0.12635700404644012\n",
      "\t partial train loss (single batch): 0.125599\n",
      "Epoch 192/1000 train loss: [array(0.12559927, dtype=float32)], val loss: 0.12487941235303879\n",
      "\t partial train loss (single batch): 0.125323\n",
      "Epoch 193/1000 train loss: [array(0.12532349, dtype=float32)], val loss: 0.12453971803188324\n",
      "\t partial train loss (single batch): 0.124327\n",
      "Epoch 194/1000 train loss: [array(0.12432726, dtype=float32)], val loss: 0.12548787891864777\n",
      "\t partial train loss (single batch): 0.124610\n",
      "Epoch 195/1000 train loss: [array(0.12461048, dtype=float32)], val loss: 0.12461069226264954\n",
      "\t partial train loss (single batch): 0.124460\n",
      "Epoch 196/1000 train loss: [array(0.12445994, dtype=float32)], val loss: 0.12501339614391327\n",
      "\t partial train loss (single batch): 0.123852\n",
      "Epoch 197/1000 train loss: [array(0.12385167, dtype=float32)], val loss: 0.12422151863574982\n",
      "\t partial train loss (single batch): 0.122936\n",
      "Epoch 198/1000 train loss: [array(0.12293621, dtype=float32)], val loss: 0.12275921553373337\n",
      "\t partial train loss (single batch): 0.123279\n",
      "Epoch 199/1000 train loss: [array(0.12327864, dtype=float32)], val loss: 0.12323732674121857\n",
      "\t partial train loss (single batch): 0.122970\n",
      "Epoch 200/1000 train loss: [array(0.1229699, dtype=float32)], val loss: 0.12301860004663467\n",
      "\t partial train loss (single batch): 0.122896\n",
      "Epoch 201/1000 train loss: [array(0.12289579, dtype=float32)], val loss: 0.12279242277145386\n",
      "\t partial train loss (single batch): 0.122258\n",
      "Epoch 202/1000 train loss: [array(0.12225779, dtype=float32)], val loss: 0.12126658111810684\n",
      "\t partial train loss (single batch): 0.122050\n",
      "Epoch 203/1000 train loss: [array(0.12205019, dtype=float32)], val loss: 0.12203580141067505\n",
      "\t partial train loss (single batch): 0.121987\n",
      "Epoch 204/1000 train loss: [array(0.12198696, dtype=float32)], val loss: 0.12182947993278503\n",
      "\t partial train loss (single batch): 0.121076\n",
      "Epoch 205/1000 train loss: [array(0.12107578, dtype=float32)], val loss: 0.12131951749324799\n",
      "\t partial train loss (single batch): 0.120811\n",
      "Epoch 206/1000 train loss: [array(0.12081111, dtype=float32)], val loss: 0.12184100598096848\n",
      "\t partial train loss (single batch): 0.121106\n",
      "Epoch 207/1000 train loss: [array(0.12110553, dtype=float32)], val loss: 0.1215716004371643\n",
      "\t partial train loss (single batch): 0.120518\n",
      "Epoch 208/1000 train loss: [array(0.12051763, dtype=float32)], val loss: 0.12073574960231781\n",
      "\t partial train loss (single batch): 0.119614\n",
      "Epoch 209/1000 train loss: [array(0.11961363, dtype=float32)], val loss: 0.11946487426757812\n",
      "\t partial train loss (single batch): 0.120171\n",
      "Epoch 210/1000 train loss: [array(0.12017057, dtype=float32)], val loss: 0.11923648416996002\n",
      "\t partial train loss (single batch): 0.118653\n",
      "Epoch 211/1000 train loss: [array(0.11865303, dtype=float32)], val loss: 0.11942978948354721\n",
      "\t partial train loss (single batch): 0.119786\n",
      "Epoch 212/1000 train loss: [array(0.11978571, dtype=float32)], val loss: 0.1196167841553688\n",
      "\t partial train loss (single batch): 0.118142\n",
      "Epoch 213/1000 train loss: [array(0.11814245, dtype=float32)], val loss: 0.11924070119857788\n",
      "\t partial train loss (single batch): 0.118047\n",
      "Epoch 214/1000 train loss: [array(0.11804666, dtype=float32)], val loss: 0.11885886639356613\n",
      "\t partial train loss (single batch): 0.118226\n",
      "Epoch 215/1000 train loss: [array(0.11822569, dtype=float32)], val loss: 0.11887320876121521\n",
      "\t partial train loss (single batch): 0.116997\n",
      "Epoch 216/1000 train loss: [array(0.11699669, dtype=float32)], val loss: 0.11889534443616867\n",
      "\t partial train loss (single batch): 0.117236\n",
      "Epoch 217/1000 train loss: [array(0.11723585, dtype=float32)], val loss: 0.11808162182569504\n",
      "\t partial train loss (single batch): 0.117673\n",
      "Epoch 218/1000 train loss: [array(0.11767256, dtype=float32)], val loss: 0.11715622991323471\n",
      "\t partial train loss (single batch): 0.116521\n",
      "Epoch 219/1000 train loss: [array(0.11652091, dtype=float32)], val loss: 0.11722578853368759\n",
      "\t partial train loss (single batch): 0.116832\n",
      "Epoch 220/1000 train loss: [array(0.11683202, dtype=float32)], val loss: 0.11711141467094421\n",
      "\t partial train loss (single batch): 0.116564\n",
      "Epoch 221/1000 train loss: [array(0.11656368, dtype=float32)], val loss: 0.11673056334257126\n",
      "\t partial train loss (single batch): 0.116376\n",
      "Epoch 222/1000 train loss: [array(0.11637567, dtype=float32)], val loss: 0.11623319983482361\n",
      "\t partial train loss (single batch): 0.115430\n",
      "Epoch 223/1000 train loss: [array(0.11542956, dtype=float32)], val loss: 0.11590572446584702\n",
      "\t partial train loss (single batch): 0.114886\n",
      "Epoch 224/1000 train loss: [array(0.11488575, dtype=float32)], val loss: 0.11559983342885971\n",
      "\t partial train loss (single batch): 0.115296\n",
      "Epoch 225/1000 train loss: [array(0.1152961, dtype=float32)], val loss: 0.11551398783922195\n",
      "\t partial train loss (single batch): 0.115934\n",
      "Epoch 226/1000 train loss: [array(0.11593375, dtype=float32)], val loss: 0.11550338566303253\n",
      "\t partial train loss (single batch): 0.115009\n",
      "Epoch 227/1000 train loss: [array(0.11500908, dtype=float32)], val loss: 0.11412280797958374\n",
      "\t partial train loss (single batch): 0.114463\n",
      "Epoch 228/1000 train loss: [array(0.11446256, dtype=float32)], val loss: 0.11564106494188309\n",
      "\t partial train loss (single batch): 0.113769\n",
      "Epoch 229/1000 train loss: [array(0.1137686, dtype=float32)], val loss: 0.1127333864569664\n",
      "\t partial train loss (single batch): 0.114038\n",
      "Epoch 230/1000 train loss: [array(0.11403761, dtype=float32)], val loss: 0.1142592653632164\n",
      "\t partial train loss (single batch): 0.113170\n",
      "Epoch 231/1000 train loss: [array(0.11317038, dtype=float32)], val loss: 0.11393368989229202\n",
      "\t partial train loss (single batch): 0.113163\n",
      "Epoch 232/1000 train loss: [array(0.11316337, dtype=float32)], val loss: 0.11283241212368011\n",
      "\t partial train loss (single batch): 0.112990\n",
      "Epoch 233/1000 train loss: [array(0.11298957, dtype=float32)], val loss: 0.11324423551559448\n",
      "\t partial train loss (single batch): 0.112452\n",
      "Epoch 234/1000 train loss: [array(0.11245162, dtype=float32)], val loss: 0.11286672204732895\n",
      "\t partial train loss (single batch): 0.112278\n",
      "Epoch 235/1000 train loss: [array(0.11227791, dtype=float32)], val loss: 0.11222977936267853\n",
      "\t partial train loss (single batch): 0.111455\n",
      "Epoch 236/1000 train loss: [array(0.11145478, dtype=float32)], val loss: 0.11278709024190903\n",
      "\t partial train loss (single batch): 0.111513\n",
      "Epoch 237/1000 train loss: [array(0.11151289, dtype=float32)], val loss: 0.11260414123535156\n",
      "\t partial train loss (single batch): 0.112322\n",
      "Epoch 238/1000 train loss: [array(0.11232187, dtype=float32)], val loss: 0.1113852933049202\n",
      "\t partial train loss (single batch): 0.111167\n",
      "Epoch 239/1000 train loss: [array(0.11116722, dtype=float32)], val loss: 0.1104302927851677\n",
      "\t partial train loss (single batch): 0.111257\n",
      "Epoch 240/1000 train loss: [array(0.11125663, dtype=float32)], val loss: 0.1112971380352974\n",
      "\t partial train loss (single batch): 0.110575\n",
      "Epoch 241/1000 train loss: [array(0.11057492, dtype=float32)], val loss: 0.11242364346981049\n",
      "\t partial train loss (single batch): 0.110304\n",
      "Epoch 242/1000 train loss: [array(0.11030353, dtype=float32)], val loss: 0.11008951812982559\n",
      "\t partial train loss (single batch): 0.110125\n",
      "Epoch 243/1000 train loss: [array(0.11012513, dtype=float32)], val loss: 0.10961364954710007\n",
      "\t partial train loss (single batch): 0.109446\n",
      "Epoch 244/1000 train loss: [array(0.10944631, dtype=float32)], val loss: 0.10942508280277252\n",
      "\t partial train loss (single batch): 0.109032\n",
      "Epoch 245/1000 train loss: [array(0.10903225, dtype=float32)], val loss: 0.10938215255737305\n",
      "\t partial train loss (single batch): 0.109562\n",
      "Epoch 246/1000 train loss: [array(0.10956234, dtype=float32)], val loss: 0.10873808711767197\n",
      "\t partial train loss (single batch): 0.108559\n",
      "Epoch 247/1000 train loss: [array(0.10855853, dtype=float32)], val loss: 0.10828632861375809\n",
      "\t partial train loss (single batch): 0.108289\n",
      "Epoch 248/1000 train loss: [array(0.10828894, dtype=float32)], val loss: 0.10817521810531616\n",
      "\t partial train loss (single batch): 0.108942\n",
      "Epoch 249/1000 train loss: [array(0.1089422, dtype=float32)], val loss: 0.10883957147598267\n",
      "\t partial train loss (single batch): 0.108609\n",
      "Epoch 250/1000 train loss: [array(0.10860872, dtype=float32)], val loss: 0.10825725644826889\n",
      "\t partial train loss (single batch): 0.108733\n",
      "Epoch 251/1000 train loss: [array(0.10873275, dtype=float32)], val loss: 0.10761047154664993\n",
      "\t partial train loss (single batch): 0.107280\n",
      "Epoch 252/1000 train loss: [array(0.10727977, dtype=float32)], val loss: 0.10682585090398788\n",
      "\t partial train loss (single batch): 0.107174\n",
      "Epoch 253/1000 train loss: [array(0.10717417, dtype=float32)], val loss: 0.10742127895355225\n",
      "\t partial train loss (single batch): 0.106919\n",
      "Epoch 254/1000 train loss: [array(0.1069191, dtype=float32)], val loss: 0.10661482810974121\n",
      "\t partial train loss (single batch): 0.106548\n",
      "Epoch 255/1000 train loss: [array(0.10654784, dtype=float32)], val loss: 0.10569685697555542\n",
      "\t partial train loss (single batch): 0.107040\n",
      "Epoch 256/1000 train loss: [array(0.1070396, dtype=float32)], val loss: 0.10552915185689926\n",
      "\t partial train loss (single batch): 0.106553\n",
      "Epoch 257/1000 train loss: [array(0.10655304, dtype=float32)], val loss: 0.1069977879524231\n",
      "\t partial train loss (single batch): 0.106533\n",
      "Epoch 258/1000 train loss: [array(0.10653322, dtype=float32)], val loss: 0.10597743093967438\n",
      "\t partial train loss (single batch): 0.106708\n",
      "Epoch 259/1000 train loss: [array(0.10670786, dtype=float32)], val loss: 0.1057119071483612\n",
      "\t partial train loss (single batch): 0.106825\n",
      "Epoch 260/1000 train loss: [array(0.1068251, dtype=float32)], val loss: 0.10628560185432434\n",
      "\t partial train loss (single batch): 0.104179\n",
      "Epoch 261/1000 train loss: [array(0.10417939, dtype=float32)], val loss: 0.10409731417894363\n",
      "\t partial train loss (single batch): 0.105090\n",
      "Epoch 262/1000 train loss: [array(0.10509013, dtype=float32)], val loss: 0.10396794229745865\n",
      "\t partial train loss (single batch): 0.105482\n",
      "Epoch 263/1000 train loss: [array(0.10548163, dtype=float32)], val loss: 0.10404905676841736\n",
      "\t partial train loss (single batch): 0.104974\n",
      "Epoch 264/1000 train loss: [array(0.10497402, dtype=float32)], val loss: 0.10430338978767395\n",
      "\t partial train loss (single batch): 0.104089\n",
      "Epoch 265/1000 train loss: [array(0.10408904, dtype=float32)], val loss: 0.1035846546292305\n",
      "\t partial train loss (single batch): 0.103721\n",
      "Epoch 266/1000 train loss: [array(0.10372076, dtype=float32)], val loss: 0.10398156195878983\n",
      "\t partial train loss (single batch): 0.103963\n",
      "Epoch 267/1000 train loss: [array(0.10396259, dtype=float32)], val loss: 0.10337904840707779\n",
      "\t partial train loss (single batch): 0.103760\n",
      "Epoch 268/1000 train loss: [array(0.10375991, dtype=float32)], val loss: 0.10273046046495438\n",
      "\t partial train loss (single batch): 0.103841\n",
      "Epoch 269/1000 train loss: [array(0.10384145, dtype=float32)], val loss: 0.10176990926265717\n",
      "\t partial train loss (single batch): 0.102744\n",
      "Epoch 270/1000 train loss: [array(0.10274368, dtype=float32)], val loss: 0.10298259556293488\n",
      "\t partial train loss (single batch): 0.103104\n",
      "Epoch 271/1000 train loss: [array(0.10310384, dtype=float32)], val loss: 0.10221463441848755\n",
      "\t partial train loss (single batch): 0.102144\n",
      "Epoch 272/1000 train loss: [array(0.10214391, dtype=float32)], val loss: 0.10218331217765808\n",
      "\t partial train loss (single batch): 0.102601\n",
      "Epoch 273/1000 train loss: [array(0.10260111, dtype=float32)], val loss: 0.10180123150348663\n",
      "\t partial train loss (single batch): 0.102749\n",
      "Epoch 274/1000 train loss: [array(0.10274949, dtype=float32)], val loss: 0.1027437224984169\n",
      "\t partial train loss (single batch): 0.102593\n",
      "Epoch 275/1000 train loss: [array(0.10259306, dtype=float32)], val loss: 0.10202239453792572\n",
      "\t partial train loss (single batch): 0.103233\n",
      "Epoch 276/1000 train loss: [array(0.10323334, dtype=float32)], val loss: 0.10090385377407074\n",
      "\t partial train loss (single batch): 0.101107\n",
      "Epoch 277/1000 train loss: [array(0.10110681, dtype=float32)], val loss: 0.10079266875982285\n",
      "\t partial train loss (single batch): 0.102588\n",
      "Epoch 278/1000 train loss: [array(0.10258798, dtype=float32)], val loss: 0.10166650265455246\n",
      "\t partial train loss (single batch): 0.102197\n",
      "Epoch 279/1000 train loss: [array(0.10219685, dtype=float32)], val loss: 0.10094860941171646\n",
      "\t partial train loss (single batch): 0.101347\n",
      "Epoch 280/1000 train loss: [array(0.10134657, dtype=float32)], val loss: 0.10133761167526245\n",
      "\t partial train loss (single batch): 0.100858\n",
      "Epoch 281/1000 train loss: [array(0.10085804, dtype=float32)], val loss: 0.1002611592411995\n",
      "\t partial train loss (single batch): 0.101033\n",
      "Epoch 282/1000 train loss: [array(0.10103335, dtype=float32)], val loss: 0.10010343044996262\n",
      "\t partial train loss (single batch): 0.101061\n",
      "Epoch 283/1000 train loss: [array(0.10106079, dtype=float32)], val loss: 0.1000434011220932\n",
      "\t partial train loss (single batch): 0.101014\n",
      "Epoch 284/1000 train loss: [array(0.10101368, dtype=float32)], val loss: 0.09968147426843643\n",
      "\t partial train loss (single batch): 0.100414\n",
      "Epoch 285/1000 train loss: [array(0.10041448, dtype=float32)], val loss: 0.0999951884150505\n",
      "\t partial train loss (single batch): 0.099766\n",
      "Epoch 286/1000 train loss: [array(0.09976581, dtype=float32)], val loss: 0.09909675270318985\n",
      "\t partial train loss (single batch): 0.099901\n",
      "Epoch 287/1000 train loss: [array(0.09990122, dtype=float32)], val loss: 0.09961947053670883\n",
      "\t partial train loss (single batch): 0.099796\n",
      "Epoch 288/1000 train loss: [array(0.09979592, dtype=float32)], val loss: 0.0987829864025116\n",
      "\t partial train loss (single batch): 0.099003\n",
      "Epoch 289/1000 train loss: [array(0.09900328, dtype=float32)], val loss: 0.09851013123989105\n",
      "\t partial train loss (single batch): 0.099896\n",
      "Epoch 290/1000 train loss: [array(0.09989593, dtype=float32)], val loss: 0.0985516607761383\n",
      "\t partial train loss (single batch): 0.099887\n",
      "Epoch 291/1000 train loss: [array(0.09988684, dtype=float32)], val loss: 0.09925605356693268\n",
      "\t partial train loss (single batch): 0.099391\n",
      "Epoch 292/1000 train loss: [array(0.09939076, dtype=float32)], val loss: 0.0988403856754303\n",
      "\t partial train loss (single batch): 0.098504\n",
      "Epoch 293/1000 train loss: [array(0.09850425, dtype=float32)], val loss: 0.09824187308549881\n",
      "\t partial train loss (single batch): 0.098400\n",
      "Epoch 294/1000 train loss: [array(0.09839996, dtype=float32)], val loss: 0.09758172184228897\n",
      "\t partial train loss (single batch): 0.098367\n",
      "Epoch 295/1000 train loss: [array(0.09836672, dtype=float32)], val loss: 0.09845539927482605\n",
      "\t partial train loss (single batch): 0.097632\n",
      "Epoch 296/1000 train loss: [array(0.09763239, dtype=float32)], val loss: 0.09818535298109055\n",
      "\t partial train loss (single batch): 0.097845\n",
      "Epoch 297/1000 train loss: [array(0.09784506, dtype=float32)], val loss: 0.09766178578138351\n",
      "\t partial train loss (single batch): 0.097750\n",
      "Epoch 298/1000 train loss: [array(0.0977497, dtype=float32)], val loss: 0.09637699276208878\n",
      "\t partial train loss (single batch): 0.097508\n",
      "Epoch 299/1000 train loss: [array(0.09750835, dtype=float32)], val loss: 0.09542274475097656\n",
      "\t partial train loss (single batch): 0.097108\n",
      "Epoch 300/1000 train loss: [array(0.09710774, dtype=float32)], val loss: 0.09729165583848953\n",
      "\t partial train loss (single batch): 0.097483\n",
      "Epoch 301/1000 train loss: [array(0.09748326, dtype=float32)], val loss: 0.09619956463575363\n",
      "\t partial train loss (single batch): 0.096721\n",
      "Epoch 302/1000 train loss: [array(0.09672147, dtype=float32)], val loss: 0.09704039990901947\n",
      "\t partial train loss (single batch): 0.096423\n",
      "Epoch 303/1000 train loss: [array(0.09642319, dtype=float32)], val loss: 0.09628405421972275\n",
      "\t partial train loss (single batch): 0.096128\n",
      "Epoch 304/1000 train loss: [array(0.09612773, dtype=float32)], val loss: 0.09614881128072739\n",
      "\t partial train loss (single batch): 0.095650\n",
      "Epoch 305/1000 train loss: [array(0.09565012, dtype=float32)], val loss: 0.0953783169388771\n",
      "\t partial train loss (single batch): 0.095632\n",
      "Epoch 306/1000 train loss: [array(0.09563237, dtype=float32)], val loss: 0.09543102234601974\n",
      "\t partial train loss (single batch): 0.096836\n",
      "Epoch 307/1000 train loss: [array(0.0968361, dtype=float32)], val loss: 0.09564246237277985\n",
      "\t partial train loss (single batch): 0.095439\n",
      "Epoch 308/1000 train loss: [array(0.09543861, dtype=float32)], val loss: 0.09612111002206802\n",
      "\t partial train loss (single batch): 0.094769\n",
      "Epoch 309/1000 train loss: [array(0.09476892, dtype=float32)], val loss: 0.09574424475431442\n",
      "\t partial train loss (single batch): 0.095302\n",
      "Epoch 310/1000 train loss: [array(0.09530195, dtype=float32)], val loss: 0.09536312520503998\n",
      "\t partial train loss (single batch): 0.096303\n",
      "Epoch 311/1000 train loss: [array(0.09630293, dtype=float32)], val loss: 0.09532507508993149\n",
      "\t partial train loss (single batch): 0.095621\n",
      "Epoch 312/1000 train loss: [array(0.09562083, dtype=float32)], val loss: 0.09524966031312943\n",
      "\t partial train loss (single batch): 0.095248\n",
      "Epoch 313/1000 train loss: [array(0.0952476, dtype=float32)], val loss: 0.09503637999296188\n",
      "\t partial train loss (single batch): 0.094239\n",
      "Epoch 314/1000 train loss: [array(0.09423894, dtype=float32)], val loss: 0.09417210519313812\n",
      "\t partial train loss (single batch): 0.094273\n",
      "Epoch 315/1000 train loss: [array(0.09427319, dtype=float32)], val loss: 0.09465997666120529\n",
      "\t partial train loss (single batch): 0.094906\n",
      "Epoch 316/1000 train loss: [array(0.09490553, dtype=float32)], val loss: 0.0931403636932373\n",
      "\t partial train loss (single batch): 0.094063\n",
      "Epoch 317/1000 train loss: [array(0.09406278, dtype=float32)], val loss: 0.09294454008340836\n",
      "\t partial train loss (single batch): 0.094326\n",
      "Epoch 318/1000 train loss: [array(0.09432576, dtype=float32)], val loss: 0.09394397586584091\n",
      "\t partial train loss (single batch): 0.093049\n",
      "Epoch 319/1000 train loss: [array(0.09304859, dtype=float32)], val loss: 0.09283459186553955\n",
      "\t partial train loss (single batch): 0.093455\n",
      "Epoch 320/1000 train loss: [array(0.09345476, dtype=float32)], val loss: 0.09371915459632874\n",
      "\t partial train loss (single batch): 0.093751\n",
      "Epoch 321/1000 train loss: [array(0.09375128, dtype=float32)], val loss: 0.09458662569522858\n",
      "\t partial train loss (single batch): 0.093448\n",
      "Epoch 322/1000 train loss: [array(0.09344811, dtype=float32)], val loss: 0.09269586205482483\n",
      "\t partial train loss (single batch): 0.092768\n",
      "Epoch 323/1000 train loss: [array(0.09276762, dtype=float32)], val loss: 0.09312556684017181\n",
      "\t partial train loss (single batch): 0.092656\n",
      "Epoch 324/1000 train loss: [array(0.09265561, dtype=float32)], val loss: 0.09285996109247208\n",
      "\t partial train loss (single batch): 0.093355\n",
      "Epoch 325/1000 train loss: [array(0.09335507, dtype=float32)], val loss: 0.09321369975805283\n",
      "\t partial train loss (single batch): 0.092574\n",
      "Epoch 326/1000 train loss: [array(0.09257402, dtype=float32)], val loss: 0.09185607731342316\n",
      "\t partial train loss (single batch): 0.092715\n",
      "Epoch 327/1000 train loss: [array(0.09271464, dtype=float32)], val loss: 0.09295357018709183\n",
      "\t partial train loss (single batch): 0.092432\n",
      "Epoch 328/1000 train loss: [array(0.09243152, dtype=float32)], val loss: 0.09172763675451279\n",
      "\t partial train loss (single batch): 0.092085\n",
      "Epoch 329/1000 train loss: [array(0.09208496, dtype=float32)], val loss: 0.09237365424633026\n",
      "\t partial train loss (single batch): 0.092789\n",
      "Epoch 330/1000 train loss: [array(0.09278855, dtype=float32)], val loss: 0.09231582283973694\n",
      "\t partial train loss (single batch): 0.092890\n",
      "Epoch 331/1000 train loss: [array(0.09289002, dtype=float32)], val loss: 0.09297017753124237\n",
      "\t partial train loss (single batch): 0.093873\n",
      "Epoch 332/1000 train loss: [array(0.0938727, dtype=float32)], val loss: 0.09188193082809448\n",
      "\t partial train loss (single batch): 0.091829\n",
      "Epoch 333/1000 train loss: [array(0.09182901, dtype=float32)], val loss: 0.09174220263957977\n",
      "\t partial train loss (single batch): 0.091673\n",
      "Epoch 334/1000 train loss: [array(0.09167252, dtype=float32)], val loss: 0.09165097028017044\n",
      "\t partial train loss (single batch): 0.090437\n",
      "Epoch 335/1000 train loss: [array(0.09043706, dtype=float32)], val loss: 0.09249599277973175\n",
      "\t partial train loss (single batch): 0.090588\n",
      "Epoch 336/1000 train loss: [array(0.09058773, dtype=float32)], val loss: 0.09050097316503525\n",
      "\t partial train loss (single batch): 0.090610\n",
      "Epoch 337/1000 train loss: [array(0.09061005, dtype=float32)], val loss: 0.09144771099090576\n",
      "\t partial train loss (single batch): 0.091579\n",
      "Epoch 338/1000 train loss: [array(0.09157909, dtype=float32)], val loss: 0.09224153310060501\n",
      "\t partial train loss (single batch): 0.090730\n",
      "Epoch 339/1000 train loss: [array(0.09073026, dtype=float32)], val loss: 0.09124521166086197\n",
      "\t partial train loss (single batch): 0.090955\n",
      "Epoch 340/1000 train loss: [array(0.09095479, dtype=float32)], val loss: 0.0904822126030922\n",
      "\t partial train loss (single batch): 0.089137\n",
      "Epoch 341/1000 train loss: [array(0.08913746, dtype=float32)], val loss: 0.09170331805944443\n",
      "\t partial train loss (single batch): 0.091869\n",
      "Epoch 342/1000 train loss: [array(0.09186924, dtype=float32)], val loss: 0.09065795689821243\n",
      "\t partial train loss (single batch): 0.090438\n",
      "Epoch 343/1000 train loss: [array(0.09043823, dtype=float32)], val loss: 0.09131139516830444\n",
      "\t partial train loss (single batch): 0.090175\n",
      "Epoch 344/1000 train loss: [array(0.09017473, dtype=float32)], val loss: 0.08982476592063904\n",
      "\t partial train loss (single batch): 0.089837\n",
      "Epoch 345/1000 train loss: [array(0.08983675, dtype=float32)], val loss: 0.08989506959915161\n",
      "\t partial train loss (single batch): 0.090688\n",
      "Epoch 346/1000 train loss: [array(0.09068836, dtype=float32)], val loss: 0.08971572667360306\n",
      "\t partial train loss (single batch): 0.089733\n",
      "Epoch 347/1000 train loss: [array(0.08973271, dtype=float32)], val loss: 0.08999013900756836\n",
      "\t partial train loss (single batch): 0.089218\n",
      "Epoch 348/1000 train loss: [array(0.08921788, dtype=float32)], val loss: 0.09006650745868683\n",
      "\t partial train loss (single batch): 0.090361\n",
      "Epoch 349/1000 train loss: [array(0.09036069, dtype=float32)], val loss: 0.08977188169956207\n",
      "\t partial train loss (single batch): 0.089194\n",
      "Epoch 350/1000 train loss: [array(0.08919389, dtype=float32)], val loss: 0.08917132765054703\n",
      "\t partial train loss (single batch): 0.090165\n",
      "Epoch 351/1000 train loss: [array(0.09016546, dtype=float32)], val loss: 0.08941377699375153\n",
      "\t partial train loss (single batch): 0.089266\n",
      "Epoch 352/1000 train loss: [array(0.08926591, dtype=float32)], val loss: 0.08885142207145691\n",
      "\t partial train loss (single batch): 0.088520\n",
      "Epoch 353/1000 train loss: [array(0.08852033, dtype=float32)], val loss: 0.08855234086513519\n",
      "\t partial train loss (single batch): 0.089343\n",
      "Epoch 354/1000 train loss: [array(0.08934254, dtype=float32)], val loss: 0.08885470777750015\n",
      "\t partial train loss (single batch): 0.088141\n",
      "Epoch 355/1000 train loss: [array(0.08814084, dtype=float32)], val loss: 0.08847199380397797\n",
      "\t partial train loss (single batch): 0.088856\n",
      "Epoch 356/1000 train loss: [array(0.08885551, dtype=float32)], val loss: 0.08783834427595139\n",
      "\t partial train loss (single batch): 0.089100\n",
      "Epoch 357/1000 train loss: [array(0.08909982, dtype=float32)], val loss: 0.08806904405355453\n",
      "\t partial train loss (single batch): 0.088680\n",
      "Epoch 358/1000 train loss: [array(0.08868004, dtype=float32)], val loss: 0.08858203142881393\n",
      "\t partial train loss (single batch): 0.088262\n",
      "Epoch 359/1000 train loss: [array(0.08826213, dtype=float32)], val loss: 0.0888826847076416\n",
      "\t partial train loss (single batch): 0.088553\n",
      "Epoch 360/1000 train loss: [array(0.08855344, dtype=float32)], val loss: 0.08772554993629456\n",
      "\t partial train loss (single batch): 0.087559\n",
      "Epoch 361/1000 train loss: [array(0.08755948, dtype=float32)], val loss: 0.08736909180879593\n",
      "\t partial train loss (single batch): 0.088069\n",
      "Epoch 362/1000 train loss: [array(0.08806893, dtype=float32)], val loss: 0.08752157539129257\n",
      "\t partial train loss (single batch): 0.087799\n",
      "Epoch 363/1000 train loss: [array(0.08779934, dtype=float32)], val loss: 0.08804267644882202\n",
      "\t partial train loss (single batch): 0.087221\n",
      "Epoch 364/1000 train loss: [array(0.08722105, dtype=float32)], val loss: 0.08716724067926407\n",
      "\t partial train loss (single batch): 0.087251\n",
      "Epoch 365/1000 train loss: [array(0.08725144, dtype=float32)], val loss: 0.08718038350343704\n",
      "\t partial train loss (single batch): 0.087323\n",
      "Epoch 366/1000 train loss: [array(0.08732269, dtype=float32)], val loss: 0.08857830613851547\n",
      "\t partial train loss (single batch): 0.087130\n",
      "Epoch 367/1000 train loss: [array(0.08713047, dtype=float32)], val loss: 0.08658573776483536\n",
      "\t partial train loss (single batch): 0.086777\n",
      "Epoch 368/1000 train loss: [array(0.08677651, dtype=float32)], val loss: 0.08711082488298416\n",
      "\t partial train loss (single batch): 0.086361\n",
      "Epoch 369/1000 train loss: [array(0.08636081, dtype=float32)], val loss: 0.08701246976852417\n",
      "\t partial train loss (single batch): 0.085918\n",
      "Epoch 370/1000 train loss: [array(0.08591761, dtype=float32)], val loss: 0.08611071854829788\n",
      "\t partial train loss (single batch): 0.085900\n",
      "Epoch 371/1000 train loss: [array(0.08589994, dtype=float32)], val loss: 0.0860910415649414\n",
      "\t partial train loss (single batch): 0.086867\n",
      "Epoch 372/1000 train loss: [array(0.08686706, dtype=float32)], val loss: 0.0872243121266365\n",
      "\t partial train loss (single batch): 0.086725\n",
      "Epoch 373/1000 train loss: [array(0.08672496, dtype=float32)], val loss: 0.0869833454489708\n",
      "\t partial train loss (single batch): 0.085202\n",
      "Epoch 374/1000 train loss: [array(0.0852017, dtype=float32)], val loss: 0.08538972586393356\n",
      "\t partial train loss (single batch): 0.086487\n",
      "Epoch 375/1000 train loss: [array(0.08648735, dtype=float32)], val loss: 0.08620130270719528\n",
      "\t partial train loss (single batch): 0.085523\n",
      "Epoch 376/1000 train loss: [array(0.08552273, dtype=float32)], val loss: 0.08626031130552292\n",
      "\t partial train loss (single batch): 0.085561\n",
      "Epoch 377/1000 train loss: [array(0.08556068, dtype=float32)], val loss: 0.08577628433704376\n",
      "\t partial train loss (single batch): 0.086530\n",
      "Epoch 378/1000 train loss: [array(0.0865299, dtype=float32)], val loss: 0.08636748790740967\n",
      "\t partial train loss (single batch): 0.086367\n",
      "Epoch 379/1000 train loss: [array(0.08636695, dtype=float32)], val loss: 0.08598063886165619\n",
      "\t partial train loss (single batch): 0.085387\n",
      "Epoch 380/1000 train loss: [array(0.08538681, dtype=float32)], val loss: 0.08526619523763657\n",
      "\t partial train loss (single batch): 0.085532\n",
      "Epoch 381/1000 train loss: [array(0.08553171, dtype=float32)], val loss: 0.08470863848924637\n",
      "\t partial train loss (single batch): 0.085679\n",
      "Epoch 382/1000 train loss: [array(0.08567888, dtype=float32)], val loss: 0.0853000283241272\n",
      "\t partial train loss (single batch): 0.086040\n",
      "Epoch 383/1000 train loss: [array(0.08603971, dtype=float32)], val loss: 0.08532164990901947\n",
      "\t partial train loss (single batch): 0.085936\n",
      "Epoch 384/1000 train loss: [array(0.08593556, dtype=float32)], val loss: 0.08676950633525848\n",
      "\t partial train loss (single batch): 0.084899\n",
      "Epoch 385/1000 train loss: [array(0.08489878, dtype=float32)], val loss: 0.08630930632352829\n",
      "\t partial train loss (single batch): 0.084262\n",
      "Epoch 386/1000 train loss: [array(0.08426213, dtype=float32)], val loss: 0.0847110003232956\n",
      "\t partial train loss (single batch): 0.084735\n",
      "Epoch 387/1000 train loss: [array(0.08473454, dtype=float32)], val loss: 0.08519536256790161\n",
      "\t partial train loss (single batch): 0.085397\n",
      "Epoch 388/1000 train loss: [array(0.08539747, dtype=float32)], val loss: 0.08607550710439682\n",
      "\t partial train loss (single batch): 0.084437\n",
      "Epoch 389/1000 train loss: [array(0.08443663, dtype=float32)], val loss: 0.08467420935630798\n",
      "\t partial train loss (single batch): 0.084557\n",
      "Epoch 390/1000 train loss: [array(0.08455671, dtype=float32)], val loss: 0.08371423929929733\n",
      "\t partial train loss (single batch): 0.084655\n",
      "Epoch 391/1000 train loss: [array(0.08465523, dtype=float32)], val loss: 0.08472084999084473\n",
      "\t partial train loss (single batch): 0.083950\n",
      "Epoch 392/1000 train loss: [array(0.08395036, dtype=float32)], val loss: 0.08448192477226257\n",
      "\t partial train loss (single batch): 0.083638\n",
      "Epoch 393/1000 train loss: [array(0.08363827, dtype=float32)], val loss: 0.0846172422170639\n",
      "\t partial train loss (single batch): 0.084848\n",
      "Epoch 394/1000 train loss: [array(0.08484821, dtype=float32)], val loss: 0.08539070188999176\n",
      "\t partial train loss (single batch): 0.084760\n",
      "Epoch 395/1000 train loss: [array(0.08475992, dtype=float32)], val loss: 0.0845232754945755\n",
      "\t partial train loss (single batch): 0.083723\n",
      "Epoch 396/1000 train loss: [array(0.08372255, dtype=float32)], val loss: 0.08396342396736145\n",
      "\t partial train loss (single batch): 0.084177\n",
      "Epoch 397/1000 train loss: [array(0.08417749, dtype=float32)], val loss: 0.08453945815563202\n",
      "\t partial train loss (single batch): 0.084620\n",
      "Epoch 398/1000 train loss: [array(0.08461973, dtype=float32)], val loss: 0.08465313166379929\n",
      "\t partial train loss (single batch): 0.083666\n",
      "Epoch 399/1000 train loss: [array(0.08366583, dtype=float32)], val loss: 0.08446191251277924\n",
      "\t partial train loss (single batch): 0.083522\n",
      "Epoch 400/1000 train loss: [array(0.08352202, dtype=float32)], val loss: 0.08449526876211166\n",
      "\t partial train loss (single batch): 0.084442\n",
      "Epoch 401/1000 train loss: [array(0.08444188, dtype=float32)], val loss: 0.08607176691293716\n",
      "\t partial train loss (single batch): 0.083469\n",
      "Epoch 402/1000 train loss: [array(0.08346934, dtype=float32)], val loss: 0.08321548998355865\n",
      "\t partial train loss (single batch): 0.082814\n",
      "Epoch 403/1000 train loss: [array(0.08281381, dtype=float32)], val loss: 0.08277301490306854\n",
      "\t partial train loss (single batch): 0.083571\n",
      "Epoch 404/1000 train loss: [array(0.08357083, dtype=float32)], val loss: 0.08442195504903793\n",
      "\t partial train loss (single batch): 0.084232\n",
      "Epoch 405/1000 train loss: [array(0.08423188, dtype=float32)], val loss: 0.08351856470108032\n",
      "\t partial train loss (single batch): 0.083398\n",
      "Epoch 406/1000 train loss: [array(0.08339774, dtype=float32)], val loss: 0.08430539071559906\n",
      "\t partial train loss (single batch): 0.082741\n",
      "Epoch 407/1000 train loss: [array(0.08274131, dtype=float32)], val loss: 0.08291462808847427\n",
      "\t partial train loss (single batch): 0.082948\n",
      "Epoch 408/1000 train loss: [array(0.08294836, dtype=float32)], val loss: 0.08288826048374176\n",
      "\t partial train loss (single batch): 0.082725\n",
      "Epoch 409/1000 train loss: [array(0.0827252, dtype=float32)], val loss: 0.08379775285720825\n",
      "\t partial train loss (single batch): 0.082888\n",
      "Epoch 410/1000 train loss: [array(0.08288808, dtype=float32)], val loss: 0.08332179486751556\n",
      "\t partial train loss (single batch): 0.083018\n",
      "Epoch 411/1000 train loss: [array(0.08301843, dtype=float32)], val loss: 0.08326929062604904\n",
      "\t partial train loss (single batch): 0.082583\n",
      "Epoch 412/1000 train loss: [array(0.08258318, dtype=float32)], val loss: 0.08262527734041214\n",
      "\t partial train loss (single batch): 0.082621\n",
      "Epoch 413/1000 train loss: [array(0.08262115, dtype=float32)], val loss: 0.08238492906093597\n",
      "\t partial train loss (single batch): 0.083238\n",
      "Epoch 414/1000 train loss: [array(0.08323768, dtype=float32)], val loss: 0.08381625264883041\n",
      "\t partial train loss (single batch): 0.083321\n",
      "Epoch 415/1000 train loss: [array(0.08332105, dtype=float32)], val loss: 0.08363064378499985\n",
      "\t partial train loss (single batch): 0.083663\n",
      "Epoch 416/1000 train loss: [array(0.08366268, dtype=float32)], val loss: 0.0825723260641098\n",
      "\t partial train loss (single batch): 0.082946\n",
      "Epoch 417/1000 train loss: [array(0.08294585, dtype=float32)], val loss: 0.08210406452417374\n",
      "\t partial train loss (single batch): 0.082483\n",
      "Epoch 418/1000 train loss: [array(0.08248264, dtype=float32)], val loss: 0.08270964026451111\n",
      "\t partial train loss (single batch): 0.082128\n",
      "Epoch 419/1000 train loss: [array(0.08212788, dtype=float32)], val loss: 0.0830584391951561\n",
      "\t partial train loss (single batch): 0.082029\n",
      "Epoch 420/1000 train loss: [array(0.08202904, dtype=float32)], val loss: 0.08307243883609772\n",
      "\t partial train loss (single batch): 0.082765\n",
      "Epoch 421/1000 train loss: [array(0.08276477, dtype=float32)], val loss: 0.08184054493904114\n",
      "\t partial train loss (single batch): 0.082245\n",
      "Epoch 422/1000 train loss: [array(0.08224496, dtype=float32)], val loss: 0.0830332487821579\n",
      "\t partial train loss (single batch): 0.081943\n",
      "Epoch 423/1000 train loss: [array(0.08194289, dtype=float32)], val loss: 0.08176138252019882\n",
      "\t partial train loss (single batch): 0.081411\n",
      "Epoch 424/1000 train loss: [array(0.08141105, dtype=float32)], val loss: 0.08251175284385681\n",
      "\t partial train loss (single batch): 0.081452\n",
      "Epoch 425/1000 train loss: [array(0.08145195, dtype=float32)], val loss: 0.08207646757364273\n",
      "\t partial train loss (single batch): 0.081585\n",
      "Epoch 426/1000 train loss: [array(0.08158531, dtype=float32)], val loss: 0.08218913525342941\n",
      "\t partial train loss (single batch): 0.082712\n",
      "Epoch 427/1000 train loss: [array(0.08271222, dtype=float32)], val loss: 0.08239180594682693\n",
      "\t partial train loss (single batch): 0.081723\n",
      "Epoch 428/1000 train loss: [array(0.08172321, dtype=float32)], val loss: 0.08197014033794403\n",
      "\t partial train loss (single batch): 0.081258\n",
      "Epoch 429/1000 train loss: [array(0.08125803, dtype=float32)], val loss: 0.08215101808309555\n",
      "\t partial train loss (single batch): 0.081660\n",
      "Epoch 430/1000 train loss: [array(0.08166046, dtype=float32)], val loss: 0.08136607706546783\n",
      "\t partial train loss (single batch): 0.082095\n",
      "Epoch 431/1000 train loss: [array(0.08209451, dtype=float32)], val loss: 0.08139684051275253\n",
      "\t partial train loss (single batch): 0.081548\n",
      "Epoch 432/1000 train loss: [array(0.08154826, dtype=float32)], val loss: 0.08066769689321518\n",
      "\t partial train loss (single batch): 0.081360\n",
      "Epoch 433/1000 train loss: [array(0.08136041, dtype=float32)], val loss: 0.08069784194231033\n",
      "\t partial train loss (single batch): 0.080785\n",
      "Epoch 434/1000 train loss: [array(0.08078462, dtype=float32)], val loss: 0.08138373494148254\n",
      "\t partial train loss (single batch): 0.080429\n",
      "Epoch 435/1000 train loss: [array(0.0804288, dtype=float32)], val loss: 0.08085393905639648\n",
      "\t partial train loss (single batch): 0.080443\n",
      "Epoch 436/1000 train loss: [array(0.08044319, dtype=float32)], val loss: 0.081360824406147\n",
      "\t partial train loss (single batch): 0.080362\n",
      "Epoch 437/1000 train loss: [array(0.08036198, dtype=float32)], val loss: 0.0816177949309349\n",
      "\t partial train loss (single batch): 0.080237\n",
      "Epoch 438/1000 train loss: [array(0.0802368, dtype=float32)], val loss: 0.08100557327270508\n",
      "\t partial train loss (single batch): 0.080259\n",
      "Epoch 439/1000 train loss: [array(0.08025882, dtype=float32)], val loss: 0.08099991083145142\n",
      "\t partial train loss (single batch): 0.080888\n",
      "Epoch 440/1000 train loss: [array(0.08088782, dtype=float32)], val loss: 0.0802132710814476\n",
      "\t partial train loss (single batch): 0.080597\n",
      "Epoch 441/1000 train loss: [array(0.08059748, dtype=float32)], val loss: 0.08025384694337845\n",
      "\t partial train loss (single batch): 0.081097\n",
      "Epoch 442/1000 train loss: [array(0.08109687, dtype=float32)], val loss: 0.08046237379312515\n",
      "\t partial train loss (single batch): 0.081384\n",
      "Epoch 443/1000 train loss: [array(0.08138358, dtype=float32)], val loss: 0.08061962574720383\n",
      "\t partial train loss (single batch): 0.079962\n",
      "Epoch 444/1000 train loss: [array(0.07996248, dtype=float32)], val loss: 0.08091899007558823\n",
      "\t partial train loss (single batch): 0.080296\n",
      "Epoch 445/1000 train loss: [array(0.08029628, dtype=float32)], val loss: 0.08048781752586365\n",
      "\t partial train loss (single batch): 0.080791\n",
      "Epoch 446/1000 train loss: [array(0.08079119, dtype=float32)], val loss: 0.07966692745685577\n",
      "\t partial train loss (single batch): 0.081107\n",
      "Epoch 447/1000 train loss: [array(0.08110651, dtype=float32)], val loss: 0.08074614405632019\n",
      "\t partial train loss (single batch): 0.079956\n",
      "Epoch 448/1000 train loss: [array(0.07995646, dtype=float32)], val loss: 0.07970698922872543\n",
      "\t partial train loss (single batch): 0.079943\n",
      "Epoch 449/1000 train loss: [array(0.07994283, dtype=float32)], val loss: 0.0794573649764061\n",
      "\t partial train loss (single batch): 0.080110\n",
      "Epoch 450/1000 train loss: [array(0.08010964, dtype=float32)], val loss: 0.0816781297326088\n",
      "\t partial train loss (single batch): 0.080467\n",
      "Epoch 451/1000 train loss: [array(0.0804667, dtype=float32)], val loss: 0.08006758987903595\n",
      "\t partial train loss (single batch): 0.079254\n",
      "Epoch 452/1000 train loss: [array(0.07925351, dtype=float32)], val loss: 0.07998425513505936\n",
      "\t partial train loss (single batch): 0.079837\n",
      "Epoch 453/1000 train loss: [array(0.07983696, dtype=float32)], val loss: 0.07952652126550674\n",
      "\t partial train loss (single batch): 0.080411\n",
      "Epoch 454/1000 train loss: [array(0.08041137, dtype=float32)], val loss: 0.0796751007437706\n",
      "\t partial train loss (single batch): 0.079733\n",
      "Epoch 455/1000 train loss: [array(0.07973343, dtype=float32)], val loss: 0.0790485218167305\n",
      "\t partial train loss (single batch): 0.079467\n",
      "Epoch 456/1000 train loss: [array(0.07946656, dtype=float32)], val loss: 0.08011840283870697\n",
      "\t partial train loss (single batch): 0.079674\n",
      "Epoch 457/1000 train loss: [array(0.07967361, dtype=float32)], val loss: 0.08104333281517029\n",
      "\t partial train loss (single batch): 0.079374\n",
      "Epoch 458/1000 train loss: [array(0.07937408, dtype=float32)], val loss: 0.08062762022018433\n",
      "\t partial train loss (single batch): 0.079689\n",
      "Epoch 459/1000 train loss: [array(0.07968943, dtype=float32)], val loss: 0.08001913875341415\n",
      "\t partial train loss (single batch): 0.078910\n",
      "Epoch 460/1000 train loss: [array(0.07891018, dtype=float32)], val loss: 0.07881186902523041\n",
      "\t partial train loss (single batch): 0.079265\n",
      "Epoch 461/1000 train loss: [array(0.07926512, dtype=float32)], val loss: 0.08053700625896454\n",
      "\t partial train loss (single batch): 0.078479\n",
      "Epoch 462/1000 train loss: [array(0.07847918, dtype=float32)], val loss: 0.08072859793901443\n",
      "\t partial train loss (single batch): 0.080294\n",
      "Epoch 463/1000 train loss: [array(0.08029393, dtype=float32)], val loss: 0.07945448160171509\n",
      "\t partial train loss (single batch): 0.079237\n",
      "Epoch 464/1000 train loss: [array(0.07923686, dtype=float32)], val loss: 0.07825454324483871\n",
      "\t partial train loss (single batch): 0.079868\n",
      "Epoch 465/1000 train loss: [array(0.07986809, dtype=float32)], val loss: 0.07934066653251648\n",
      "\t partial train loss (single batch): 0.077276\n",
      "Epoch 466/1000 train loss: [array(0.07727632, dtype=float32)], val loss: 0.07891660183668137\n",
      "\t partial train loss (single batch): 0.079232\n",
      "Epoch 467/1000 train loss: [array(0.07923212, dtype=float32)], val loss: 0.07912110537290573\n",
      "\t partial train loss (single batch): 0.078694\n",
      "Epoch 468/1000 train loss: [array(0.07869355, dtype=float32)], val loss: 0.07919224351644516\n",
      "\t partial train loss (single batch): 0.078811\n",
      "Epoch 469/1000 train loss: [array(0.07881102, dtype=float32)], val loss: 0.07885202020406723\n",
      "\t partial train loss (single batch): 0.079625\n",
      "Epoch 470/1000 train loss: [array(0.07962535, dtype=float32)], val loss: 0.07867186516523361\n",
      "\t partial train loss (single batch): 0.078938\n",
      "Epoch 471/1000 train loss: [array(0.07893772, dtype=float32)], val loss: 0.07814562320709229\n",
      "\t partial train loss (single batch): 0.078510\n",
      "Epoch 472/1000 train loss: [array(0.07850986, dtype=float32)], val loss: 0.0789642482995987\n",
      "\t partial train loss (single batch): 0.078289\n",
      "Epoch 473/1000 train loss: [array(0.07828926, dtype=float32)], val loss: 0.07875649631023407\n",
      "\t partial train loss (single batch): 0.077474\n",
      "Epoch 474/1000 train loss: [array(0.07747439, dtype=float32)], val loss: 0.07864142209291458\n",
      "\t partial train loss (single batch): 0.078355\n",
      "Epoch 475/1000 train loss: [array(0.07835469, dtype=float32)], val loss: 0.07927950471639633\n",
      "\t partial train loss (single batch): 0.077683\n",
      "Epoch 476/1000 train loss: [array(0.07768334, dtype=float32)], val loss: 0.08042331784963608\n",
      "\t partial train loss (single batch): 0.078156\n",
      "Epoch 477/1000 train loss: [array(0.07815645, dtype=float32)], val loss: 0.07991764694452286\n",
      "\t partial train loss (single batch): 0.078978\n",
      "Epoch 478/1000 train loss: [array(0.07897836, dtype=float32)], val loss: 0.07877472043037415\n",
      "\t partial train loss (single batch): 0.078290\n",
      "Epoch 479/1000 train loss: [array(0.07828972, dtype=float32)], val loss: 0.07778183370828629\n",
      "\t partial train loss (single batch): 0.078322\n",
      "Epoch 480/1000 train loss: [array(0.07832167, dtype=float32)], val loss: 0.07696273922920227\n",
      "\t partial train loss (single batch): 0.077787\n",
      "Epoch 481/1000 train loss: [array(0.077787, dtype=float32)], val loss: 0.07819439470767975\n",
      "\t partial train loss (single batch): 0.078428\n",
      "Epoch 482/1000 train loss: [array(0.07842755, dtype=float32)], val loss: 0.07868804782629013\n",
      "\t partial train loss (single batch): 0.076428\n",
      "Epoch 483/1000 train loss: [array(0.07642795, dtype=float32)], val loss: 0.07932239770889282\n",
      "\t partial train loss (single batch): 0.076514\n",
      "Epoch 484/1000 train loss: [array(0.07651412, dtype=float32)], val loss: 0.078560471534729\n",
      "\t partial train loss (single batch): 0.078864\n",
      "Epoch 485/1000 train loss: [array(0.0788644, dtype=float32)], val loss: 0.07772903889417648\n",
      "\t partial train loss (single batch): 0.078222\n",
      "Epoch 486/1000 train loss: [array(0.07822242, dtype=float32)], val loss: 0.07813161611557007\n",
      "\t partial train loss (single batch): 0.078246\n",
      "Epoch 487/1000 train loss: [array(0.07824633, dtype=float32)], val loss: 0.07780188322067261\n",
      "\t partial train loss (single batch): 0.078490\n",
      "Epoch 488/1000 train loss: [array(0.07848997, dtype=float32)], val loss: 0.07824317365884781\n",
      "\t partial train loss (single batch): 0.076898\n",
      "Epoch 489/1000 train loss: [array(0.07689814, dtype=float32)], val loss: 0.0775412991642952\n",
      "\t partial train loss (single batch): 0.076946\n",
      "Epoch 490/1000 train loss: [array(0.07694565, dtype=float32)], val loss: 0.07717407494783401\n",
      "\t partial train loss (single batch): 0.077237\n",
      "Epoch 491/1000 train loss: [array(0.07723723, dtype=float32)], val loss: 0.07814831286668777\n",
      "\t partial train loss (single batch): 0.076713\n",
      "Epoch 492/1000 train loss: [array(0.07671259, dtype=float32)], val loss: 0.07650498300790787\n",
      "\t partial train loss (single batch): 0.077731\n",
      "Epoch 493/1000 train loss: [array(0.07773109, dtype=float32)], val loss: 0.0781530886888504\n",
      "\t partial train loss (single batch): 0.077710\n",
      "Epoch 494/1000 train loss: [array(0.07770954, dtype=float32)], val loss: 0.07809550315141678\n",
      "\t partial train loss (single batch): 0.076587\n",
      "Epoch 495/1000 train loss: [array(0.07658687, dtype=float32)], val loss: 0.07858995348215103\n",
      "\t partial train loss (single batch): 0.076561\n",
      "Epoch 496/1000 train loss: [array(0.07656066, dtype=float32)], val loss: 0.07644675672054291\n",
      "\t partial train loss (single batch): 0.077007\n",
      "Epoch 497/1000 train loss: [array(0.07700695, dtype=float32)], val loss: 0.07811618596315384\n",
      "\t partial train loss (single batch): 0.076432\n",
      "Epoch 498/1000 train loss: [array(0.07643174, dtype=float32)], val loss: 0.07809345424175262\n",
      "\t partial train loss (single batch): 0.078097\n",
      "Epoch 499/1000 train loss: [array(0.07809713, dtype=float32)], val loss: 0.07732700556516647\n",
      "\t partial train loss (single batch): 0.076807\n",
      "Epoch 500/1000 train loss: [array(0.07680658, dtype=float32)], val loss: 0.07812264561653137\n",
      "\t partial train loss (single batch): 0.076953\n",
      "Epoch 501/1000 train loss: [array(0.07695296, dtype=float32)], val loss: 0.07695117592811584\n",
      "\t partial train loss (single batch): 0.076903\n",
      "Epoch 502/1000 train loss: [array(0.07690295, dtype=float32)], val loss: 0.07656192034482956\n",
      "\t partial train loss (single batch): 0.075827\n",
      "Epoch 503/1000 train loss: [array(0.07582717, dtype=float32)], val loss: 0.07742112129926682\n",
      "\t partial train loss (single batch): 0.077494\n",
      "Epoch 504/1000 train loss: [array(0.07749406, dtype=float32)], val loss: 0.07769959419965744\n",
      "\t partial train loss (single batch): 0.076957\n",
      "Epoch 505/1000 train loss: [array(0.07695712, dtype=float32)], val loss: 0.0783127173781395\n",
      "\t partial train loss (single batch): 0.076275\n",
      "Epoch 506/1000 train loss: [array(0.0762755, dtype=float32)], val loss: 0.07692379504442215\n",
      "\t partial train loss (single batch): 0.076683\n",
      "Epoch 507/1000 train loss: [array(0.07668263, dtype=float32)], val loss: 0.07754189521074295\n",
      "\t partial train loss (single batch): 0.076032\n",
      "Epoch 508/1000 train loss: [array(0.07603249, dtype=float32)], val loss: 0.07743267714977264\n",
      "\t partial train loss (single batch): 0.076794\n",
      "Epoch 509/1000 train loss: [array(0.07679363, dtype=float32)], val loss: 0.07620256394147873\n",
      "\t partial train loss (single batch): 0.077219\n",
      "Epoch 510/1000 train loss: [array(0.07721855, dtype=float32)], val loss: 0.07697311788797379\n",
      "\t partial train loss (single batch): 0.076570\n",
      "Epoch 511/1000 train loss: [array(0.07656988, dtype=float32)], val loss: 0.07590240985155106\n",
      "\t partial train loss (single batch): 0.076761\n",
      "Epoch 512/1000 train loss: [array(0.07676076, dtype=float32)], val loss: 0.07476983219385147\n",
      "\t partial train loss (single batch): 0.077100\n",
      "Epoch 513/1000 train loss: [array(0.07710031, dtype=float32)], val loss: 0.07775436341762543\n",
      "\t partial train loss (single batch): 0.076291\n",
      "Epoch 514/1000 train loss: [array(0.07629121, dtype=float32)], val loss: 0.07711261510848999\n",
      "\t partial train loss (single batch): 0.077085\n",
      "Epoch 515/1000 train loss: [array(0.07708526, dtype=float32)], val loss: 0.07649511098861694\n",
      "\t partial train loss (single batch): 0.076226\n",
      "Epoch 516/1000 train loss: [array(0.07622636, dtype=float32)], val loss: 0.07646164298057556\n",
      "\t partial train loss (single batch): 0.075951\n",
      "Epoch 517/1000 train loss: [array(0.07595063, dtype=float32)], val loss: 0.0774516761302948\n",
      "\t partial train loss (single batch): 0.075867\n",
      "Epoch 518/1000 train loss: [array(0.07586684, dtype=float32)], val loss: 0.07651589810848236\n",
      "\t partial train loss (single batch): 0.076034\n",
      "Epoch 519/1000 train loss: [array(0.07603417, dtype=float32)], val loss: 0.07472187280654907\n",
      "\t partial train loss (single batch): 0.076029\n",
      "Epoch 520/1000 train loss: [array(0.07602884, dtype=float32)], val loss: 0.07646021991968155\n",
      "\t partial train loss (single batch): 0.075069\n",
      "Epoch 521/1000 train loss: [array(0.07506879, dtype=float32)], val loss: 0.0763804242014885\n",
      "\t partial train loss (single batch): 0.075758\n",
      "Epoch 522/1000 train loss: [array(0.07575808, dtype=float32)], val loss: 0.07635939121246338\n",
      "\t partial train loss (single batch): 0.075682\n",
      "Epoch 523/1000 train loss: [array(0.07568248, dtype=float32)], val loss: 0.07582451403141022\n",
      "\t partial train loss (single batch): 0.076457\n",
      "Epoch 524/1000 train loss: [array(0.07645744, dtype=float32)], val loss: 0.07566698640584946\n",
      "\t partial train loss (single batch): 0.076885\n",
      "Epoch 525/1000 train loss: [array(0.07688478, dtype=float32)], val loss: 0.07618919014930725\n",
      "\t partial train loss (single batch): 0.075824\n",
      "Epoch 526/1000 train loss: [array(0.07582423, dtype=float32)], val loss: 0.07597347348928452\n",
      "\t partial train loss (single batch): 0.075838\n",
      "Epoch 527/1000 train loss: [array(0.07583849, dtype=float32)], val loss: 0.07626045495271683\n",
      "\t partial train loss (single batch): 0.076032\n",
      "Epoch 528/1000 train loss: [array(0.07603242, dtype=float32)], val loss: 0.07664243876934052\n",
      "\t partial train loss (single batch): 0.075724\n",
      "Epoch 529/1000 train loss: [array(0.07572398, dtype=float32)], val loss: 0.0760306790471077\n",
      "\t partial train loss (single batch): 0.074597\n",
      "Epoch 530/1000 train loss: [array(0.07459722, dtype=float32)], val loss: 0.07545878738164902\n",
      "\t partial train loss (single batch): 0.075758\n",
      "Epoch 531/1000 train loss: [array(0.0757577, dtype=float32)], val loss: 0.07568079233169556\n",
      "\t partial train loss (single batch): 0.074499\n",
      "Epoch 532/1000 train loss: [array(0.07449887, dtype=float32)], val loss: 0.07490067183971405\n",
      "\t partial train loss (single batch): 0.076062\n",
      "Epoch 533/1000 train loss: [array(0.076062, dtype=float32)], val loss: 0.0759749487042427\n",
      "\t partial train loss (single batch): 0.075106\n",
      "Epoch 534/1000 train loss: [array(0.07510594, dtype=float32)], val loss: 0.07579275965690613\n",
      "\t partial train loss (single batch): 0.075599\n",
      "Epoch 535/1000 train loss: [array(0.07559868, dtype=float32)], val loss: 0.07576607167720795\n",
      "\t partial train loss (single batch): 0.076231\n",
      "Epoch 536/1000 train loss: [array(0.07623073, dtype=float32)], val loss: 0.07562214881181717\n",
      "\t partial train loss (single batch): 0.075411\n",
      "Epoch 537/1000 train loss: [array(0.07541117, dtype=float32)], val loss: 0.07523725181818008\n",
      "\t partial train loss (single batch): 0.075213\n",
      "Epoch 538/1000 train loss: [array(0.07521349, dtype=float32)], val loss: 0.07666874676942825\n",
      "\t partial train loss (single batch): 0.074590\n",
      "Epoch 539/1000 train loss: [array(0.07459039, dtype=float32)], val loss: 0.07423360645771027\n",
      "\t partial train loss (single batch): 0.075322\n",
      "Epoch 540/1000 train loss: [array(0.07532173, dtype=float32)], val loss: 0.07613075524568558\n",
      "\t partial train loss (single batch): 0.074347\n",
      "Epoch 541/1000 train loss: [array(0.07434693, dtype=float32)], val loss: 0.0751575455069542\n",
      "\t partial train loss (single batch): 0.075843\n",
      "Epoch 542/1000 train loss: [array(0.07584339, dtype=float32)], val loss: 0.07513043284416199\n",
      "\t partial train loss (single batch): 0.074377\n",
      "Epoch 543/1000 train loss: [array(0.07437744, dtype=float32)], val loss: 0.07540745288133621\n",
      "\t partial train loss (single batch): 0.075667\n",
      "Epoch 544/1000 train loss: [array(0.07566739, dtype=float32)], val loss: 0.07525863498449326\n",
      "\t partial train loss (single batch): 0.075649\n",
      "Epoch 545/1000 train loss: [array(0.07564948, dtype=float32)], val loss: 0.07472654432058334\n",
      "\t partial train loss (single batch): 0.074047\n",
      "Epoch 546/1000 train loss: [array(0.07404686, dtype=float32)], val loss: 0.07544984668493271\n",
      "\t partial train loss (single batch): 0.074307\n",
      "Epoch 547/1000 train loss: [array(0.07430702, dtype=float32)], val loss: 0.07572407275438309\n",
      "\t partial train loss (single batch): 0.074483\n",
      "Epoch 548/1000 train loss: [array(0.07448284, dtype=float32)], val loss: 0.07358884811401367\n",
      "\t partial train loss (single batch): 0.074347\n",
      "Epoch 549/1000 train loss: [array(0.07434709, dtype=float32)], val loss: 0.07541574537754059\n",
      "\t partial train loss (single batch): 0.074991\n",
      "Epoch 550/1000 train loss: [array(0.07499136, dtype=float32)], val loss: 0.07503156363964081\n",
      "\t partial train loss (single batch): 0.075161\n",
      "Epoch 551/1000 train loss: [array(0.0751611, dtype=float32)], val loss: 0.07513889670372009\n",
      "\t partial train loss (single batch): 0.073677\n",
      "Epoch 552/1000 train loss: [array(0.07367683, dtype=float32)], val loss: 0.07447901368141174\n",
      "\t partial train loss (single batch): 0.073377\n",
      "Epoch 553/1000 train loss: [array(0.07337745, dtype=float32)], val loss: 0.07534508407115936\n",
      "\t partial train loss (single batch): 0.075677\n",
      "Epoch 554/1000 train loss: [array(0.07567699, dtype=float32)], val loss: 0.0751248151063919\n",
      "\t partial train loss (single batch): 0.074627\n",
      "Epoch 555/1000 train loss: [array(0.07462737, dtype=float32)], val loss: 0.075088731944561\n",
      "\t partial train loss (single batch): 0.074439\n",
      "Epoch 556/1000 train loss: [array(0.07443873, dtype=float32)], val loss: 0.07432077825069427\n",
      "\t partial train loss (single batch): 0.073655\n",
      "Epoch 557/1000 train loss: [array(0.07365546, dtype=float32)], val loss: 0.0752657800912857\n",
      "\t partial train loss (single batch): 0.074774\n",
      "Epoch 558/1000 train loss: [array(0.07477447, dtype=float32)], val loss: 0.07380466908216476\n",
      "\t partial train loss (single batch): 0.074332\n",
      "Epoch 559/1000 train loss: [array(0.07433242, dtype=float32)], val loss: 0.07451416552066803\n",
      "\t partial train loss (single batch): 0.073192\n",
      "Epoch 560/1000 train loss: [array(0.07319184, dtype=float32)], val loss: 0.07380493730306625\n",
      "\t partial train loss (single batch): 0.073659\n",
      "Epoch 561/1000 train loss: [array(0.07365876, dtype=float32)], val loss: 0.07478561252355576\n",
      "\t partial train loss (single batch): 0.074439\n",
      "Epoch 562/1000 train loss: [array(0.0744385, dtype=float32)], val loss: 0.07324327528476715\n",
      "\t partial train loss (single batch): 0.072963\n",
      "Epoch 563/1000 train loss: [array(0.07296348, dtype=float32)], val loss: 0.07442423701286316\n",
      "\t partial train loss (single batch): 0.073957\n",
      "Epoch 564/1000 train loss: [array(0.07395665, dtype=float32)], val loss: 0.07362023741006851\n",
      "\t partial train loss (single batch): 0.074364\n",
      "Epoch 565/1000 train loss: [array(0.07436444, dtype=float32)], val loss: 0.0742579773068428\n",
      "\t partial train loss (single batch): 0.073998\n",
      "Epoch 566/1000 train loss: [array(0.07399825, dtype=float32)], val loss: 0.0732954889535904\n",
      "\t partial train loss (single batch): 0.074394\n",
      "Epoch 567/1000 train loss: [array(0.07439403, dtype=float32)], val loss: 0.0736570805311203\n",
      "\t partial train loss (single batch): 0.074633\n",
      "Epoch 568/1000 train loss: [array(0.07463302, dtype=float32)], val loss: 0.0751609280705452\n",
      "\t partial train loss (single batch): 0.075048\n",
      "Epoch 569/1000 train loss: [array(0.07504807, dtype=float32)], val loss: 0.07464566081762314\n",
      "\t partial train loss (single batch): 0.074616\n",
      "Epoch 570/1000 train loss: [array(0.07461608, dtype=float32)], val loss: 0.0732499212026596\n",
      "\t partial train loss (single batch): 0.074571\n",
      "Epoch 571/1000 train loss: [array(0.07457136, dtype=float32)], val loss: 0.07499738037586212\n",
      "\t partial train loss (single batch): 0.073624\n",
      "Epoch 572/1000 train loss: [array(0.07362358, dtype=float32)], val loss: 0.07447303831577301\n",
      "\t partial train loss (single batch): 0.074214\n",
      "Epoch 573/1000 train loss: [array(0.0742138, dtype=float32)], val loss: 0.07434897124767303\n",
      "\t partial train loss (single batch): 0.074405\n",
      "Epoch 574/1000 train loss: [array(0.07440499, dtype=float32)], val loss: 0.07583624869585037\n",
      "\t partial train loss (single batch): 0.074346\n",
      "Epoch 575/1000 train loss: [array(0.07434572, dtype=float32)], val loss: 0.0740756168961525\n",
      "\t partial train loss (single batch): 0.073876\n",
      "Epoch 576/1000 train loss: [array(0.07387585, dtype=float32)], val loss: 0.07402967661619186\n",
      "\t partial train loss (single batch): 0.073775\n",
      "Epoch 577/1000 train loss: [array(0.07377479, dtype=float32)], val loss: 0.0727386474609375\n",
      "\t partial train loss (single batch): 0.073728\n",
      "Epoch 578/1000 train loss: [array(0.07372816, dtype=float32)], val loss: 0.07277347892522812\n",
      "\t partial train loss (single batch): 0.073777\n",
      "Epoch 579/1000 train loss: [array(0.0737769, dtype=float32)], val loss: 0.07333240658044815\n",
      "\t partial train loss (single batch): 0.073916\n",
      "Epoch 580/1000 train loss: [array(0.07391601, dtype=float32)], val loss: 0.07420716434717178\n",
      "\t partial train loss (single batch): 0.074655\n",
      "Epoch 581/1000 train loss: [array(0.07465483, dtype=float32)], val loss: 0.07345574349164963\n",
      "\t partial train loss (single batch): 0.072978\n",
      "Epoch 582/1000 train loss: [array(0.07297846, dtype=float32)], val loss: 0.07375689595937729\n",
      "\t partial train loss (single batch): 0.072827\n",
      "Epoch 583/1000 train loss: [array(0.07282659, dtype=float32)], val loss: 0.07368263602256775\n",
      "\t partial train loss (single batch): 0.074512\n",
      "Epoch 584/1000 train loss: [array(0.07451246, dtype=float32)], val loss: 0.07289712876081467\n",
      "\t partial train loss (single batch): 0.073424\n",
      "Epoch 585/1000 train loss: [array(0.07342381, dtype=float32)], val loss: 0.07300004363059998\n",
      "\t partial train loss (single batch): 0.073144\n",
      "Epoch 586/1000 train loss: [array(0.07314396, dtype=float32)], val loss: 0.07315628230571747\n",
      "\t partial train loss (single batch): 0.072775\n",
      "Epoch 587/1000 train loss: [array(0.07277534, dtype=float32)], val loss: 0.07353481650352478\n",
      "\t partial train loss (single batch): 0.072868\n",
      "Epoch 588/1000 train loss: [array(0.07286805, dtype=float32)], val loss: 0.07335054129362106\n",
      "\t partial train loss (single batch): 0.074172\n",
      "Epoch 589/1000 train loss: [array(0.07417189, dtype=float32)], val loss: 0.07318679988384247\n",
      "\t partial train loss (single batch): 0.073646\n",
      "Epoch 590/1000 train loss: [array(0.07364567, dtype=float32)], val loss: 0.07307466119527817\n",
      "\t partial train loss (single batch): 0.073398\n",
      "Epoch 591/1000 train loss: [array(0.07339761, dtype=float32)], val loss: 0.07254341244697571\n",
      "\t partial train loss (single batch): 0.072136\n",
      "Epoch 592/1000 train loss: [array(0.07213645, dtype=float32)], val loss: 0.07328439503908157\n",
      "\t partial train loss (single batch): 0.073499\n",
      "Epoch 593/1000 train loss: [array(0.0734985, dtype=float32)], val loss: 0.0732436403632164\n",
      "\t partial train loss (single batch): 0.072433\n",
      "Epoch 594/1000 train loss: [array(0.07243261, dtype=float32)], val loss: 0.07425294816493988\n",
      "\t partial train loss (single batch): 0.072575\n",
      "Epoch 595/1000 train loss: [array(0.07257472, dtype=float32)], val loss: 0.0735674649477005\n",
      "\t partial train loss (single batch): 0.072281\n",
      "Epoch 596/1000 train loss: [array(0.07228131, dtype=float32)], val loss: 0.07338355481624603\n",
      "\t partial train loss (single batch): 0.071921\n",
      "Epoch 597/1000 train loss: [array(0.07192057, dtype=float32)], val loss: 0.07263465970754623\n",
      "\t partial train loss (single batch): 0.071854\n",
      "Epoch 598/1000 train loss: [array(0.07185363, dtype=float32)], val loss: 0.07360264658927917\n",
      "\t partial train loss (single batch): 0.072805\n",
      "Epoch 599/1000 train loss: [array(0.07280543, dtype=float32)], val loss: 0.07331475615501404\n",
      "\t partial train loss (single batch): 0.073463\n",
      "Epoch 600/1000 train loss: [array(0.07346306, dtype=float32)], val loss: 0.07355955988168716\n",
      "\t partial train loss (single batch): 0.072263\n",
      "Epoch 601/1000 train loss: [array(0.07226286, dtype=float32)], val loss: 0.07233360409736633\n",
      "\t partial train loss (single batch): 0.072126\n",
      "Epoch 602/1000 train loss: [array(0.07212611, dtype=float32)], val loss: 0.07225263863801956\n",
      "\t partial train loss (single batch): 0.073810\n",
      "Epoch 603/1000 train loss: [array(0.07381014, dtype=float32)], val loss: 0.07198291271924973\n",
      "\t partial train loss (single batch): 0.073267\n",
      "Epoch 604/1000 train loss: [array(0.07326702, dtype=float32)], val loss: 0.07284242659807205\n",
      "\t partial train loss (single batch): 0.072660\n",
      "Epoch 605/1000 train loss: [array(0.07265971, dtype=float32)], val loss: 0.07275936752557755\n",
      "\t partial train loss (single batch): 0.071923\n",
      "Epoch 606/1000 train loss: [array(0.07192346, dtype=float32)], val loss: 0.07285020500421524\n",
      "\t partial train loss (single batch): 0.073174\n",
      "Epoch 607/1000 train loss: [array(0.07317363, dtype=float32)], val loss: 0.07294151186943054\n",
      "\t partial train loss (single batch): 0.072996\n",
      "Epoch 608/1000 train loss: [array(0.07299636, dtype=float32)], val loss: 0.07302863895893097\n",
      "\t partial train loss (single batch): 0.072341\n",
      "Epoch 609/1000 train loss: [array(0.07234095, dtype=float32)], val loss: 0.07208895683288574\n",
      "\t partial train loss (single batch): 0.073134\n",
      "Epoch 610/1000 train loss: [array(0.07313424, dtype=float32)], val loss: 0.07275471091270447\n",
      "\t partial train loss (single batch): 0.071758\n",
      "Epoch 611/1000 train loss: [array(0.07175816, dtype=float32)], val loss: 0.07334941625595093\n",
      "\t partial train loss (single batch): 0.072603\n",
      "Epoch 612/1000 train loss: [array(0.07260261, dtype=float32)], val loss: 0.07332858443260193\n",
      "\t partial train loss (single batch): 0.072074\n",
      "Epoch 613/1000 train loss: [array(0.07207376, dtype=float32)], val loss: 0.07248756289482117\n",
      "\t partial train loss (single batch): 0.072623\n",
      "Epoch 614/1000 train loss: [array(0.0726231, dtype=float32)], val loss: 0.07302713394165039\n",
      "\t partial train loss (single batch): 0.072077\n",
      "Epoch 615/1000 train loss: [array(0.07207675, dtype=float32)], val loss: 0.07127616554498672\n",
      "\t partial train loss (single batch): 0.071772\n",
      "Epoch 616/1000 train loss: [array(0.07177223, dtype=float32)], val loss: 0.07292723655700684\n",
      "\t partial train loss (single batch): 0.072479\n",
      "Epoch 617/1000 train loss: [array(0.07247923, dtype=float32)], val loss: 0.0731593444943428\n",
      "\t partial train loss (single batch): 0.071901\n",
      "Epoch 618/1000 train loss: [array(0.07190149, dtype=float32)], val loss: 0.07296700030565262\n",
      "\t partial train loss (single batch): 0.072287\n",
      "Epoch 619/1000 train loss: [array(0.07228706, dtype=float32)], val loss: 0.07277248799800873\n",
      "\t partial train loss (single batch): 0.073363\n",
      "Epoch 620/1000 train loss: [array(0.07336278, dtype=float32)], val loss: 0.07246869802474976\n",
      "\t partial train loss (single batch): 0.072374\n",
      "Epoch 621/1000 train loss: [array(0.07237403, dtype=float32)], val loss: 0.07265745848417282\n",
      "\t partial train loss (single batch): 0.071105\n",
      "Epoch 622/1000 train loss: [array(0.07110518, dtype=float32)], val loss: 0.0719156339764595\n",
      "\t partial train loss (single batch): 0.072415\n",
      "Epoch 623/1000 train loss: [array(0.07241542, dtype=float32)], val loss: 0.07223060727119446\n",
      "\t partial train loss (single batch): 0.072763\n",
      "Epoch 624/1000 train loss: [array(0.07276253, dtype=float32)], val loss: 0.07283472269773483\n",
      "\t partial train loss (single batch): 0.071925\n",
      "Epoch 625/1000 train loss: [array(0.07192542, dtype=float32)], val loss: 0.07394561171531677\n",
      "\t partial train loss (single batch): 0.072450\n",
      "Epoch 626/1000 train loss: [array(0.07245003, dtype=float32)], val loss: 0.07166215032339096\n",
      "\t partial train loss (single batch): 0.071154\n",
      "Epoch 627/1000 train loss: [array(0.07115417, dtype=float32)], val loss: 0.07276080548763275\n",
      "\t partial train loss (single batch): 0.071095\n",
      "Epoch 628/1000 train loss: [array(0.07109478, dtype=float32)], val loss: 0.07321609556674957\n",
      "\t partial train loss (single batch): 0.071399\n",
      "Epoch 629/1000 train loss: [array(0.07139889, dtype=float32)], val loss: 0.07147244364023209\n",
      "\t partial train loss (single batch): 0.071345\n",
      "Epoch 630/1000 train loss: [array(0.07134468, dtype=float32)], val loss: 0.07154721021652222\n",
      "\t partial train loss (single batch): 0.072173\n",
      "Epoch 631/1000 train loss: [array(0.07217313, dtype=float32)], val loss: 0.07276778668165207\n",
      "\t partial train loss (single batch): 0.071352\n",
      "Epoch 632/1000 train loss: [array(0.07135171, dtype=float32)], val loss: 0.0723932757973671\n",
      "\t partial train loss (single batch): 0.071283\n",
      "Epoch 633/1000 train loss: [array(0.07128338, dtype=float32)], val loss: 0.0721399337053299\n",
      "\t partial train loss (single batch): 0.071907\n",
      "Epoch 634/1000 train loss: [array(0.07190675, dtype=float32)], val loss: 0.07199405133724213\n",
      "\t partial train loss (single batch): 0.071819\n",
      "Epoch 635/1000 train loss: [array(0.0718189, dtype=float32)], val loss: 0.07101854681968689\n",
      "\t partial train loss (single batch): 0.070638\n",
      "Epoch 636/1000 train loss: [array(0.07063846, dtype=float32)], val loss: 0.07175061851739883\n",
      "\t partial train loss (single batch): 0.071674\n",
      "Epoch 637/1000 train loss: [array(0.07167364, dtype=float32)], val loss: 0.07136905938386917\n",
      "\t partial train loss (single batch): 0.071861\n",
      "Epoch 638/1000 train loss: [array(0.0718606, dtype=float32)], val loss: 0.07213648408651352\n",
      "\t partial train loss (single batch): 0.071224\n",
      "Epoch 639/1000 train loss: [array(0.07122383, dtype=float32)], val loss: 0.07180186361074448\n",
      "\t partial train loss (single batch): 0.071555\n",
      "Epoch 640/1000 train loss: [array(0.07155466, dtype=float32)], val loss: 0.07223978638648987\n",
      "\t partial train loss (single batch): 0.071327\n",
      "Epoch 641/1000 train loss: [array(0.07132749, dtype=float32)], val loss: 0.07158546149730682\n",
      "\t partial train loss (single batch): 0.072423\n",
      "Epoch 642/1000 train loss: [array(0.07242291, dtype=float32)], val loss: 0.07212872058153152\n",
      "\t partial train loss (single batch): 0.070419\n",
      "Epoch 643/1000 train loss: [array(0.07041892, dtype=float32)], val loss: 0.07200886309146881\n",
      "\t partial train loss (single batch): 0.070586\n",
      "Epoch 644/1000 train loss: [array(0.07058597, dtype=float32)], val loss: 0.07095092535018921\n",
      "\t partial train loss (single batch): 0.070730\n",
      "Epoch 645/1000 train loss: [array(0.07072973, dtype=float32)], val loss: 0.07205754518508911\n",
      "\t partial train loss (single batch): 0.070909\n",
      "Epoch 646/1000 train loss: [array(0.07090943, dtype=float32)], val loss: 0.07178694009780884\n",
      "\t partial train loss (single batch): 0.070178\n",
      "Epoch 647/1000 train loss: [array(0.07017783, dtype=float32)], val loss: 0.07133950293064117\n",
      "\t partial train loss (single batch): 0.071265\n",
      "Epoch 648/1000 train loss: [array(0.071265, dtype=float32)], val loss: 0.07158557325601578\n",
      "\t partial train loss (single batch): 0.071537\n",
      "Epoch 649/1000 train loss: [array(0.07153715, dtype=float32)], val loss: 0.06996840983629227\n",
      "\t partial train loss (single batch): 0.071073\n",
      "Epoch 650/1000 train loss: [array(0.07107314, dtype=float32)], val loss: 0.07122308015823364\n",
      "\t partial train loss (single batch): 0.071044\n",
      "Epoch 651/1000 train loss: [array(0.07104352, dtype=float32)], val loss: 0.07231191545724869\n",
      "\t partial train loss (single batch): 0.071056\n",
      "Epoch 652/1000 train loss: [array(0.07105584, dtype=float32)], val loss: 0.07231403887271881\n",
      "\t partial train loss (single batch): 0.071303\n",
      "Epoch 653/1000 train loss: [array(0.07130267, dtype=float32)], val loss: 0.07156062871217728\n",
      "\t partial train loss (single batch): 0.071174\n",
      "Epoch 654/1000 train loss: [array(0.07117419, dtype=float32)], val loss: 0.0708678811788559\n",
      "\t partial train loss (single batch): 0.070869\n",
      "Epoch 655/1000 train loss: [array(0.07086927, dtype=float32)], val loss: 0.07152485102415085\n",
      "\t partial train loss (single batch): 0.069785\n",
      "Epoch 656/1000 train loss: [array(0.06978549, dtype=float32)], val loss: 0.07152049988508224\n",
      "\t partial train loss (single batch): 0.070834\n",
      "Epoch 657/1000 train loss: [array(0.07083441, dtype=float32)], val loss: 0.07124146074056625\n",
      "\t partial train loss (single batch): 0.071243\n",
      "Epoch 658/1000 train loss: [array(0.07124288, dtype=float32)], val loss: 0.07129327952861786\n",
      "\t partial train loss (single batch): 0.070732\n",
      "Epoch 659/1000 train loss: [array(0.07073195, dtype=float32)], val loss: 0.07028090208768845\n",
      "\t partial train loss (single batch): 0.070113\n",
      "Epoch 660/1000 train loss: [array(0.07011321, dtype=float32)], val loss: 0.07160281389951706\n",
      "\t partial train loss (single batch): 0.070395\n",
      "Epoch 661/1000 train loss: [array(0.07039519, dtype=float32)], val loss: 0.07126260548830032\n",
      "\t partial train loss (single batch): 0.071035\n",
      "Epoch 662/1000 train loss: [array(0.0710347, dtype=float32)], val loss: 0.07093130797147751\n",
      "\t partial train loss (single batch): 0.070536\n",
      "Epoch 663/1000 train loss: [array(0.07053579, dtype=float32)], val loss: 0.07103599607944489\n",
      "\t partial train loss (single batch): 0.071904\n",
      "Epoch 664/1000 train loss: [array(0.07190412, dtype=float32)], val loss: 0.07140908390283585\n",
      "\t partial train loss (single batch): 0.070635\n",
      "Epoch 665/1000 train loss: [array(0.07063545, dtype=float32)], val loss: 0.07058577984571457\n",
      "\t partial train loss (single batch): 0.070797\n",
      "Epoch 666/1000 train loss: [array(0.07079741, dtype=float32)], val loss: 0.06879150122404099\n",
      "\t partial train loss (single batch): 0.071114\n",
      "Epoch 667/1000 train loss: [array(0.07111446, dtype=float32)], val loss: 0.07059174031019211\n",
      "\t partial train loss (single batch): 0.071154\n",
      "Epoch 668/1000 train loss: [array(0.07115352, dtype=float32)], val loss: 0.0710904598236084\n",
      "\t partial train loss (single batch): 0.070694\n",
      "Epoch 669/1000 train loss: [array(0.07069418, dtype=float32)], val loss: 0.07046252489089966\n",
      "\t partial train loss (single batch): 0.072098\n",
      "Epoch 670/1000 train loss: [array(0.0720984, dtype=float32)], val loss: 0.07079724222421646\n",
      "\t partial train loss (single batch): 0.070115\n",
      "Epoch 671/1000 train loss: [array(0.07011541, dtype=float32)], val loss: 0.07033099234104156\n",
      "\t partial train loss (single batch): 0.071380\n",
      "Epoch 672/1000 train loss: [array(0.07137977, dtype=float32)], val loss: 0.07015387713909149\n",
      "\t partial train loss (single batch): 0.069470\n",
      "Epoch 673/1000 train loss: [array(0.06947022, dtype=float32)], val loss: 0.07016773521900177\n",
      "\t partial train loss (single batch): 0.070537\n",
      "Epoch 674/1000 train loss: [array(0.07053723, dtype=float32)], val loss: 0.07069148123264313\n",
      "\t partial train loss (single batch): 0.070974\n",
      "Epoch 675/1000 train loss: [array(0.07097446, dtype=float32)], val loss: 0.06977010518312454\n",
      "\t partial train loss (single batch): 0.071207\n",
      "Epoch 676/1000 train loss: [array(0.07120685, dtype=float32)], val loss: 0.07080281525850296\n",
      "\t partial train loss (single batch): 0.070309\n",
      "Epoch 677/1000 train loss: [array(0.07030942, dtype=float32)], val loss: 0.07165900617837906\n",
      "\t partial train loss (single batch): 0.070901\n",
      "Epoch 678/1000 train loss: [array(0.07090146, dtype=float32)], val loss: 0.0703658014535904\n",
      "\t partial train loss (single batch): 0.070014\n",
      "Epoch 679/1000 train loss: [array(0.07001407, dtype=float32)], val loss: 0.07043173909187317\n",
      "\t partial train loss (single batch): 0.070699\n",
      "Epoch 680/1000 train loss: [array(0.07069869, dtype=float32)], val loss: 0.07052242010831833\n",
      "\t partial train loss (single batch): 0.069872\n",
      "Epoch 681/1000 train loss: [array(0.06987163, dtype=float32)], val loss: 0.07095879316329956\n",
      "\t partial train loss (single batch): 0.069902\n",
      "Epoch 682/1000 train loss: [array(0.0699024, dtype=float32)], val loss: 0.06983833014965057\n",
      "\t partial train loss (single batch): 0.070292\n",
      "Epoch 683/1000 train loss: [array(0.07029198, dtype=float32)], val loss: 0.07022395730018616\n",
      "\t partial train loss (single batch): 0.069998\n",
      "Epoch 684/1000 train loss: [array(0.06999812, dtype=float32)], val loss: 0.07079867273569107\n",
      "\t partial train loss (single batch): 0.070967\n",
      "Epoch 685/1000 train loss: [array(0.07096714, dtype=float32)], val loss: 0.06968235969543457\n",
      "\t partial train loss (single batch): 0.069354\n",
      "Epoch 686/1000 train loss: [array(0.06935358, dtype=float32)], val loss: 0.07011885195970535\n",
      "\t partial train loss (single batch): 0.071460\n",
      "Epoch 687/1000 train loss: [array(0.0714603, dtype=float32)], val loss: 0.07020513713359833\n",
      "\t partial train loss (single batch): 0.070836\n",
      "Epoch 688/1000 train loss: [array(0.07083569, dtype=float32)], val loss: 0.07008097320795059\n",
      "\t partial train loss (single batch): 0.069838\n",
      "Epoch 689/1000 train loss: [array(0.06983785, dtype=float32)], val loss: 0.06993943452835083\n",
      "\t partial train loss (single batch): 0.070465\n",
      "Epoch 690/1000 train loss: [array(0.07046506, dtype=float32)], val loss: 0.07080517709255219\n",
      "\t partial train loss (single batch): 0.069830\n",
      "Epoch 691/1000 train loss: [array(0.06982987, dtype=float32)], val loss: 0.07034016400575638\n",
      "\t partial train loss (single batch): 0.070145\n",
      "Epoch 692/1000 train loss: [array(0.07014547, dtype=float32)], val loss: 0.0709226131439209\n",
      "\t partial train loss (single batch): 0.069791\n",
      "Epoch 693/1000 train loss: [array(0.06979111, dtype=float32)], val loss: 0.06935995817184448\n",
      "\t partial train loss (single batch): 0.069830\n",
      "Epoch 694/1000 train loss: [array(0.06982956, dtype=float32)], val loss: 0.07008881866931915\n",
      "\t partial train loss (single batch): 0.070758\n",
      "Epoch 695/1000 train loss: [array(0.07075779, dtype=float32)], val loss: 0.07047457993030548\n",
      "\t partial train loss (single batch): 0.069569\n",
      "Epoch 696/1000 train loss: [array(0.06956933, dtype=float32)], val loss: 0.06962472200393677\n",
      "\t partial train loss (single batch): 0.069181\n",
      "Epoch 697/1000 train loss: [array(0.06918149, dtype=float32)], val loss: 0.07050690054893494\n",
      "\t partial train loss (single batch): 0.069769\n",
      "Epoch 698/1000 train loss: [array(0.06976873, dtype=float32)], val loss: 0.06945139169692993\n",
      "\t partial train loss (single batch): 0.070005\n",
      "Epoch 699/1000 train loss: [array(0.07000541, dtype=float32)], val loss: 0.07031451910734177\n",
      "\t partial train loss (single batch): 0.069686\n",
      "Epoch 700/1000 train loss: [array(0.06968638, dtype=float32)], val loss: 0.07022744417190552\n",
      "\t partial train loss (single batch): 0.069609\n",
      "Epoch 701/1000 train loss: [array(0.06960898, dtype=float32)], val loss: 0.07055248320102692\n",
      "\t partial train loss (single batch): 0.070461\n",
      "Epoch 702/1000 train loss: [array(0.07046146, dtype=float32)], val loss: 0.07040297240018845\n",
      "\t partial train loss (single batch): 0.068804\n",
      "Epoch 703/1000 train loss: [array(0.06880417, dtype=float32)], val loss: 0.07042942196130753\n",
      "\t partial train loss (single batch): 0.071307\n",
      "Epoch 704/1000 train loss: [array(0.07130682, dtype=float32)], val loss: 0.0697619691491127\n",
      "\t partial train loss (single batch): 0.069277\n",
      "Epoch 705/1000 train loss: [array(0.06927724, dtype=float32)], val loss: 0.06975743174552917\n",
      "\t partial train loss (single batch): 0.069966\n",
      "Epoch 706/1000 train loss: [array(0.06996574, dtype=float32)], val loss: 0.07013280689716339\n",
      "\t partial train loss (single batch): 0.068696\n",
      "Epoch 707/1000 train loss: [array(0.06869633, dtype=float32)], val loss: 0.06971023976802826\n",
      "\t partial train loss (single batch): 0.069436\n",
      "Epoch 708/1000 train loss: [array(0.06943625, dtype=float32)], val loss: 0.06927773356437683\n",
      "\t partial train loss (single batch): 0.068322\n",
      "Epoch 709/1000 train loss: [array(0.06832239, dtype=float32)], val loss: 0.06938958913087845\n",
      "\t partial train loss (single batch): 0.069409\n",
      "Epoch 710/1000 train loss: [array(0.06940864, dtype=float32)], val loss: 0.06959862262010574\n",
      "\t partial train loss (single batch): 0.069449\n",
      "Epoch 711/1000 train loss: [array(0.06944934, dtype=float32)], val loss: 0.06957037001848221\n",
      "\t partial train loss (single batch): 0.069719\n",
      "Epoch 712/1000 train loss: [array(0.06971893, dtype=float32)], val loss: 0.07015739381313324\n",
      "\t partial train loss (single batch): 0.069762\n",
      "Epoch 713/1000 train loss: [array(0.06976173, dtype=float32)], val loss: 0.07181751728057861\n",
      "\t partial train loss (single batch): 0.070224\n",
      "Epoch 714/1000 train loss: [array(0.07022424, dtype=float32)], val loss: 0.07008280605077744\n",
      "\t partial train loss (single batch): 0.069025\n",
      "Epoch 715/1000 train loss: [array(0.069025, dtype=float32)], val loss: 0.07044057548046112\n",
      "\t partial train loss (single batch): 0.070288\n",
      "Epoch 716/1000 train loss: [array(0.07028778, dtype=float32)], val loss: 0.06896565109491348\n",
      "\t partial train loss (single batch): 0.068993\n",
      "Epoch 717/1000 train loss: [array(0.06899263, dtype=float32)], val loss: 0.06917499005794525\n",
      "\t partial train loss (single batch): 0.069151\n",
      "Epoch 718/1000 train loss: [array(0.06915099, dtype=float32)], val loss: 0.06978607922792435\n",
      "\t partial train loss (single batch): 0.069832\n",
      "Epoch 719/1000 train loss: [array(0.06983158, dtype=float32)], val loss: 0.070565365254879\n",
      "\t partial train loss (single batch): 0.070554\n",
      "Epoch 720/1000 train loss: [array(0.07055438, dtype=float32)], val loss: 0.06907019019126892\n",
      "\t partial train loss (single batch): 0.070235\n",
      "Epoch 721/1000 train loss: [array(0.07023477, dtype=float32)], val loss: 0.07085324078798294\n",
      "\t partial train loss (single batch): 0.069434\n",
      "Epoch 722/1000 train loss: [array(0.06943402, dtype=float32)], val loss: 0.07000674307346344\n",
      "\t partial train loss (single batch): 0.068974\n",
      "Epoch 723/1000 train loss: [array(0.06897432, dtype=float32)], val loss: 0.06824278831481934\n",
      "\t partial train loss (single batch): 0.068739\n",
      "Epoch 724/1000 train loss: [array(0.06873918, dtype=float32)], val loss: 0.06841059774160385\n",
      "\t partial train loss (single batch): 0.070017\n",
      "Epoch 725/1000 train loss: [array(0.07001742, dtype=float32)], val loss: 0.06993017345666885\n",
      "\t partial train loss (single batch): 0.069182\n",
      "Epoch 726/1000 train loss: [array(0.06918166, dtype=float32)], val loss: 0.06976108998060226\n",
      "\t partial train loss (single batch): 0.068598\n",
      "Epoch 727/1000 train loss: [array(0.06859811, dtype=float32)], val loss: 0.06873813271522522\n",
      "\t partial train loss (single batch): 0.068672\n",
      "Epoch 728/1000 train loss: [array(0.06867211, dtype=float32)], val loss: 0.06907184422016144\n",
      "\t partial train loss (single batch): 0.068540\n",
      "Epoch 729/1000 train loss: [array(0.06854043, dtype=float32)], val loss: 0.0682033896446228\n",
      "\t partial train loss (single batch): 0.069712\n",
      "Epoch 730/1000 train loss: [array(0.06971202, dtype=float32)], val loss: 0.06952807307243347\n",
      "\t partial train loss (single batch): 0.070851\n",
      "Epoch 731/1000 train loss: [array(0.07085085, dtype=float32)], val loss: 0.07043655216693878\n",
      "\t partial train loss (single batch): 0.068725\n",
      "Epoch 732/1000 train loss: [array(0.06872471, dtype=float32)], val loss: 0.07023768872022629\n",
      "\t partial train loss (single batch): 0.069680\n",
      "Epoch 733/1000 train loss: [array(0.06968022, dtype=float32)], val loss: 0.06919669359922409\n",
      "\t partial train loss (single batch): 0.068159\n",
      "Epoch 734/1000 train loss: [array(0.06815854, dtype=float32)], val loss: 0.06947309523820877\n",
      "\t partial train loss (single batch): 0.069135\n",
      "Epoch 735/1000 train loss: [array(0.06913514, dtype=float32)], val loss: 0.06981711834669113\n",
      "\t partial train loss (single batch): 0.069884\n",
      "Epoch 736/1000 train loss: [array(0.06988411, dtype=float32)], val loss: 0.06855551898479462\n",
      "\t partial train loss (single batch): 0.069853\n",
      "Epoch 737/1000 train loss: [array(0.0698533, dtype=float32)], val loss: 0.06946490705013275\n",
      "\t partial train loss (single batch): 0.068996\n",
      "Epoch 738/1000 train loss: [array(0.06899586, dtype=float32)], val loss: 0.06780961900949478\n",
      "\t partial train loss (single batch): 0.069167\n",
      "Epoch 739/1000 train loss: [array(0.06916742, dtype=float32)], val loss: 0.06971406936645508\n",
      "\t partial train loss (single batch): 0.068937\n",
      "Epoch 740/1000 train loss: [array(0.06893712, dtype=float32)], val loss: 0.06898623704910278\n",
      "\t partial train loss (single batch): 0.069134\n",
      "Epoch 741/1000 train loss: [array(0.06913368, dtype=float32)], val loss: 0.069244384765625\n",
      "\t partial train loss (single batch): 0.069277\n",
      "Epoch 742/1000 train loss: [array(0.06927721, dtype=float32)], val loss: 0.06999985128641129\n",
      "\t partial train loss (single batch): 0.069649\n",
      "Epoch 743/1000 train loss: [array(0.06964947, dtype=float32)], val loss: 0.07022726535797119\n",
      "\t partial train loss (single batch): 0.069054\n",
      "Epoch 744/1000 train loss: [array(0.06905437, dtype=float32)], val loss: 0.06882932037115097\n",
      "\t partial train loss (single batch): 0.069398\n",
      "Epoch 745/1000 train loss: [array(0.06939775, dtype=float32)], val loss: 0.07092294096946716\n",
      "\t partial train loss (single batch): 0.068896\n",
      "Epoch 746/1000 train loss: [array(0.06889569, dtype=float32)], val loss: 0.0692237839102745\n",
      "\t partial train loss (single batch): 0.068650\n",
      "Epoch 747/1000 train loss: [array(0.06864961, dtype=float32)], val loss: 0.06819858402013779\n",
      "\t partial train loss (single batch): 0.069685\n",
      "Epoch 748/1000 train loss: [array(0.06968486, dtype=float32)], val loss: 0.06994608044624329\n",
      "\t partial train loss (single batch): 0.069012\n",
      "Epoch 749/1000 train loss: [array(0.06901223, dtype=float32)], val loss: 0.06923267990350723\n",
      "\t partial train loss (single batch): 0.068583\n",
      "Epoch 750/1000 train loss: [array(0.06858253, dtype=float32)], val loss: 0.0687021017074585\n",
      "\t partial train loss (single batch): 0.068027\n",
      "Epoch 751/1000 train loss: [array(0.06802739, dtype=float32)], val loss: 0.06926444172859192\n",
      "\t partial train loss (single batch): 0.068229\n",
      "Epoch 752/1000 train loss: [array(0.06822861, dtype=float32)], val loss: 0.06992630660533905\n",
      "\t partial train loss (single batch): 0.068398\n",
      "Epoch 753/1000 train loss: [array(0.06839751, dtype=float32)], val loss: 0.06960724294185638\n",
      "\t partial train loss (single batch): 0.067737\n",
      "Epoch 754/1000 train loss: [array(0.06773662, dtype=float32)], val loss: 0.06957880407571793\n",
      "\t partial train loss (single batch): 0.068406\n",
      "Epoch 755/1000 train loss: [array(0.06840567, dtype=float32)], val loss: 0.06872687488794327\n",
      "\t partial train loss (single batch): 0.068489\n",
      "Epoch 756/1000 train loss: [array(0.06848901, dtype=float32)], val loss: 0.06812093406915665\n",
      "\t partial train loss (single batch): 0.069807\n",
      "Epoch 757/1000 train loss: [array(0.06980682, dtype=float32)], val loss: 0.06854087859392166\n",
      "\t partial train loss (single batch): 0.069040\n",
      "Epoch 758/1000 train loss: [array(0.06903987, dtype=float32)], val loss: 0.06923846155405045\n",
      "\t partial train loss (single batch): 0.067745\n",
      "Epoch 759/1000 train loss: [array(0.06774453, dtype=float32)], val loss: 0.06894191354513168\n",
      "\t partial train loss (single batch): 0.069709\n",
      "Epoch 760/1000 train loss: [array(0.06970938, dtype=float32)], val loss: 0.06865257769823074\n",
      "\t partial train loss (single batch): 0.067678\n",
      "Epoch 761/1000 train loss: [array(0.06767812, dtype=float32)], val loss: 0.06886372715234756\n",
      "\t partial train loss (single batch): 0.068348\n",
      "Epoch 762/1000 train loss: [array(0.06834807, dtype=float32)], val loss: 0.06849648058414459\n",
      "\t partial train loss (single batch): 0.069317\n",
      "Epoch 763/1000 train loss: [array(0.06931657, dtype=float32)], val loss: 0.06863421201705933\n",
      "\t partial train loss (single batch): 0.068800\n",
      "Epoch 764/1000 train loss: [array(0.0688001, dtype=float32)], val loss: 0.06851287186145782\n",
      "\t partial train loss (single batch): 0.067971\n",
      "Epoch 765/1000 train loss: [array(0.06797121, dtype=float32)], val loss: 0.06834466010332108\n",
      "\t partial train loss (single batch): 0.068429\n",
      "Epoch 766/1000 train loss: [array(0.06842876, dtype=float32)], val loss: 0.06889937818050385\n",
      "\t partial train loss (single batch): 0.068839\n",
      "Epoch 767/1000 train loss: [array(0.06883919, dtype=float32)], val loss: 0.06845469027757645\n",
      "\t partial train loss (single batch): 0.068190\n",
      "Epoch 768/1000 train loss: [array(0.0681904, dtype=float32)], val loss: 0.06790096312761307\n",
      "\t partial train loss (single batch): 0.068312\n",
      "Epoch 769/1000 train loss: [array(0.06831183, dtype=float32)], val loss: 0.06862617284059525\n",
      "\t partial train loss (single batch): 0.068631\n",
      "Epoch 770/1000 train loss: [array(0.06863149, dtype=float32)], val loss: 0.06889568269252777\n",
      "\t partial train loss (single batch): 0.068879\n",
      "Epoch 771/1000 train loss: [array(0.06887899, dtype=float32)], val loss: 0.06913767755031586\n",
      "\t partial train loss (single batch): 0.069015\n",
      "Epoch 772/1000 train loss: [array(0.06901544, dtype=float32)], val loss: 0.06873122602701187\n",
      "\t partial train loss (single batch): 0.068883\n",
      "Epoch 773/1000 train loss: [array(0.06888311, dtype=float32)], val loss: 0.0682372972369194\n",
      "\t partial train loss (single batch): 0.067670\n",
      "Epoch 774/1000 train loss: [array(0.0676699, dtype=float32)], val loss: 0.06902258843183517\n",
      "\t partial train loss (single batch): 0.068340\n",
      "Epoch 775/1000 train loss: [array(0.06833988, dtype=float32)], val loss: 0.06899929791688919\n",
      "\t partial train loss (single batch): 0.067962\n",
      "Epoch 776/1000 train loss: [array(0.06796198, dtype=float32)], val loss: 0.06947171688079834\n",
      "\t partial train loss (single batch): 0.068945\n",
      "Epoch 777/1000 train loss: [array(0.06894477, dtype=float32)], val loss: 0.06724641472101212\n",
      "\t partial train loss (single batch): 0.068208\n",
      "Epoch 778/1000 train loss: [array(0.06820755, dtype=float32)], val loss: 0.06945060193538666\n",
      "\t partial train loss (single batch): 0.067751\n",
      "Epoch 779/1000 train loss: [array(0.06775073, dtype=float32)], val loss: 0.06956911087036133\n",
      "\t partial train loss (single batch): 0.067579\n",
      "Epoch 780/1000 train loss: [array(0.06757917, dtype=float32)], val loss: 0.06794395297765732\n",
      "\t partial train loss (single batch): 0.069045\n",
      "Epoch 781/1000 train loss: [array(0.06904472, dtype=float32)], val loss: 0.06932801008224487\n",
      "\t partial train loss (single batch): 0.067797\n",
      "Epoch 782/1000 train loss: [array(0.06779749, dtype=float32)], val loss: 0.068473681807518\n",
      "\t partial train loss (single batch): 0.068601\n",
      "Epoch 783/1000 train loss: [array(0.06860101, dtype=float32)], val loss: 0.0678214430809021\n",
      "\t partial train loss (single batch): 0.068461\n",
      "Epoch 784/1000 train loss: [array(0.06846069, dtype=float32)], val loss: 0.06791562587022781\n",
      "\t partial train loss (single batch): 0.067777\n",
      "Epoch 785/1000 train loss: [array(0.06777697, dtype=float32)], val loss: 0.06836068630218506\n",
      "\t partial train loss (single batch): 0.068141\n",
      "Epoch 786/1000 train loss: [array(0.06814135, dtype=float32)], val loss: 0.06876756995916367\n",
      "\t partial train loss (single batch): 0.068677\n",
      "Epoch 787/1000 train loss: [array(0.0686767, dtype=float32)], val loss: 0.0692901760339737\n",
      "\t partial train loss (single batch): 0.068076\n",
      "Epoch 788/1000 train loss: [array(0.06807626, dtype=float32)], val loss: 0.06831889599561691\n",
      "\t partial train loss (single batch): 0.067461\n",
      "Epoch 789/1000 train loss: [array(0.06746085, dtype=float32)], val loss: 0.06836532056331635\n",
      "\t partial train loss (single batch): 0.068014\n",
      "Epoch 790/1000 train loss: [array(0.06801361, dtype=float32)], val loss: 0.06891649216413498\n",
      "\t partial train loss (single batch): 0.068957\n",
      "Epoch 791/1000 train loss: [array(0.06895746, dtype=float32)], val loss: 0.06791263073682785\n",
      "\t partial train loss (single batch): 0.068418\n",
      "Epoch 792/1000 train loss: [array(0.06841794, dtype=float32)], val loss: 0.06857484579086304\n",
      "\t partial train loss (single batch): 0.067482\n",
      "Epoch 793/1000 train loss: [array(0.06748151, dtype=float32)], val loss: 0.06857607513666153\n",
      "\t partial train loss (single batch): 0.068180\n",
      "Epoch 794/1000 train loss: [array(0.06818025, dtype=float32)], val loss: 0.06694736331701279\n",
      "\t partial train loss (single batch): 0.067907\n",
      "Epoch 795/1000 train loss: [array(0.06790739, dtype=float32)], val loss: 0.06761602312326431\n",
      "\t partial train loss (single batch): 0.068668\n",
      "Epoch 796/1000 train loss: [array(0.06866834, dtype=float32)], val loss: 0.06734981387853622\n",
      "\t partial train loss (single batch): 0.067962\n",
      "Epoch 797/1000 train loss: [array(0.06796192, dtype=float32)], val loss: 0.06808754056692123\n",
      "\t partial train loss (single batch): 0.066505\n",
      "Epoch 798/1000 train loss: [array(0.06650493, dtype=float32)], val loss: 0.06865254789590836\n",
      "\t partial train loss (single batch): 0.069308\n",
      "Epoch 799/1000 train loss: [array(0.06930804, dtype=float32)], val loss: 0.06825045496225357\n",
      "\t partial train loss (single batch): 0.067525\n",
      "Epoch 800/1000 train loss: [array(0.06752489, dtype=float32)], val loss: 0.068120077252388\n",
      "\t partial train loss (single batch): 0.067583\n",
      "Epoch 801/1000 train loss: [array(0.06758314, dtype=float32)], val loss: 0.06769981980323792\n",
      "\t partial train loss (single batch): 0.066385\n",
      "Epoch 802/1000 train loss: [array(0.06638499, dtype=float32)], val loss: 0.06871640682220459\n",
      "\t partial train loss (single batch): 0.068169\n",
      "Epoch 803/1000 train loss: [array(0.06816854, dtype=float32)], val loss: 0.0676954984664917\n",
      "\t partial train loss (single batch): 0.068073\n",
      "Epoch 804/1000 train loss: [array(0.06807338, dtype=float32)], val loss: 0.06857816129922867\n",
      "\t partial train loss (single batch): 0.067713\n",
      "Epoch 805/1000 train loss: [array(0.06771336, dtype=float32)], val loss: 0.06676562130451202\n",
      "\t partial train loss (single batch): 0.067291\n",
      "Epoch 806/1000 train loss: [array(0.06729071, dtype=float32)], val loss: 0.06723777949810028\n",
      "\t partial train loss (single batch): 0.068171\n",
      "Epoch 807/1000 train loss: [array(0.06817109, dtype=float32)], val loss: 0.06737132370471954\n",
      "\t partial train loss (single batch): 0.067750\n",
      "Epoch 808/1000 train loss: [array(0.06774974, dtype=float32)], val loss: 0.06925635784864426\n",
      "\t partial train loss (single batch): 0.068068\n",
      "Epoch 809/1000 train loss: [array(0.06806793, dtype=float32)], val loss: 0.06720989942550659\n",
      "\t partial train loss (single batch): 0.067302\n",
      "Epoch 810/1000 train loss: [array(0.06730232, dtype=float32)], val loss: 0.06858447939157486\n",
      "\t partial train loss (single batch): 0.067619\n",
      "Epoch 811/1000 train loss: [array(0.06761894, dtype=float32)], val loss: 0.0671447366476059\n",
      "\t partial train loss (single batch): 0.067670\n",
      "Epoch 812/1000 train loss: [array(0.06766991, dtype=float32)], val loss: 0.06836682558059692\n",
      "\t partial train loss (single batch): 0.066426\n",
      "Epoch 813/1000 train loss: [array(0.0664262, dtype=float32)], val loss: 0.06797374039888382\n",
      "\t partial train loss (single batch): 0.068775\n",
      "Epoch 814/1000 train loss: [array(0.06877545, dtype=float32)], val loss: 0.06786512583494186\n",
      "\t partial train loss (single batch): 0.067588\n",
      "Epoch 815/1000 train loss: [array(0.06758822, dtype=float32)], val loss: 0.06912685930728912\n",
      "\t partial train loss (single batch): 0.067324\n",
      "Epoch 816/1000 train loss: [array(0.06732396, dtype=float32)], val loss: 0.06792022287845612\n",
      "\t partial train loss (single batch): 0.068384\n",
      "Epoch 817/1000 train loss: [array(0.06838405, dtype=float32)], val loss: 0.06765855103731155\n",
      "\t partial train loss (single batch): 0.068029\n",
      "Epoch 818/1000 train loss: [array(0.06802869, dtype=float32)], val loss: 0.06738006323575974\n",
      "\t partial train loss (single batch): 0.067802\n",
      "Epoch 819/1000 train loss: [array(0.06780218, dtype=float32)], val loss: 0.06879241019487381\n",
      "\t partial train loss (single batch): 0.066956\n",
      "Epoch 820/1000 train loss: [array(0.06695634, dtype=float32)], val loss: 0.06875339895486832\n",
      "\t partial train loss (single batch): 0.067943\n",
      "Epoch 821/1000 train loss: [array(0.06794275, dtype=float32)], val loss: 0.06826835125684738\n",
      "\t partial train loss (single batch): 0.068061\n",
      "Epoch 822/1000 train loss: [array(0.0680607, dtype=float32)], val loss: 0.0662446841597557\n",
      "\t partial train loss (single batch): 0.067368\n",
      "Epoch 823/1000 train loss: [array(0.06736811, dtype=float32)], val loss: 0.06868746131658554\n",
      "\t partial train loss (single batch): 0.068910\n",
      "Epoch 824/1000 train loss: [array(0.06890965, dtype=float32)], val loss: 0.06755191832780838\n",
      "\t partial train loss (single batch): 0.066698\n",
      "Epoch 825/1000 train loss: [array(0.06669771, dtype=float32)], val loss: 0.06789693981409073\n",
      "\t partial train loss (single batch): 0.067279\n",
      "Epoch 826/1000 train loss: [array(0.06727921, dtype=float32)], val loss: 0.06817086040973663\n",
      "\t partial train loss (single batch): 0.068484\n",
      "Epoch 827/1000 train loss: [array(0.06848431, dtype=float32)], val loss: 0.06809233874082565\n",
      "\t partial train loss (single batch): 0.068183\n",
      "Epoch 828/1000 train loss: [array(0.06818252, dtype=float32)], val loss: 0.06937029957771301\n",
      "\t partial train loss (single batch): 0.067510\n",
      "Epoch 829/1000 train loss: [array(0.0675096, dtype=float32)], val loss: 0.0693667009472847\n",
      "\t partial train loss (single batch): 0.067658\n",
      "Epoch 830/1000 train loss: [array(0.06765789, dtype=float32)], val loss: 0.06800828874111176\n",
      "\t partial train loss (single batch): 0.068187\n",
      "Epoch 831/1000 train loss: [array(0.06818666, dtype=float32)], val loss: 0.06684932857751846\n",
      "\t partial train loss (single batch): 0.067327\n",
      "Epoch 832/1000 train loss: [array(0.06732716, dtype=float32)], val loss: 0.06713948398828506\n",
      "\t partial train loss (single batch): 0.066742\n",
      "Epoch 833/1000 train loss: [array(0.06674212, dtype=float32)], val loss: 0.06798574328422546\n",
      "\t partial train loss (single batch): 0.067298\n",
      "Epoch 834/1000 train loss: [array(0.06729832, dtype=float32)], val loss: 0.06722754240036011\n",
      "\t partial train loss (single batch): 0.066265\n",
      "Epoch 835/1000 train loss: [array(0.06626498, dtype=float32)], val loss: 0.06736605614423752\n",
      "\t partial train loss (single batch): 0.066841\n",
      "Epoch 836/1000 train loss: [array(0.06684077, dtype=float32)], val loss: 0.06726992130279541\n",
      "\t partial train loss (single batch): 0.067546\n",
      "Epoch 837/1000 train loss: [array(0.067546, dtype=float32)], val loss: 0.06656476855278015\n",
      "\t partial train loss (single batch): 0.066782\n",
      "Epoch 838/1000 train loss: [array(0.06678222, dtype=float32)], val loss: 0.06784097105264664\n",
      "\t partial train loss (single batch): 0.068056\n",
      "Epoch 839/1000 train loss: [array(0.06805601, dtype=float32)], val loss: 0.06821674853563309\n",
      "\t partial train loss (single batch): 0.066832\n",
      "Epoch 840/1000 train loss: [array(0.06683225, dtype=float32)], val loss: 0.06657971441745758\n",
      "\t partial train loss (single batch): 0.068129\n",
      "Epoch 841/1000 train loss: [array(0.06812859, dtype=float32)], val loss: 0.06710468977689743\n",
      "\t partial train loss (single batch): 0.068040\n",
      "Epoch 842/1000 train loss: [array(0.06803957, dtype=float32)], val loss: 0.06789790093898773\n",
      "\t partial train loss (single batch): 0.067997\n",
      "Epoch 843/1000 train loss: [array(0.0679969, dtype=float32)], val loss: 0.06736110150814056\n",
      "\t partial train loss (single batch): 0.067484\n",
      "Epoch 844/1000 train loss: [array(0.06748439, dtype=float32)], val loss: 0.0673251524567604\n",
      "\t partial train loss (single batch): 0.067116\n",
      "Epoch 845/1000 train loss: [array(0.06711631, dtype=float32)], val loss: 0.06723827868700027\n",
      "\t partial train loss (single batch): 0.067013\n",
      "Epoch 846/1000 train loss: [array(0.06701266, dtype=float32)], val loss: 0.06736742705106735\n",
      "\t partial train loss (single batch): 0.066667\n",
      "Epoch 847/1000 train loss: [array(0.06666712, dtype=float32)], val loss: 0.06757669150829315\n",
      "\t partial train loss (single batch): 0.067999\n",
      "Epoch 848/1000 train loss: [array(0.06799911, dtype=float32)], val loss: 0.06835909187793732\n",
      "\t partial train loss (single batch): 0.066779\n",
      "Epoch 849/1000 train loss: [array(0.06677899, dtype=float32)], val loss: 0.06693726032972336\n",
      "\t partial train loss (single batch): 0.067398\n",
      "Epoch 850/1000 train loss: [array(0.06739829, dtype=float32)], val loss: 0.06699629873037338\n",
      "\t partial train loss (single batch): 0.066127\n",
      "Epoch 851/1000 train loss: [array(0.06612701, dtype=float32)], val loss: 0.06815103441476822\n",
      "\t partial train loss (single batch): 0.067631\n",
      "Epoch 852/1000 train loss: [array(0.06763098, dtype=float32)], val loss: 0.06815631687641144\n",
      "\t partial train loss (single batch): 0.067419\n",
      "Epoch 853/1000 train loss: [array(0.06741922, dtype=float32)], val loss: 0.06736835092306137\n",
      "\t partial train loss (single batch): 0.067370\n",
      "Epoch 854/1000 train loss: [array(0.06737019, dtype=float32)], val loss: 0.0672231912612915\n",
      "\t partial train loss (single batch): 0.067053\n",
      "Epoch 855/1000 train loss: [array(0.06705322, dtype=float32)], val loss: 0.0677708238363266\n",
      "\t partial train loss (single batch): 0.066060\n",
      "Epoch 856/1000 train loss: [array(0.06605954, dtype=float32)], val loss: 0.06775976717472076\n",
      "\t partial train loss (single batch): 0.066237\n",
      "Epoch 857/1000 train loss: [array(0.06623682, dtype=float32)], val loss: 0.06653131544589996\n",
      "\t partial train loss (single batch): 0.067200\n",
      "Epoch 858/1000 train loss: [array(0.06719968, dtype=float32)], val loss: 0.06668037921190262\n",
      "\t partial train loss (single batch): 0.066896\n",
      "Epoch 859/1000 train loss: [array(0.06689581, dtype=float32)], val loss: 0.06633464246988297\n",
      "\t partial train loss (single batch): 0.066264\n",
      "Epoch 860/1000 train loss: [array(0.06626388, dtype=float32)], val loss: 0.0667518898844719\n",
      "\t partial train loss (single batch): 0.067365\n",
      "Epoch 861/1000 train loss: [array(0.06736537, dtype=float32)], val loss: 0.0672469213604927\n",
      "\t partial train loss (single batch): 0.065733\n",
      "Epoch 862/1000 train loss: [array(0.06573316, dtype=float32)], val loss: 0.06684268265962601\n",
      "\t partial train loss (single batch): 0.067640\n",
      "Epoch 863/1000 train loss: [array(0.06763987, dtype=float32)], val loss: 0.06698445230722427\n",
      "\t partial train loss (single batch): 0.066537\n",
      "Epoch 864/1000 train loss: [array(0.06653675, dtype=float32)], val loss: 0.06708974391222\n",
      "\t partial train loss (single batch): 0.067923\n",
      "Epoch 865/1000 train loss: [array(0.06792265, dtype=float32)], val loss: 0.06740083545446396\n",
      "\t partial train loss (single batch): 0.067147\n",
      "Epoch 866/1000 train loss: [array(0.06714702, dtype=float32)], val loss: 0.06747014075517654\n",
      "\t partial train loss (single batch): 0.066165\n",
      "Epoch 867/1000 train loss: [array(0.06616538, dtype=float32)], val loss: 0.0660610944032669\n",
      "\t partial train loss (single batch): 0.066641\n",
      "Epoch 868/1000 train loss: [array(0.06664134, dtype=float32)], val loss: 0.0671953409910202\n",
      "\t partial train loss (single batch): 0.066569\n",
      "Epoch 869/1000 train loss: [array(0.06656942, dtype=float32)], val loss: 0.06727080047130585\n",
      "\t partial train loss (single batch): 0.066108\n",
      "Epoch 870/1000 train loss: [array(0.06610787, dtype=float32)], val loss: 0.06703194230794907\n",
      "\t partial train loss (single batch): 0.067215\n",
      "Epoch 871/1000 train loss: [array(0.06721535, dtype=float32)], val loss: 0.06640004366636276\n",
      "\t partial train loss (single batch): 0.066337\n",
      "Epoch 872/1000 train loss: [array(0.06633723, dtype=float32)], val loss: 0.0669674277305603\n",
      "\t partial train loss (single batch): 0.065953\n",
      "Epoch 873/1000 train loss: [array(0.06595313, dtype=float32)], val loss: 0.06622492522001266\n",
      "\t partial train loss (single batch): 0.067095\n",
      "Epoch 874/1000 train loss: [array(0.06709475, dtype=float32)], val loss: 0.06619372963905334\n",
      "\t partial train loss (single batch): 0.066541\n",
      "Epoch 875/1000 train loss: [array(0.06654082, dtype=float32)], val loss: 0.06816178560256958\n",
      "\t partial train loss (single batch): 0.066608\n",
      "Epoch 876/1000 train loss: [array(0.06660841, dtype=float32)], val loss: 0.06620673835277557\n",
      "\t partial train loss (single batch): 0.066744\n",
      "Epoch 877/1000 train loss: [array(0.06674398, dtype=float32)], val loss: 0.06676936894655228\n",
      "\t partial train loss (single batch): 0.066626\n",
      "Epoch 878/1000 train loss: [array(0.06662568, dtype=float32)], val loss: 0.06720511615276337\n",
      "\t partial train loss (single batch): 0.066639\n",
      "Epoch 879/1000 train loss: [array(0.0666391, dtype=float32)], val loss: 0.06619281321763992\n",
      "\t partial train loss (single batch): 0.066336\n",
      "Epoch 880/1000 train loss: [array(0.06633608, dtype=float32)], val loss: 0.06730541586875916\n",
      "\t partial train loss (single batch): 0.067466\n",
      "Epoch 881/1000 train loss: [array(0.06746596, dtype=float32)], val loss: 0.06695040315389633\n",
      "\t partial train loss (single batch): 0.066739\n",
      "Epoch 882/1000 train loss: [array(0.06673899, dtype=float32)], val loss: 0.065981924533844\n",
      "\t partial train loss (single batch): 0.064802\n",
      "Epoch 883/1000 train loss: [array(0.06480223, dtype=float32)], val loss: 0.06728457659482956\n",
      "\t partial train loss (single batch): 0.066835\n",
      "Epoch 884/1000 train loss: [array(0.06683455, dtype=float32)], val loss: 0.06598605960607529\n",
      "\t partial train loss (single batch): 0.066708\n",
      "Epoch 885/1000 train loss: [array(0.06670769, dtype=float32)], val loss: 0.06848642975091934\n",
      "\t partial train loss (single batch): 0.067245\n",
      "Epoch 886/1000 train loss: [array(0.06724457, dtype=float32)], val loss: 0.06741727143526077\n",
      "\t partial train loss (single batch): 0.065157\n",
      "Epoch 887/1000 train loss: [array(0.06515699, dtype=float32)], val loss: 0.0667424276471138\n",
      "\t partial train loss (single batch): 0.066435\n",
      "Epoch 888/1000 train loss: [array(0.06643514, dtype=float32)], val loss: 0.06630522757768631\n",
      "\t partial train loss (single batch): 0.066113\n",
      "Epoch 889/1000 train loss: [array(0.06611267, dtype=float32)], val loss: 0.06672688573598862\n",
      "\t partial train loss (single batch): 0.066587\n",
      "Epoch 890/1000 train loss: [array(0.06658666, dtype=float32)], val loss: 0.06720012426376343\n",
      "\t partial train loss (single batch): 0.066111\n",
      "Epoch 891/1000 train loss: [array(0.06611077, dtype=float32)], val loss: 0.06680161505937576\n",
      "\t partial train loss (single batch): 0.066461\n",
      "Epoch 892/1000 train loss: [array(0.06646144, dtype=float32)], val loss: 0.06665989756584167\n",
      "\t partial train loss (single batch): 0.066969\n",
      "Epoch 893/1000 train loss: [array(0.0669691, dtype=float32)], val loss: 0.0662083700299263\n",
      "\t partial train loss (single batch): 0.066652\n",
      "Epoch 894/1000 train loss: [array(0.06665209, dtype=float32)], val loss: 0.06678315997123718\n",
      "\t partial train loss (single batch): 0.065467\n",
      "Epoch 895/1000 train loss: [array(0.06546681, dtype=float32)], val loss: 0.06774412095546722\n",
      "\t partial train loss (single batch): 0.066022\n",
      "Epoch 896/1000 train loss: [array(0.06602181, dtype=float32)], val loss: 0.06647082418203354\n",
      "\t partial train loss (single batch): 0.066074\n",
      "Epoch 897/1000 train loss: [array(0.0660745, dtype=float32)], val loss: 0.06678210198879242\n",
      "\t partial train loss (single batch): 0.066025\n",
      "Epoch 898/1000 train loss: [array(0.06602485, dtype=float32)], val loss: 0.06696367263793945\n",
      "\t partial train loss (single batch): 0.066847\n",
      "Epoch 899/1000 train loss: [array(0.06684709, dtype=float32)], val loss: 0.06702429801225662\n",
      "\t partial train loss (single batch): 0.066775\n",
      "Epoch 900/1000 train loss: [array(0.06677459, dtype=float32)], val loss: 0.06709934026002884\n",
      "\t partial train loss (single batch): 0.066079\n",
      "Epoch 901/1000 train loss: [array(0.06607942, dtype=float32)], val loss: 0.06587456166744232\n",
      "\t partial train loss (single batch): 0.065800\n",
      "Epoch 902/1000 train loss: [array(0.06580033, dtype=float32)], val loss: 0.06681591272354126\n",
      "\t partial train loss (single batch): 0.065746\n",
      "Epoch 903/1000 train loss: [array(0.06574601, dtype=float32)], val loss: 0.06593158096075058\n",
      "\t partial train loss (single batch): 0.065916\n",
      "Epoch 904/1000 train loss: [array(0.06591615, dtype=float32)], val loss: 0.06602606177330017\n",
      "\t partial train loss (single batch): 0.066109\n",
      "Epoch 905/1000 train loss: [array(0.06610911, dtype=float32)], val loss: 0.06620725244283676\n",
      "\t partial train loss (single batch): 0.065518\n",
      "Epoch 906/1000 train loss: [array(0.06551836, dtype=float32)], val loss: 0.06661538034677505\n",
      "\t partial train loss (single batch): 0.065435\n",
      "Epoch 907/1000 train loss: [array(0.06543547, dtype=float32)], val loss: 0.06548366695642471\n",
      "\t partial train loss (single batch): 0.065599\n",
      "Epoch 908/1000 train loss: [array(0.06559915, dtype=float32)], val loss: 0.06754444539546967\n",
      "\t partial train loss (single batch): 0.066498\n",
      "Epoch 909/1000 train loss: [array(0.06649793, dtype=float32)], val loss: 0.06731875240802765\n",
      "\t partial train loss (single batch): 0.065949\n",
      "Epoch 910/1000 train loss: [array(0.06594881, dtype=float32)], val loss: 0.06645498424768448\n",
      "\t partial train loss (single batch): 0.065326\n",
      "Epoch 911/1000 train loss: [array(0.06532607, dtype=float32)], val loss: 0.0657658725976944\n",
      "\t partial train loss (single batch): 0.066638\n",
      "Epoch 912/1000 train loss: [array(0.06663765, dtype=float32)], val loss: 0.06562545895576477\n",
      "\t partial train loss (single batch): 0.065764\n",
      "Epoch 913/1000 train loss: [array(0.06576436, dtype=float32)], val loss: 0.06686905026435852\n",
      "\t partial train loss (single batch): 0.066298\n",
      "Epoch 914/1000 train loss: [array(0.06629842, dtype=float32)], val loss: 0.06511671096086502\n",
      "\t partial train loss (single batch): 0.065546\n",
      "Epoch 915/1000 train loss: [array(0.06554601, dtype=float32)], val loss: 0.06542274355888367\n",
      "\t partial train loss (single batch): 0.065509\n",
      "Epoch 916/1000 train loss: [array(0.06550911, dtype=float32)], val loss: 0.06682990491390228\n",
      "\t partial train loss (single batch): 0.065756\n",
      "Epoch 917/1000 train loss: [array(0.06575647, dtype=float32)], val loss: 0.06619228422641754\n",
      "\t partial train loss (single batch): 0.065282\n",
      "Epoch 918/1000 train loss: [array(0.06528246, dtype=float32)], val loss: 0.06582480669021606\n",
      "\t partial train loss (single batch): 0.066057\n",
      "Epoch 919/1000 train loss: [array(0.06605724, dtype=float32)], val loss: 0.06571629643440247\n",
      "\t partial train loss (single batch): 0.065171\n",
      "Epoch 920/1000 train loss: [array(0.06517141, dtype=float32)], val loss: 0.06551721692085266\n",
      "\t partial train loss (single batch): 0.066312\n",
      "Epoch 921/1000 train loss: [array(0.06631233, dtype=float32)], val loss: 0.06703174114227295\n",
      "\t partial train loss (single batch): 0.066375\n",
      "Epoch 922/1000 train loss: [array(0.06637465, dtype=float32)], val loss: 0.06623522192239761\n",
      "\t partial train loss (single batch): 0.065174\n",
      "Epoch 923/1000 train loss: [array(0.06517402, dtype=float32)], val loss: 0.06582607328891754\n",
      "\t partial train loss (single batch): 0.067433\n",
      "Epoch 924/1000 train loss: [array(0.06743251, dtype=float32)], val loss: 0.06593623757362366\n",
      "\t partial train loss (single batch): 0.065240\n",
      "Epoch 925/1000 train loss: [array(0.06524032, dtype=float32)], val loss: 0.06624549627304077\n",
      "\t partial train loss (single batch): 0.065678\n",
      "Epoch 926/1000 train loss: [array(0.06567831, dtype=float32)], val loss: 0.06599757820367813\n",
      "\t partial train loss (single batch): 0.065130\n",
      "Epoch 927/1000 train loss: [array(0.06512991, dtype=float32)], val loss: 0.06644140183925629\n",
      "\t partial train loss (single batch): 0.065910\n",
      "Epoch 928/1000 train loss: [array(0.06591009, dtype=float32)], val loss: 0.06492418795824051\n",
      "\t partial train loss (single batch): 0.065877\n",
      "Epoch 929/1000 train loss: [array(0.06587723, dtype=float32)], val loss: 0.06661985069513321\n",
      "\t partial train loss (single batch): 0.065987\n",
      "Epoch 930/1000 train loss: [array(0.06598711, dtype=float32)], val loss: 0.0653931051492691\n",
      "\t partial train loss (single batch): 0.065944\n",
      "Epoch 931/1000 train loss: [array(0.06594425, dtype=float32)], val loss: 0.0665067657828331\n",
      "\t partial train loss (single batch): 0.065800\n",
      "Epoch 932/1000 train loss: [array(0.0657995, dtype=float32)], val loss: 0.06581751257181168\n",
      "\t partial train loss (single batch): 0.064979\n",
      "Epoch 933/1000 train loss: [array(0.06497876, dtype=float32)], val loss: 0.06609785556793213\n",
      "\t partial train loss (single batch): 0.065457\n",
      "Epoch 934/1000 train loss: [array(0.06545731, dtype=float32)], val loss: 0.06613428890705109\n",
      "\t partial train loss (single batch): 0.066592\n",
      "Epoch 935/1000 train loss: [array(0.06659204, dtype=float32)], val loss: 0.06552749872207642\n",
      "\t partial train loss (single batch): 0.065583\n",
      "Epoch 936/1000 train loss: [array(0.06558289, dtype=float32)], val loss: 0.06687392294406891\n",
      "\t partial train loss (single batch): 0.066021\n",
      "Epoch 937/1000 train loss: [array(0.06602129, dtype=float32)], val loss: 0.06563184410333633\n",
      "\t partial train loss (single batch): 0.066328\n",
      "Epoch 938/1000 train loss: [array(0.06632844, dtype=float32)], val loss: 0.06618407368659973\n",
      "\t partial train loss (single batch): 0.066246\n",
      "Epoch 939/1000 train loss: [array(0.06624587, dtype=float32)], val loss: 0.06609231233596802\n",
      "\t partial train loss (single batch): 0.065495\n",
      "Epoch 940/1000 train loss: [array(0.06549489, dtype=float32)], val loss: 0.06650449335575104\n",
      "\t partial train loss (single batch): 0.066185\n",
      "Epoch 941/1000 train loss: [array(0.06618506, dtype=float32)], val loss: 0.06577900797128677\n",
      "\t partial train loss (single batch): 0.065560\n",
      "Epoch 942/1000 train loss: [array(0.06555963, dtype=float32)], val loss: 0.06610490381717682\n",
      "\t partial train loss (single batch): 0.064699\n",
      "Epoch 943/1000 train loss: [array(0.06469882, dtype=float32)], val loss: 0.06623441725969315\n",
      "\t partial train loss (single batch): 0.065660\n",
      "Epoch 944/1000 train loss: [array(0.0656602, dtype=float32)], val loss: 0.06614965200424194\n",
      "\t partial train loss (single batch): 0.065086\n",
      "Epoch 945/1000 train loss: [array(0.06508566, dtype=float32)], val loss: 0.06435742229223251\n",
      "\t partial train loss (single batch): 0.065806\n",
      "Epoch 946/1000 train loss: [array(0.06580554, dtype=float32)], val loss: 0.06549180299043655\n",
      "\t partial train loss (single batch): 0.065987\n",
      "Epoch 947/1000 train loss: [array(0.06598689, dtype=float32)], val loss: 0.06567861884832382\n",
      "\t partial train loss (single batch): 0.064650\n",
      "Epoch 948/1000 train loss: [array(0.06465035, dtype=float32)], val loss: 0.0661671906709671\n",
      "\t partial train loss (single batch): 0.065912\n",
      "Epoch 949/1000 train loss: [array(0.06591247, dtype=float32)], val loss: 0.06581632047891617\n",
      "\t partial train loss (single batch): 0.065095\n",
      "Epoch 950/1000 train loss: [array(0.06509528, dtype=float32)], val loss: 0.06656848639249802\n",
      "\t partial train loss (single batch): 0.065142\n",
      "Epoch 951/1000 train loss: [array(0.06514164, dtype=float32)], val loss: 0.06600504368543625\n",
      "\t partial train loss (single batch): 0.065617\n",
      "Epoch 952/1000 train loss: [array(0.06561688, dtype=float32)], val loss: 0.06514208763837814\n",
      "\t partial train loss (single batch): 0.065286\n",
      "Epoch 953/1000 train loss: [array(0.06528606, dtype=float32)], val loss: 0.06572111696004868\n",
      "\t partial train loss (single batch): 0.064984\n",
      "Epoch 954/1000 train loss: [array(0.06498429, dtype=float32)], val loss: 0.06414824724197388\n",
      "\t partial train loss (single batch): 0.064054\n",
      "Epoch 955/1000 train loss: [array(0.06405351, dtype=float32)], val loss: 0.06643935292959213\n",
      "\t partial train loss (single batch): 0.066344\n",
      "Epoch 956/1000 train loss: [array(0.06634432, dtype=float32)], val loss: 0.06512312591075897\n",
      "\t partial train loss (single batch): 0.065103\n",
      "Epoch 957/1000 train loss: [array(0.06510275, dtype=float32)], val loss: 0.06616716831922531\n",
      "\t partial train loss (single batch): 0.065106\n",
      "Epoch 958/1000 train loss: [array(0.06510561, dtype=float32)], val loss: 0.06654509902000427\n",
      "\t partial train loss (single batch): 0.065343\n",
      "Epoch 959/1000 train loss: [array(0.06534328, dtype=float32)], val loss: 0.06592146307229996\n",
      "\t partial train loss (single batch): 0.065110\n",
      "Epoch 960/1000 train loss: [array(0.06511013, dtype=float32)], val loss: 0.06567207723855972\n",
      "\t partial train loss (single batch): 0.065887\n",
      "Epoch 961/1000 train loss: [array(0.06588659, dtype=float32)], val loss: 0.06528130918741226\n",
      "\t partial train loss (single batch): 0.065153\n",
      "Epoch 962/1000 train loss: [array(0.06515332, dtype=float32)], val loss: 0.06600970029830933\n",
      "\t partial train loss (single batch): 0.064781\n",
      "Epoch 963/1000 train loss: [array(0.06478071, dtype=float32)], val loss: 0.06530378013849258\n",
      "\t partial train loss (single batch): 0.064938\n",
      "Epoch 964/1000 train loss: [array(0.06493766, dtype=float32)], val loss: 0.06524728238582611\n",
      "\t partial train loss (single batch): 0.065975\n",
      "Epoch 965/1000 train loss: [array(0.06597483, dtype=float32)], val loss: 0.06597207486629486\n",
      "\t partial train loss (single batch): 0.066215\n",
      "Epoch 966/1000 train loss: [array(0.06621462, dtype=float32)], val loss: 0.0645441859960556\n",
      "\t partial train loss (single batch): 0.065171\n",
      "Epoch 967/1000 train loss: [array(0.06517102, dtype=float32)], val loss: 0.06599290668964386\n",
      "\t partial train loss (single batch): 0.066192\n",
      "Epoch 968/1000 train loss: [array(0.06619227, dtype=float32)], val loss: 0.06604081392288208\n",
      "\t partial train loss (single batch): 0.064502\n",
      "Epoch 969/1000 train loss: [array(0.06450162, dtype=float32)], val loss: 0.06536775827407837\n",
      "\t partial train loss (single batch): 0.065273\n",
      "Epoch 970/1000 train loss: [array(0.06527297, dtype=float32)], val loss: 0.06600357592105865\n",
      "\t partial train loss (single batch): 0.065846\n",
      "Epoch 971/1000 train loss: [array(0.06584637, dtype=float32)], val loss: 0.06602925807237625\n",
      "\t partial train loss (single batch): 0.065854\n",
      "Epoch 972/1000 train loss: [array(0.0658543, dtype=float32)], val loss: 0.06512146443128586\n",
      "\t partial train loss (single batch): 0.064824\n",
      "Epoch 973/1000 train loss: [array(0.06482357, dtype=float32)], val loss: 0.0651777982711792\n",
      "\t partial train loss (single batch): 0.065831\n",
      "Epoch 974/1000 train loss: [array(0.06583066, dtype=float32)], val loss: 0.06494840234518051\n",
      "\t partial train loss (single batch): 0.064815\n",
      "Epoch 975/1000 train loss: [array(0.06481475, dtype=float32)], val loss: 0.06489456444978714\n",
      "\t partial train loss (single batch): 0.065214\n",
      "Epoch 976/1000 train loss: [array(0.06521444, dtype=float32)], val loss: 0.06559022516012192\n",
      "\t partial train loss (single batch): 0.066018\n",
      "Epoch 977/1000 train loss: [array(0.06601838, dtype=float32)], val loss: 0.06534840166568756\n",
      "\t partial train loss (single batch): 0.065126\n",
      "Epoch 978/1000 train loss: [array(0.06512599, dtype=float32)], val loss: 0.06524541974067688\n",
      "\t partial train loss (single batch): 0.064452\n",
      "Epoch 979/1000 train loss: [array(0.06445213, dtype=float32)], val loss: 0.06548000872135162\n",
      "\t partial train loss (single batch): 0.064531\n",
      "Epoch 980/1000 train loss: [array(0.06453083, dtype=float32)], val loss: 0.06562966108322144\n",
      "\t partial train loss (single batch): 0.064912\n",
      "Epoch 981/1000 train loss: [array(0.06491208, dtype=float32)], val loss: 0.06571158766746521\n",
      "\t partial train loss (single batch): 0.064724\n",
      "Epoch 982/1000 train loss: [array(0.06472432, dtype=float32)], val loss: 0.06550014764070511\n",
      "\t partial train loss (single batch): 0.065126\n",
      "Epoch 983/1000 train loss: [array(0.06512608, dtype=float32)], val loss: 0.06453710049390793\n",
      "\t partial train loss (single batch): 0.063894\n",
      "Epoch 984/1000 train loss: [array(0.06389429, dtype=float32)], val loss: 0.06595715135335922\n",
      "\t partial train loss (single batch): 0.065314\n",
      "Epoch 985/1000 train loss: [array(0.06531357, dtype=float32)], val loss: 0.0651431679725647\n",
      "\t partial train loss (single batch): 0.065548\n",
      "Epoch 986/1000 train loss: [array(0.06554753, dtype=float32)], val loss: 0.0650022104382515\n",
      "\t partial train loss (single batch): 0.064203\n",
      "Epoch 987/1000 train loss: [array(0.06420342, dtype=float32)], val loss: 0.06540228426456451\n",
      "\t partial train loss (single batch): 0.065424\n",
      "Epoch 988/1000 train loss: [array(0.06542363, dtype=float32)], val loss: 0.0654309093952179\n",
      "\t partial train loss (single batch): 0.065420\n",
      "Epoch 989/1000 train loss: [array(0.06542002, dtype=float32)], val loss: 0.06548645347356796\n",
      "\t partial train loss (single batch): 0.065214\n",
      "Epoch 990/1000 train loss: [array(0.06521379, dtype=float32)], val loss: 0.0660891905426979\n",
      "\t partial train loss (single batch): 0.065754\n",
      "Epoch 991/1000 train loss: [array(0.06575444, dtype=float32)], val loss: 0.06581015139818192\n",
      "\t partial train loss (single batch): 0.064250\n",
      "Epoch 992/1000 train loss: [array(0.06424983, dtype=float32)], val loss: 0.06498251855373383\n",
      "\t partial train loss (single batch): 0.065358\n",
      "Epoch 993/1000 train loss: [array(0.06535796, dtype=float32)], val loss: 0.06563518196344376\n",
      "\t partial train loss (single batch): 0.065211\n",
      "Epoch 994/1000 train loss: [array(0.06521133, dtype=float32)], val loss: 0.06415904313325882\n",
      "\t partial train loss (single batch): 0.064274\n",
      "Epoch 995/1000 train loss: [array(0.06427365, dtype=float32)], val loss: 0.06484099477529526\n",
      "\t partial train loss (single batch): 0.064807\n",
      "Epoch 996/1000 train loss: [array(0.06480663, dtype=float32)], val loss: 0.06521529704332352\n",
      "\t partial train loss (single batch): 0.064816\n",
      "Epoch 997/1000 train loss: [array(0.06481554, dtype=float32)], val loss: 0.06395648419857025\n",
      "\t partial train loss (single batch): 0.064522\n",
      "Epoch 998/1000 train loss: [array(0.06452209, dtype=float32)], val loss: 0.06518327444791794\n",
      "\t partial train loss (single batch): 0.064665\n",
      "Epoch 999/1000 train loss: [array(0.06466461, dtype=float32)], val loss: 0.06542918086051941\n",
      "\t partial train loss (single batch): 0.064505\n",
      "Epoch 1000/1000 train loss: [array(0.06450486, dtype=float32)], val loss: 0.06440355628728867\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import random \n",
    "num_epochs = 1000\n",
    "history={'train_loss':[],'val_loss':[]}\n",
    "for epoch in range(num_epochs):\n",
    "   img_train, labels_train = generator.get_random_batch(training=True)\n",
    "   img_test, labels_test = generator.get_random_batch(training=False)\n",
    "\n",
    "   #img_train = np.expand_dims(img_train, axis=-1)\n",
    "   #img_train = np.swapaxes(img_train, 1, -1)\n",
    "   #img_test = np.expand_dims(img_test, axis=-1)\n",
    "   #img_test = np.swapaxes(img_test, 1, -1)\n",
    "   img_train = torch.tensor(img_train)\n",
    "   img_train = img_train.type(torch.FloatTensor)\n",
    "   img_test = torch.tensor(img_test)\n",
    "   img_test = img_test.type(torch.FloatTensor)\n",
    "   train_loss = train_epoch(encoder,decoder,loss_fn,optim, img_train)\n",
    "   val_loss = test_epoch(encoder,decoder,loss_fn, img_test)\n",
    "   print(f\"Epoch {epoch+1}/{num_epochs} train loss: {train_loss}, val loss: {val_loss}\")\n",
    "   #print('\\n EPOCH {}/{} \\t train loss {:.3f} \\t val loss {:.3f}'.format(epoch + 1, num_epochs,train_loss,val_loss))\n",
    "   history['train_loss'].append(train_loss)\n",
    "   history['val_loss'].append(val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "plt.semilogy(history['train_loss'], label='Train')\n",
    "plt.semilogy(history['val_loss'], label='Valid')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Average Loss')\n",
    "#plt.grid()\n",
    "plt.legend()\n",
    "#plt.title('loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 3, 28, 28])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7faddb8a6700>"
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "z = np.random.randn(100, d)\n",
    "z = torch.Tensor(z)\n",
    "with torch.no_grad():\n",
    "    x_hat = decoder(z)\n",
    "print(x_hat.shape)\n",
    "x = np.array(x_hat[0])\n",
    "#x = x[2, :, :]\n",
    "#x = x.reshape((28, 28))\n",
    "x = np.swapaxes(x, 0, 1)\n",
    "x = np.swapaxes(x, 1, 2)\n",
    "\n",
    "plt.imshow(x)\n",
    "\n",
    "r = x[:, :, 0]\n",
    "g = x[:, :, 1]\n",
    "b = x[:, :, 2]\n",
    "\n",
    "plt.imshow(r)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "49ff9a1d55cbad36b515c3ded8837e12145fab330794be4b4ac6e95d3772d975"
  },
  "kernelspec": {
   "display_name": "Python 3.8.0 ('tensorflow')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

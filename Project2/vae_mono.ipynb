{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "from stacked_mnist import StackedMNISTData, DataMode\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_dim = 28 * 28\n",
    "intermediate_dim = 64\n",
    "latent_dim = 8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_img = keras.Input(shape=(28, 28, 1))\n",
    "\n",
    "# Encoder\n",
    "# Encoder\n",
    "x = layers.Conv2D(32, (3, 3), activation=\"relu\", padding=\"same\")(input_img)\n",
    "x = layers.MaxPooling2D((2, 2), padding=\"same\")(x)\n",
    "x = layers.Conv2D(32, (3, 3), activation=\"relu\", padding=\"same\")(x)\n",
    "x = layers.MaxPooling2D((2, 2), padding=\"same\")(x)\n",
    "x = layers.Flatten()(x)\n",
    "h = layers.Dense(64, activation='relu')(x)\n",
    "z_mean = layers.Dense(latent_dim)(h)\n",
    "z_log_sigma = layers.Dense(latent_dim)(h)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "def sampling(args):\n",
    "    z_mean, z_log_sigma = args\n",
    "    epsilon = K.random_normal(shape=(K.shape(z_mean)[0], latent_dim),\n",
    "                              mean=0., stddev=0.1)\n",
    "    return z_mean + K.exp(z_log_sigma) * epsilon\n",
    "\n",
    "z = layers.Lambda(sampling)([z_mean, z_log_sigma])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_16 (InputLayer)          [(None, 28, 28, 1)]  0           []                               \n",
      "                                                                                                  \n",
      " conv2d_29 (Conv2D)             (None, 28, 28, 32)   320         ['input_16[0][0]']               \n",
      "                                                                                                  \n",
      " max_pooling2d_19 (MaxPooling2D  (None, 14, 14, 32)  0           ['conv2d_29[0][0]']              \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " conv2d_30 (Conv2D)             (None, 14, 14, 32)   9248        ['max_pooling2d_19[0][0]']       \n",
      "                                                                                                  \n",
      " max_pooling2d_20 (MaxPooling2D  (None, 7, 7, 32)    0           ['conv2d_30[0][0]']              \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " flatten_8 (Flatten)            (None, 1568)         0           ['max_pooling2d_20[0][0]']       \n",
      "                                                                                                  \n",
      " dense_71 (Dense)               (None, 64)           100416      ['flatten_8[0][0]']              \n",
      "                                                                                                  \n",
      " dense_72 (Dense)               (None, 8)            520         ['dense_71[0][0]']               \n",
      "                                                                                                  \n",
      " dense_73 (Dense)               (None, 8)            520         ['dense_71[0][0]']               \n",
      "                                                                                                  \n",
      " lambda_8 (Lambda)              (None, 8)            0           ['dense_72[0][0]',               \n",
      "                                                                  'dense_73[0][0]']               \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 111,024\n",
      "Trainable params: 111,024\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " z_sampling (InputLayer)     [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_74 (Dense)            (None, 64)                576       \n",
      "                                                                 \n",
      " dense_75 (Dense)            (None, 1568)              101920    \n",
      "                                                                 \n",
      " reshape_8 (Reshape)         (None, 7, 7, 32)          0         \n",
      "                                                                 \n",
      " conv2d_transpose_16 (Conv2D  (None, 14, 14, 32)       9248      \n",
      " Transpose)                                                      \n",
      "                                                                 \n",
      " conv2d_transpose_17 (Conv2D  (None, 28, 28, 32)       9248      \n",
      " Transpose)                                                      \n",
      "                                                                 \n",
      " conv2d_31 (Conv2D)          (None, 28, 28, 1)         289       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 121,281\n",
      "Trainable params: 121,281\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"vae_mlp\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_16 (InputLayer)       [(None, 28, 28, 1)]       0         \n",
      "                                                                 \n",
      " encoder (Functional)        [(None, 8),               111024    \n",
      "                              (None, 8),                         \n",
      "                              (None, 8)]                         \n",
      "                                                                 \n",
      " decoder (Functional)        (None, 28, 28, 1)         121281    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 232,305\n",
      "Trainable params: 232,305\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Create encoder\n",
    "encoder = keras.Model(input_img, [z_mean, z_log_sigma, z], name='encoder')\n",
    "encoder.summary()\n",
    "\n",
    "# Create decoder\n",
    "latent_inputs = keras.Input(shape=(latent_dim,), name='z_sampling')\n",
    "x = layers.Dense(intermediate_dim, activation='relu')(latent_inputs)\n",
    "x = layers.Dense(1568, activation='relu')(x)\n",
    "x = layers.Reshape((7, 7, 32))(x)\n",
    "x = layers.Conv2DTranspose(32, (3, 3), strides=2, activation=\"relu\", padding=\"same\")(x)\n",
    "x = layers.Conv2DTranspose(32, (3, 3), strides=2, activation=\"relu\", padding=\"same\")(x)\n",
    "outputs = layers.Conv2D(1, (3, 3), activation=\"sigmoid\", padding=\"same\")(x)\n",
    "decoder = keras.Model(latent_inputs, outputs, name='decoder')\n",
    "decoder.summary()\n",
    "\n",
    "# instantiate VAE model\n",
    "outputs = decoder(encoder(input_img)[2])\n",
    "vae = keras.Model(input_img, outputs, name='vae_mlp')\n",
    "vae.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstruction_loss = keras.losses.binary_crossentropy(input_img, outputs)\n",
    "reconstruction_loss *= original_dim\n",
    "kl_loss = 1 + z_log_sigma - K.square(z_mean) - K.exp(z_log_sigma)\n",
    "kl_loss = K.sum(kl_loss, axis=-1)\n",
    "kl_loss *= -0.5\n",
    "vae_loss = K.mean(reconstruction_loss + kl_loss)\n",
    "vae.add_loss(vae_loss)\n",
    "vae.compile(optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28, 1)\n",
      "(10000, 28, 28, 1)\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/usr/local/anaconda3/envs/tensorflow/lib/python3.8/site-packages/keras/engine/training.py\", line 878, in train_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/anaconda3/envs/tensorflow/lib/python3.8/site-packages/keras/engine/training.py\", line 867, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/anaconda3/envs/tensorflow/lib/python3.8/site-packages/keras/engine/training.py\", line 860, in run_step  **\n        outputs = model.train_step(data)\n    File \"/usr/local/anaconda3/envs/tensorflow/lib/python3.8/site-packages/keras/engine/training.py\", line 808, in train_step\n        y_pred = self(x, training=True)\n    File \"/usr/local/anaconda3/envs/tensorflow/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 67, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n\n    ValueError: Exception encountered when calling layer \"tf.__operators__.add_31\" (type TFOpLambda).\n    \n    Dimensions must be equal, but are 28 and 32 for '{{node vae_mlp/tf.__operators__.add_31/AddV2}} = AddV2[T=DT_FLOAT](vae_mlp/tf.math.multiply_29/Mul, vae_mlp/tf.math.multiply_30/Mul)' with input shapes: [32,28,28], [32].\n    \n    Call arguments received:\n      • x=tf.Tensor(shape=(32, 28, 28), dtype=float32)\n      • y=tf.Tensor(shape=(32,), dtype=float32)\n      • name=None\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/w2/4g3c4yrn38g2nwwdhjdlx6fc0000gn/T/ipykernel_27275/3609834628.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m vae.fit(x_train, x_train,\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/tensorflow/lib/python3.8/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/tensorflow/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mautograph_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1127\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1128\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1129\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1130\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/usr/local/anaconda3/envs/tensorflow/lib/python3.8/site-packages/keras/engine/training.py\", line 878, in train_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/anaconda3/envs/tensorflow/lib/python3.8/site-packages/keras/engine/training.py\", line 867, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/anaconda3/envs/tensorflow/lib/python3.8/site-packages/keras/engine/training.py\", line 860, in run_step  **\n        outputs = model.train_step(data)\n    File \"/usr/local/anaconda3/envs/tensorflow/lib/python3.8/site-packages/keras/engine/training.py\", line 808, in train_step\n        y_pred = self(x, training=True)\n    File \"/usr/local/anaconda3/envs/tensorflow/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 67, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n\n    ValueError: Exception encountered when calling layer \"tf.__operators__.add_31\" (type TFOpLambda).\n    \n    Dimensions must be equal, but are 28 and 32 for '{{node vae_mlp/tf.__operators__.add_31/AddV2}} = AddV2[T=DT_FLOAT](vae_mlp/tf.math.multiply_29/Mul, vae_mlp/tf.math.multiply_30/Mul)' with input shapes: [32,28,28], [32].\n    \n    Call arguments received:\n      • x=tf.Tensor(shape=(32, 28, 28), dtype=float32)\n      • y=tf.Tensor(shape=(32,), dtype=float32)\n      • name=None\n"
     ]
    }
   ],
   "source": [
    "generator = StackedMNISTData(mode=DataMode.MONO_BINARY_COMPLETE, default_batch_size=2048)\n",
    "\n",
    "x_train, y_train = generator.get_full_data_set(training=True)\n",
    "x_test, y_test = generator.get_full_data_set(training=False)\n",
    "\n",
    "# \"Translate\": Only look at \"red\" channel; only use the last digit. Use one-hot for labels during training\n",
    "x_train = x_train[:, :, :, [0]]\n",
    "y_train = keras.utils.to_categorical((y_train % 10).astype(np.int), 10)\n",
    "x_test = x_test[:, :, :, [0]]\n",
    "y_test = keras.utils.to_categorical((y_test % 10).astype(np.int), 10)\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)\n",
    "vae.fit(x_train, x_train,\n",
    "        epochs=50,\n",
    "        batch_size=32,\n",
    "        validation_data=(x_test, x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not tuple",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/w2/4g3c4yrn38g2nwwdhjdlx6fc0000gn/T/ipykernel_27275/2138102838.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mx_test_encoded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test_encoded\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test_encoded\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolorbar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not tuple"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x432 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_test_encoded = encoder.predict(x_test)\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(x_test_encoded[:, 0], x_test_encoded[:, 1], c=y_test)\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/usr/local/anaconda3/envs/tensorflow/lib/python3.8/site-packages/keras/engine/training.py\", line 1621, in predict_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/anaconda3/envs/tensorflow/lib/python3.8/site-packages/keras/engine/training.py\", line 1611, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/anaconda3/envs/tensorflow/lib/python3.8/site-packages/keras/engine/training.py\", line 1604, in run_step  **\n        outputs = model.predict_step(data)\n    File \"/usr/local/anaconda3/envs/tensorflow/lib/python3.8/site-packages/keras/engine/training.py\", line 1572, in predict_step\n        return self(x, training=False)\n    File \"/usr/local/anaconda3/envs/tensorflow/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 67, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/usr/local/anaconda3/envs/tensorflow/lib/python3.8/site-packages/keras/engine/input_spec.py\", line 263, in assert_input_compatibility\n        raise ValueError(f'Input {input_index} of layer \"{layer_name}\" is '\n\n    ValueError: Input 0 of layer \"decoder\" is incompatible with the layer: expected shape=(None, 8), found shape=(None, 2)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/w2/4g3c4yrn38g2nwwdhjdlx6fc0000gn/T/ipykernel_27275/2829671452.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrid_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mz_sample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mxi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mx_decoded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz_sample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mdigit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_decoded\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdigit_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdigit_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         figure[i * digit_size: (i + 1) * digit_size,\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/tensorflow/lib/python3.8/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/tensorflow/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mautograph_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1127\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1128\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1129\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1130\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/usr/local/anaconda3/envs/tensorflow/lib/python3.8/site-packages/keras/engine/training.py\", line 1621, in predict_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/anaconda3/envs/tensorflow/lib/python3.8/site-packages/keras/engine/training.py\", line 1611, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/anaconda3/envs/tensorflow/lib/python3.8/site-packages/keras/engine/training.py\", line 1604, in run_step  **\n        outputs = model.predict_step(data)\n    File \"/usr/local/anaconda3/envs/tensorflow/lib/python3.8/site-packages/keras/engine/training.py\", line 1572, in predict_step\n        return self(x, training=False)\n    File \"/usr/local/anaconda3/envs/tensorflow/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 67, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/usr/local/anaconda3/envs/tensorflow/lib/python3.8/site-packages/keras/engine/input_spec.py\", line 263, in assert_input_compatibility\n        raise ValueError(f'Input {input_index} of layer \"{layer_name}\" is '\n\n    ValueError: Input 0 of layer \"decoder\" is incompatible with the layer: expected shape=(None, 8), found shape=(None, 2)\n"
     ]
    }
   ],
   "source": [
    "# Display a 2D manifold of the digits\n",
    "n = 15  # figure with 15x15 digits\n",
    "digit_size = 28\n",
    "figure = np.zeros((digit_size * n, digit_size * n))\n",
    "# We will sample n points within [-15, 15] standard deviations\n",
    "grid_x = np.linspace(-15, 15, n)\n",
    "grid_y = np.linspace(-15, 15, n)\n",
    "\n",
    "for i, yi in enumerate(grid_x):\n",
    "    for j, xi in enumerate(grid_y):\n",
    "        z_sample = np.array([[xi, yi]])\n",
    "        x_decoded = decoder.predict(z_sample)\n",
    "        digit = x_decoded[0].reshape(digit_size, digit_size)\n",
    "        figure[i * digit_size: (i + 1) * digit_size,\n",
    "               j * digit_size: (j + 1) * digit_size] = digit\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(figure)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 360x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "z_sample = np.random.standard_normal((10, 8))\n",
    "x_decoded = decoder.predict(z_sample)\n",
    "\n",
    "plt.figure(figsize=(5, 4))\n",
    "for i in range(1, 5):\n",
    "    plt.imshow(x_decoded[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "49ff9a1d55cbad36b515c3ded8837e12145fab330794be4b4ac6e95d3772d975"
  },
  "kernelspec": {
   "display_name": "Python 3.8.0 ('tensorflow')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "from stacked_mnist import StackedMNISTData, DataMode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, encoded_space_dim, fc2_input_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        ### Convolutional section\n",
    "        self.encoder_cnn = nn.Sequential(\n",
    "            # First convolutional layer\n",
    "            nn.Conv2d(1, 8, 3, stride=2, padding=1),\n",
    "            # nn.BatchNorm2d(8),\n",
    "            nn.ReLU(True),\n",
    "            # Second convolutional layer\n",
    "            nn.Conv2d(8, 16, 3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(True),\n",
    "            # Third convolutional layer\n",
    "            nn.Conv2d(16, 32, 3, stride=2, padding=0),\n",
    "            # nn.BatchNorm2d(32),\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "\n",
    "        ### Flatten layer\n",
    "        self.flatten = nn.Flatten(start_dim=1)\n",
    "\n",
    "        ### Linear section\n",
    "        self.encoder_lin = nn.Sequential(\n",
    "            # First linear layer\n",
    "            nn.Linear(3 * 3 * 32, 128),\n",
    "            nn.ReLU(True),\n",
    "            # Second linear layer\n",
    "            nn.Linear(128, encoded_space_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply convolutions\n",
    "        x = self.encoder_cnn(x)\n",
    "        # Flatten\n",
    "        x = self.flatten(x)\n",
    "        # # Apply linear layers\n",
    "        x = self.encoder_lin(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, encoded_space_dim,fc2_input_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        ### Linear section\n",
    "        self.decoder_lin = nn.Sequential(\n",
    "            # First linear layer\n",
    "            nn.Linear(encoded_space_dim, 128),\n",
    "            nn.ReLU(True),\n",
    "            # Second linear layer\n",
    "            nn.Linear(128, 3 * 3 * 32),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "\n",
    "        ### Unflatten\n",
    "        self.unflatten = nn.Unflatten(dim=1, unflattened_size=(32, 3, 3))\n",
    "\n",
    "        ### Convolutional section\n",
    "        self.decoder_conv = nn.Sequential(\n",
    "            # First transposed convolution\n",
    "            nn.ConvTranspose2d(32, 16, 3, stride=2, output_padding=0),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(True),\n",
    "            # Second transposed convolution\n",
    "            nn.ConvTranspose2d(16, 8, 3, stride=2, padding=1, output_padding=1),\n",
    "            nn.BatchNorm2d(8),\n",
    "            nn.ReLU(True),\n",
    "            # Third transposed convolution\n",
    "            nn.ConvTranspose2d(8, 1, 3, stride=2, padding=1, output_padding=1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Apply linear layers\n",
    "        x = self.decoder_lin(x)\n",
    "        # Unflatten\n",
    "        x = self.unflatten(x)\n",
    "        # Apply transposed convolutions\n",
    "        x = self.decoder_conv(x)\n",
    "        # Apply a sigmoid to force the output to be between 0 and 1 (valid pixel values)\n",
    "        x = torch.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = 8\n",
    "encoder = Encoder(encoded_space_dim=d,fc2_input_dim=128)\n",
    "decoder = Decoder(encoded_space_dim=d,fc2_input_dim=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.BCELoss()\n",
    "lr= 0.001\n",
    "params_to_optimize = [\n",
    "    {'params': encoder.parameters()},\n",
    "    {'params': decoder.parameters()}\n",
    "]\n",
    "optim = torch.optim.Adam(params_to_optimize, lr=lr, weight_decay=1e-05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Training function\n",
    "def train_epoch(encoder, decoder, loss_fn, optimizer, x):\n",
    "    # Set train mode for both the encoder and the decoder\n",
    "    encoder.train()\n",
    "    decoder.train()\n",
    "    train_loss = []\n",
    "\n",
    "    # Encode data\n",
    "    encoded_data = encoder(x)\n",
    "    # Decode data\n",
    "    decoded_data = decoder(encoded_data)\n",
    "    # Evaluate loss\n",
    "    loss = loss_fn(decoded_data, x)\n",
    "    # Backward pass\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    # Print batch loss\n",
    "    print('\\t partial train loss (single batch): %f' % (loss.data))\n",
    "    train_loss.append(loss.detach().numpy())\n",
    "\n",
    "    return train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Testing function\n",
    "def test_epoch(encoder, decoder, loss_fn, x):\n",
    "    # Set evaluation mode for encoder and decoder\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    with torch.no_grad(): # No need to track the gradients\n",
    "        # Define the lists to store the outputs for each batch\n",
    "        conc_out = []\n",
    "        conc_label = []\n",
    "        # Encode data\n",
    "        encoded_data = encoder(x)\n",
    "        # Decode data\n",
    "        decoded_data = decoder(encoded_data)\n",
    "        # Append the network output and the original image to the lists\n",
    "        conc_out.append(decoded_data)\n",
    "        conc_label.append(x)\n",
    "        # Create a single tensor with all the values in the lists\n",
    "        conc_out = torch.cat(conc_out)\n",
    "        conc_label = torch.cat(conc_label) \n",
    "        # Evaluate global loss\n",
    "        val_loss = loss_fn(conc_out, conc_label)\n",
    "    return val_loss.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = StackedMNISTData(mode=DataMode.MONO_BINARY_COMPLETE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t partial train loss (single batch): 0.796594\n",
      "Epoch 1/1000 train loss: [array(0.7965939, dtype=float32)], val loss: 0.72342449426651\n",
      "\t partial train loss (single batch): 0.780184\n",
      "Epoch 2/1000 train loss: [array(0.7801836, dtype=float32)], val loss: 0.7235666513442993\n",
      "\t partial train loss (single batch): 0.765440\n",
      "Epoch 3/1000 train loss: [array(0.76544017, dtype=float32)], val loss: 0.7237394452095032\n",
      "\t partial train loss (single batch): 0.753330\n",
      "Epoch 4/1000 train loss: [array(0.75333, dtype=float32)], val loss: 0.7235433459281921\n",
      "\t partial train loss (single batch): 0.744719\n",
      "Epoch 5/1000 train loss: [array(0.7447194, dtype=float32)], val loss: 0.723918616771698\n",
      "\t partial train loss (single batch): 0.736432\n",
      "Epoch 6/1000 train loss: [array(0.73643154, dtype=float32)], val loss: 0.7243598103523254\n",
      "\t partial train loss (single batch): 0.729565\n",
      "Epoch 7/1000 train loss: [array(0.7295654, dtype=float32)], val loss: 0.7239956259727478\n",
      "\t partial train loss (single batch): 0.723727\n",
      "Epoch 8/1000 train loss: [array(0.72372663, dtype=float32)], val loss: 0.7239655256271362\n",
      "\t partial train loss (single batch): 0.717757\n",
      "Epoch 9/1000 train loss: [array(0.7177569, dtype=float32)], val loss: 0.7236322164535522\n",
      "\t partial train loss (single batch): 0.712262\n",
      "Epoch 10/1000 train loss: [array(0.71226203, dtype=float32)], val loss: 0.7235572934150696\n",
      "\t partial train loss (single batch): 0.706237\n",
      "Epoch 11/1000 train loss: [array(0.706237, dtype=float32)], val loss: 0.7233195304870605\n",
      "\t partial train loss (single batch): 0.701154\n",
      "Epoch 12/1000 train loss: [array(0.70115423, dtype=float32)], val loss: 0.7227721214294434\n",
      "\t partial train loss (single batch): 0.695882\n",
      "Epoch 13/1000 train loss: [array(0.6958825, dtype=float32)], val loss: 0.7216247916221619\n",
      "\t partial train loss (single batch): 0.692299\n",
      "Epoch 14/1000 train loss: [array(0.6922993, dtype=float32)], val loss: 0.7207949757575989\n",
      "\t partial train loss (single batch): 0.687562\n",
      "Epoch 15/1000 train loss: [array(0.68756187, dtype=float32)], val loss: 0.7202979326248169\n",
      "\t partial train loss (single batch): 0.681138\n",
      "Epoch 16/1000 train loss: [array(0.68113834, dtype=float32)], val loss: 0.7188315987586975\n",
      "\t partial train loss (single batch): 0.677616\n",
      "Epoch 17/1000 train loss: [array(0.6776158, dtype=float32)], val loss: 0.7177625894546509\n",
      "\t partial train loss (single batch): 0.673438\n",
      "Epoch 18/1000 train loss: [array(0.6734376, dtype=float32)], val loss: 0.7167887091636658\n",
      "\t partial train loss (single batch): 0.669971\n",
      "Epoch 19/1000 train loss: [array(0.6699712, dtype=float32)], val loss: 0.7154760360717773\n",
      "\t partial train loss (single batch): 0.665470\n",
      "Epoch 20/1000 train loss: [array(0.66547036, dtype=float32)], val loss: 0.7138750553131104\n",
      "\t partial train loss (single batch): 0.660781\n",
      "Epoch 21/1000 train loss: [array(0.66078126, dtype=float32)], val loss: 0.7122894525527954\n",
      "\t partial train loss (single batch): 0.656464\n",
      "Epoch 22/1000 train loss: [array(0.6564641, dtype=float32)], val loss: 0.7103816270828247\n",
      "\t partial train loss (single batch): 0.653715\n",
      "Epoch 23/1000 train loss: [array(0.65371466, dtype=float32)], val loss: 0.7082212567329407\n",
      "\t partial train loss (single batch): 0.649360\n",
      "Epoch 24/1000 train loss: [array(0.64935976, dtype=float32)], val loss: 0.7062518000602722\n",
      "\t partial train loss (single batch): 0.646243\n",
      "Epoch 25/1000 train loss: [array(0.64624304, dtype=float32)], val loss: 0.7037652730941772\n",
      "\t partial train loss (single batch): 0.641900\n",
      "Epoch 26/1000 train loss: [array(0.6419004, dtype=float32)], val loss: 0.7016200423240662\n",
      "\t partial train loss (single batch): 0.638592\n",
      "Epoch 27/1000 train loss: [array(0.6385919, dtype=float32)], val loss: 0.6992884278297424\n",
      "\t partial train loss (single batch): 0.635534\n",
      "Epoch 28/1000 train loss: [array(0.6355341, dtype=float32)], val loss: 0.6962674260139465\n",
      "\t partial train loss (single batch): 0.631233\n",
      "Epoch 29/1000 train loss: [array(0.63123274, dtype=float32)], val loss: 0.6936034560203552\n",
      "\t partial train loss (single batch): 0.629471\n",
      "Epoch 30/1000 train loss: [array(0.6294707, dtype=float32)], val loss: 0.6908378005027771\n",
      "\t partial train loss (single batch): 0.625777\n",
      "Epoch 31/1000 train loss: [array(0.62577736, dtype=float32)], val loss: 0.6879562139511108\n",
      "\t partial train loss (single batch): 0.622035\n",
      "Epoch 32/1000 train loss: [array(0.62203455, dtype=float32)], val loss: 0.684928297996521\n",
      "\t partial train loss (single batch): 0.619140\n",
      "Epoch 33/1000 train loss: [array(0.6191397, dtype=float32)], val loss: 0.6817219257354736\n",
      "\t partial train loss (single batch): 0.614953\n",
      "Epoch 34/1000 train loss: [array(0.6149527, dtype=float32)], val loss: 0.6783032417297363\n",
      "\t partial train loss (single batch): 0.611470\n",
      "Epoch 35/1000 train loss: [array(0.61146975, dtype=float32)], val loss: 0.6753578186035156\n",
      "\t partial train loss (single batch): 0.607755\n",
      "Epoch 36/1000 train loss: [array(0.6077551, dtype=float32)], val loss: 0.6713966727256775\n",
      "\t partial train loss (single batch): 0.605473\n",
      "Epoch 37/1000 train loss: [array(0.6054734, dtype=float32)], val loss: 0.6679959297180176\n",
      "\t partial train loss (single batch): 0.603267\n",
      "Epoch 38/1000 train loss: [array(0.6032675, dtype=float32)], val loss: 0.6642364263534546\n",
      "\t partial train loss (single batch): 0.599251\n",
      "Epoch 39/1000 train loss: [array(0.59925115, dtype=float32)], val loss: 0.6607413291931152\n",
      "\t partial train loss (single batch): 0.596349\n",
      "Epoch 40/1000 train loss: [array(0.5963486, dtype=float32)], val loss: 0.6571246385574341\n",
      "\t partial train loss (single batch): 0.592965\n",
      "Epoch 41/1000 train loss: [array(0.59296536, dtype=float32)], val loss: 0.6527879238128662\n",
      "\t partial train loss (single batch): 0.590208\n",
      "Epoch 42/1000 train loss: [array(0.59020835, dtype=float32)], val loss: 0.6495201587677002\n",
      "\t partial train loss (single batch): 0.588174\n",
      "Epoch 43/1000 train loss: [array(0.5881742, dtype=float32)], val loss: 0.645332396030426\n",
      "\t partial train loss (single batch): 0.584176\n",
      "Epoch 44/1000 train loss: [array(0.5841762, dtype=float32)], val loss: 0.6410072445869446\n",
      "\t partial train loss (single batch): 0.580125\n",
      "Epoch 45/1000 train loss: [array(0.5801248, dtype=float32)], val loss: 0.6359837651252747\n",
      "\t partial train loss (single batch): 0.578128\n",
      "Epoch 46/1000 train loss: [array(0.5781275, dtype=float32)], val loss: 0.6310569047927856\n",
      "\t partial train loss (single batch): 0.574873\n",
      "Epoch 47/1000 train loss: [array(0.57487303, dtype=float32)], val loss: 0.6263481974601746\n",
      "\t partial train loss (single batch): 0.573388\n",
      "Epoch 48/1000 train loss: [array(0.5733884, dtype=float32)], val loss: 0.622474730014801\n",
      "\t partial train loss (single batch): 0.569018\n",
      "Epoch 49/1000 train loss: [array(0.5690175, dtype=float32)], val loss: 0.6179704666137695\n",
      "\t partial train loss (single batch): 0.566996\n",
      "Epoch 50/1000 train loss: [array(0.5669958, dtype=float32)], val loss: 0.6123545169830322\n",
      "\t partial train loss (single batch): 0.565167\n",
      "Epoch 51/1000 train loss: [array(0.5651672, dtype=float32)], val loss: 0.6076953411102295\n",
      "\t partial train loss (single batch): 0.561090\n",
      "Epoch 52/1000 train loss: [array(0.5610896, dtype=float32)], val loss: 0.6026701927185059\n",
      "\t partial train loss (single batch): 0.557905\n",
      "Epoch 53/1000 train loss: [array(0.5579054, dtype=float32)], val loss: 0.5996866822242737\n",
      "\t partial train loss (single batch): 0.556126\n",
      "Epoch 54/1000 train loss: [array(0.5561259, dtype=float32)], val loss: 0.595014214515686\n",
      "\t partial train loss (single batch): 0.554088\n",
      "Epoch 55/1000 train loss: [array(0.55408806, dtype=float32)], val loss: 0.5893833041191101\n",
      "\t partial train loss (single batch): 0.550550\n",
      "Epoch 56/1000 train loss: [array(0.5505496, dtype=float32)], val loss: 0.5840500593185425\n",
      "\t partial train loss (single batch): 0.547859\n",
      "Epoch 57/1000 train loss: [array(0.547859, dtype=float32)], val loss: 0.5803565979003906\n",
      "\t partial train loss (single batch): 0.545633\n",
      "Epoch 58/1000 train loss: [array(0.54563314, dtype=float32)], val loss: 0.5751059651374817\n",
      "\t partial train loss (single batch): 0.542334\n",
      "Epoch 59/1000 train loss: [array(0.5423339, dtype=float32)], val loss: 0.5710198283195496\n",
      "\t partial train loss (single batch): 0.541351\n",
      "Epoch 60/1000 train loss: [array(0.5413509, dtype=float32)], val loss: 0.5658281445503235\n",
      "\t partial train loss (single batch): 0.536612\n",
      "Epoch 61/1000 train loss: [array(0.536612, dtype=float32)], val loss: 0.560617208480835\n",
      "\t partial train loss (single batch): 0.534887\n",
      "Epoch 62/1000 train loss: [array(0.5348869, dtype=float32)], val loss: 0.5573593378067017\n",
      "\t partial train loss (single batch): 0.532947\n",
      "Epoch 63/1000 train loss: [array(0.53294665, dtype=float32)], val loss: 0.5535680651664734\n",
      "\t partial train loss (single batch): 0.528173\n",
      "Epoch 64/1000 train loss: [array(0.5281731, dtype=float32)], val loss: 0.5493255257606506\n",
      "\t partial train loss (single batch): 0.526514\n",
      "Epoch 65/1000 train loss: [array(0.52651393, dtype=float32)], val loss: 0.5434109568595886\n",
      "\t partial train loss (single batch): 0.523056\n",
      "Epoch 66/1000 train loss: [array(0.5230561, dtype=float32)], val loss: 0.5396970510482788\n",
      "\t partial train loss (single batch): 0.521873\n",
      "Epoch 67/1000 train loss: [array(0.5218734, dtype=float32)], val loss: 0.5369231104850769\n",
      "\t partial train loss (single batch): 0.518987\n",
      "Epoch 68/1000 train loss: [array(0.51898664, dtype=float32)], val loss: 0.5345867872238159\n",
      "\t partial train loss (single batch): 0.515807\n",
      "Epoch 69/1000 train loss: [array(0.51580715, dtype=float32)], val loss: 0.5292903184890747\n",
      "\t partial train loss (single batch): 0.514015\n",
      "Epoch 70/1000 train loss: [array(0.51401466, dtype=float32)], val loss: 0.5251831412315369\n",
      "\t partial train loss (single batch): 0.510835\n",
      "Epoch 71/1000 train loss: [array(0.51083523, dtype=float32)], val loss: 0.5204786062240601\n",
      "\t partial train loss (single batch): 0.508609\n",
      "Epoch 72/1000 train loss: [array(0.50860876, dtype=float32)], val loss: 0.5190479159355164\n",
      "\t partial train loss (single batch): 0.506399\n",
      "Epoch 73/1000 train loss: [array(0.5063986, dtype=float32)], val loss: 0.515433132648468\n",
      "\t partial train loss (single batch): 0.503259\n",
      "Epoch 74/1000 train loss: [array(0.50325894, dtype=float32)], val loss: 0.5120359063148499\n",
      "\t partial train loss (single batch): 0.502278\n",
      "Epoch 75/1000 train loss: [array(0.5022781, dtype=float32)], val loss: 0.509932279586792\n",
      "\t partial train loss (single batch): 0.498294\n",
      "Epoch 76/1000 train loss: [array(0.4982941, dtype=float32)], val loss: 0.5056514143943787\n",
      "\t partial train loss (single batch): 0.496686\n",
      "Epoch 77/1000 train loss: [array(0.4966858, dtype=float32)], val loss: 0.503956139087677\n",
      "\t partial train loss (single batch): 0.495122\n",
      "Epoch 78/1000 train loss: [array(0.4951219, dtype=float32)], val loss: 0.4992121160030365\n",
      "\t partial train loss (single batch): 0.491345\n",
      "Epoch 79/1000 train loss: [array(0.491345, dtype=float32)], val loss: 0.4987763464450836\n",
      "\t partial train loss (single batch): 0.490708\n",
      "Epoch 80/1000 train loss: [array(0.4907077, dtype=float32)], val loss: 0.49684953689575195\n",
      "\t partial train loss (single batch): 0.486791\n",
      "Epoch 81/1000 train loss: [array(0.4867911, dtype=float32)], val loss: 0.49322283267974854\n",
      "\t partial train loss (single batch): 0.483842\n",
      "Epoch 82/1000 train loss: [array(0.4838418, dtype=float32)], val loss: 0.4876821041107178\n",
      "\t partial train loss (single batch): 0.482500\n",
      "Epoch 83/1000 train loss: [array(0.48250043, dtype=float32)], val loss: 0.4844866394996643\n",
      "\t partial train loss (single batch): 0.480127\n",
      "Epoch 84/1000 train loss: [array(0.48012707, dtype=float32)], val loss: 0.4862360656261444\n",
      "\t partial train loss (single batch): 0.476730\n",
      "Epoch 85/1000 train loss: [array(0.4767302, dtype=float32)], val loss: 0.4806816577911377\n",
      "\t partial train loss (single batch): 0.473671\n",
      "Epoch 86/1000 train loss: [array(0.47367108, dtype=float32)], val loss: 0.475989431142807\n",
      "\t partial train loss (single batch): 0.473260\n",
      "Epoch 87/1000 train loss: [array(0.47326022, dtype=float32)], val loss: 0.47667214274406433\n",
      "\t partial train loss (single batch): 0.470694\n",
      "Epoch 88/1000 train loss: [array(0.4706938, dtype=float32)], val loss: 0.4770595133304596\n",
      "\t partial train loss (single batch): 0.469100\n",
      "Epoch 89/1000 train loss: [array(0.46909985, dtype=float32)], val loss: 0.4705908000469208\n",
      "\t partial train loss (single batch): 0.467616\n",
      "Epoch 90/1000 train loss: [array(0.46761551, dtype=float32)], val loss: 0.4693518280982971\n",
      "\t partial train loss (single batch): 0.464554\n",
      "Epoch 91/1000 train loss: [array(0.4645536, dtype=float32)], val loss: 0.4656692445278168\n",
      "\t partial train loss (single batch): 0.463318\n",
      "Epoch 92/1000 train loss: [array(0.4633183, dtype=float32)], val loss: 0.4635026156902313\n",
      "\t partial train loss (single batch): 0.459526\n",
      "Epoch 93/1000 train loss: [array(0.45952645, dtype=float32)], val loss: 0.463296115398407\n",
      "\t partial train loss (single batch): 0.458564\n",
      "Epoch 94/1000 train loss: [array(0.45856372, dtype=float32)], val loss: 0.46063458919525146\n",
      "\t partial train loss (single batch): 0.456310\n",
      "Epoch 95/1000 train loss: [array(0.45631042, dtype=float32)], val loss: 0.45757296681404114\n",
      "\t partial train loss (single batch): 0.452644\n",
      "Epoch 96/1000 train loss: [array(0.45264372, dtype=float32)], val loss: 0.4538135826587677\n",
      "\t partial train loss (single batch): 0.451299\n",
      "Epoch 97/1000 train loss: [array(0.45129862, dtype=float32)], val loss: 0.4501321613788605\n",
      "\t partial train loss (single batch): 0.448748\n",
      "Epoch 98/1000 train loss: [array(0.44874772, dtype=float32)], val loss: 0.4506452977657318\n",
      "\t partial train loss (single batch): 0.448354\n",
      "Epoch 99/1000 train loss: [array(0.4483541, dtype=float32)], val loss: 0.4499029815196991\n",
      "\t partial train loss (single batch): 0.444920\n",
      "Epoch 100/1000 train loss: [array(0.4449203, dtype=float32)], val loss: 0.4447808563709259\n",
      "\t partial train loss (single batch): 0.443478\n",
      "Epoch 101/1000 train loss: [array(0.44347823, dtype=float32)], val loss: 0.44259870052337646\n",
      "\t partial train loss (single batch): 0.439971\n",
      "Epoch 102/1000 train loss: [array(0.43997055, dtype=float32)], val loss: 0.4410238564014435\n",
      "\t partial train loss (single batch): 0.439026\n",
      "Epoch 103/1000 train loss: [array(0.43902624, dtype=float32)], val loss: 0.4391184449195862\n",
      "\t partial train loss (single batch): 0.438170\n",
      "Epoch 104/1000 train loss: [array(0.4381695, dtype=float32)], val loss: 0.4372331202030182\n",
      "\t partial train loss (single batch): 0.436465\n",
      "Epoch 105/1000 train loss: [array(0.43646485, dtype=float32)], val loss: 0.430795282125473\n",
      "\t partial train loss (single batch): 0.432661\n",
      "Epoch 106/1000 train loss: [array(0.43266127, dtype=float32)], val loss: 0.42928341031074524\n",
      "\t partial train loss (single batch): 0.429465\n",
      "Epoch 107/1000 train loss: [array(0.42946526, dtype=float32)], val loss: 0.4258595407009125\n",
      "\t partial train loss (single batch): 0.429191\n",
      "Epoch 108/1000 train loss: [array(0.42919108, dtype=float32)], val loss: 0.4259142279624939\n",
      "\t partial train loss (single batch): 0.427501\n",
      "Epoch 109/1000 train loss: [array(0.4275008, dtype=float32)], val loss: 0.42346394062042236\n",
      "\t partial train loss (single batch): 0.426232\n",
      "Epoch 110/1000 train loss: [array(0.42623198, dtype=float32)], val loss: 0.42416688799858093\n",
      "\t partial train loss (single batch): 0.421209\n",
      "Epoch 111/1000 train loss: [array(0.42120922, dtype=float32)], val loss: 0.4183559715747833\n",
      "\t partial train loss (single batch): 0.419817\n",
      "Epoch 112/1000 train loss: [array(0.41981685, dtype=float32)], val loss: 0.4203154444694519\n",
      "\t partial train loss (single batch): 0.417603\n",
      "Epoch 113/1000 train loss: [array(0.41760325, dtype=float32)], val loss: 0.41524234414100647\n",
      "\t partial train loss (single batch): 0.416974\n",
      "Epoch 114/1000 train loss: [array(0.41697422, dtype=float32)], val loss: 0.41309791803359985\n",
      "\t partial train loss (single batch): 0.417402\n",
      "Epoch 115/1000 train loss: [array(0.41740212, dtype=float32)], val loss: 0.41412609815597534\n",
      "\t partial train loss (single batch): 0.415311\n",
      "Epoch 116/1000 train loss: [array(0.41531092, dtype=float32)], val loss: 0.412057489156723\n",
      "\t partial train loss (single batch): 0.411962\n",
      "Epoch 117/1000 train loss: [array(0.4119619, dtype=float32)], val loss: 0.40578335523605347\n",
      "\t partial train loss (single batch): 0.410718\n",
      "Epoch 118/1000 train loss: [array(0.41071776, dtype=float32)], val loss: 0.4066367745399475\n",
      "\t partial train loss (single batch): 0.408509\n",
      "Epoch 119/1000 train loss: [array(0.40850946, dtype=float32)], val loss: 0.4069052040576935\n",
      "\t partial train loss (single batch): 0.405428\n",
      "Epoch 120/1000 train loss: [array(0.40542772, dtype=float32)], val loss: 0.4045758843421936\n",
      "\t partial train loss (single batch): 0.403597\n",
      "Epoch 121/1000 train loss: [array(0.40359715, dtype=float32)], val loss: 0.4032982885837555\n",
      "\t partial train loss (single batch): 0.403481\n",
      "Epoch 122/1000 train loss: [array(0.40348074, dtype=float32)], val loss: 0.40265825390815735\n",
      "\t partial train loss (single batch): 0.401087\n",
      "Epoch 123/1000 train loss: [array(0.40108672, dtype=float32)], val loss: 0.395905464887619\n",
      "\t partial train loss (single batch): 0.397034\n",
      "Epoch 124/1000 train loss: [array(0.39703393, dtype=float32)], val loss: 0.39408454298973083\n",
      "\t partial train loss (single batch): 0.398575\n",
      "Epoch 125/1000 train loss: [array(0.39857504, dtype=float32)], val loss: 0.39475685358047485\n",
      "\t partial train loss (single batch): 0.395931\n",
      "Epoch 126/1000 train loss: [array(0.39593104, dtype=float32)], val loss: 0.395033597946167\n",
      "\t partial train loss (single batch): 0.393016\n",
      "Epoch 127/1000 train loss: [array(0.39301604, dtype=float32)], val loss: 0.39131855964660645\n",
      "\t partial train loss (single batch): 0.392301\n",
      "Epoch 128/1000 train loss: [array(0.39230126, dtype=float32)], val loss: 0.38689959049224854\n",
      "\t partial train loss (single batch): 0.389856\n",
      "Epoch 129/1000 train loss: [array(0.38985604, dtype=float32)], val loss: 0.386278361082077\n",
      "\t partial train loss (single batch): 0.389872\n",
      "Epoch 130/1000 train loss: [array(0.3898725, dtype=float32)], val loss: 0.38925641775131226\n",
      "\t partial train loss (single batch): 0.388334\n",
      "Epoch 131/1000 train loss: [array(0.3883344, dtype=float32)], val loss: 0.3844059407711029\n",
      "\t partial train loss (single batch): 0.385393\n",
      "Epoch 132/1000 train loss: [array(0.38539338, dtype=float32)], val loss: 0.38087111711502075\n",
      "\t partial train loss (single batch): 0.382765\n",
      "Epoch 133/1000 train loss: [array(0.3827654, dtype=float32)], val loss: 0.3790845274925232\n",
      "\t partial train loss (single batch): 0.381145\n",
      "Epoch 134/1000 train loss: [array(0.38114548, dtype=float32)], val loss: 0.37885603308677673\n",
      "\t partial train loss (single batch): 0.379719\n",
      "Epoch 135/1000 train loss: [array(0.37971854, dtype=float32)], val loss: 0.3789248466491699\n",
      "\t partial train loss (single batch): 0.378991\n",
      "Epoch 136/1000 train loss: [array(0.37899143, dtype=float32)], val loss: 0.3749863803386688\n",
      "\t partial train loss (single batch): 0.375329\n",
      "Epoch 137/1000 train loss: [array(0.375329, dtype=float32)], val loss: 0.37253904342651367\n",
      "\t partial train loss (single batch): 0.378134\n",
      "Epoch 138/1000 train loss: [array(0.3781342, dtype=float32)], val loss: 0.3733067512512207\n",
      "\t partial train loss (single batch): 0.373320\n",
      "Epoch 139/1000 train loss: [array(0.37331983, dtype=float32)], val loss: 0.37055930495262146\n",
      "\t partial train loss (single batch): 0.373953\n",
      "Epoch 140/1000 train loss: [array(0.37395328, dtype=float32)], val loss: 0.3660198450088501\n",
      "\t partial train loss (single batch): 0.372353\n",
      "Epoch 141/1000 train loss: [array(0.37235314, dtype=float32)], val loss: 0.366852730512619\n",
      "\t partial train loss (single batch): 0.368355\n",
      "Epoch 142/1000 train loss: [array(0.36835492, dtype=float32)], val loss: 0.3633285164833069\n",
      "\t partial train loss (single batch): 0.368851\n",
      "Epoch 143/1000 train loss: [array(0.3688507, dtype=float32)], val loss: 0.3642939031124115\n",
      "\t partial train loss (single batch): 0.364177\n",
      "Epoch 144/1000 train loss: [array(0.3641766, dtype=float32)], val loss: 0.36432188749313354\n",
      "\t partial train loss (single batch): 0.364546\n",
      "Epoch 145/1000 train loss: [array(0.36454648, dtype=float32)], val loss: 0.3643675148487091\n",
      "\t partial train loss (single batch): 0.362191\n",
      "Epoch 146/1000 train loss: [array(0.36219102, dtype=float32)], val loss: 0.36250224709510803\n",
      "\t partial train loss (single batch): 0.361494\n",
      "Epoch 147/1000 train loss: [array(0.36149433, dtype=float32)], val loss: 0.36088356375694275\n",
      "\t partial train loss (single batch): 0.359960\n",
      "Epoch 148/1000 train loss: [array(0.35995963, dtype=float32)], val loss: 0.3606374263763428\n",
      "\t partial train loss (single batch): 0.357090\n",
      "Epoch 149/1000 train loss: [array(0.3570901, dtype=float32)], val loss: 0.35796138644218445\n",
      "\t partial train loss (single batch): 0.357777\n",
      "Epoch 150/1000 train loss: [array(0.3577774, dtype=float32)], val loss: 0.35550421476364136\n",
      "\t partial train loss (single batch): 0.353758\n",
      "Epoch 151/1000 train loss: [array(0.35375765, dtype=float32)], val loss: 0.35318636894226074\n",
      "\t partial train loss (single batch): 0.353342\n",
      "Epoch 152/1000 train loss: [array(0.35334164, dtype=float32)], val loss: 0.3497239649295807\n",
      "\t partial train loss (single batch): 0.351660\n",
      "Epoch 153/1000 train loss: [array(0.35166043, dtype=float32)], val loss: 0.3502054810523987\n",
      "\t partial train loss (single batch): 0.352925\n",
      "Epoch 154/1000 train loss: [array(0.3529246, dtype=float32)], val loss: 0.34732910990715027\n",
      "\t partial train loss (single batch): 0.349575\n",
      "Epoch 155/1000 train loss: [array(0.3495753, dtype=float32)], val loss: 0.34840479493141174\n",
      "\t partial train loss (single batch): 0.348888\n",
      "Epoch 156/1000 train loss: [array(0.348888, dtype=float32)], val loss: 0.3445378541946411\n",
      "\t partial train loss (single batch): 0.347127\n",
      "Epoch 157/1000 train loss: [array(0.34712738, dtype=float32)], val loss: 0.34724950790405273\n",
      "\t partial train loss (single batch): 0.344783\n",
      "Epoch 158/1000 train loss: [array(0.344783, dtype=float32)], val loss: 0.3456326425075531\n",
      "\t partial train loss (single batch): 0.344203\n",
      "Epoch 159/1000 train loss: [array(0.34420332, dtype=float32)], val loss: 0.34169504046440125\n",
      "\t partial train loss (single batch): 0.344187\n",
      "Epoch 160/1000 train loss: [array(0.34418675, dtype=float32)], val loss: 0.33926451206207275\n",
      "\t partial train loss (single batch): 0.339799\n",
      "Epoch 161/1000 train loss: [array(0.33979928, dtype=float32)], val loss: 0.3397786021232605\n",
      "\t partial train loss (single batch): 0.340998\n",
      "Epoch 162/1000 train loss: [array(0.34099796, dtype=float32)], val loss: 0.3382979929447174\n",
      "\t partial train loss (single batch): 0.338349\n",
      "Epoch 163/1000 train loss: [array(0.33834878, dtype=float32)], val loss: 0.3362518548965454\n",
      "\t partial train loss (single batch): 0.336487\n",
      "Epoch 164/1000 train loss: [array(0.33648655, dtype=float32)], val loss: 0.33506447076797485\n",
      "\t partial train loss (single batch): 0.336308\n",
      "Epoch 165/1000 train loss: [array(0.33630773, dtype=float32)], val loss: 0.32847240567207336\n",
      "\t partial train loss (single batch): 0.332285\n",
      "Epoch 166/1000 train loss: [array(0.33228523, dtype=float32)], val loss: 0.33143338561058044\n",
      "\t partial train loss (single batch): 0.334612\n",
      "Epoch 167/1000 train loss: [array(0.33461154, dtype=float32)], val loss: 0.330646276473999\n",
      "\t partial train loss (single batch): 0.329980\n",
      "Epoch 168/1000 train loss: [array(0.32997996, dtype=float32)], val loss: 0.32907119393348694\n",
      "\t partial train loss (single batch): 0.332558\n",
      "Epoch 169/1000 train loss: [array(0.3325581, dtype=float32)], val loss: 0.3246743381023407\n",
      "\t partial train loss (single batch): 0.329283\n",
      "Epoch 170/1000 train loss: [array(0.3292834, dtype=float32)], val loss: 0.3284684121608734\n",
      "\t partial train loss (single batch): 0.327868\n",
      "Epoch 171/1000 train loss: [array(0.327868, dtype=float32)], val loss: 0.32258301973342896\n",
      "\t partial train loss (single batch): 0.327100\n",
      "Epoch 172/1000 train loss: [array(0.32709995, dtype=float32)], val loss: 0.3239177465438843\n",
      "\t partial train loss (single batch): 0.327279\n",
      "Epoch 173/1000 train loss: [array(0.3272793, dtype=float32)], val loss: 0.3202975392341614\n",
      "\t partial train loss (single batch): 0.321805\n",
      "Epoch 174/1000 train loss: [array(0.32180476, dtype=float32)], val loss: 0.32455572485923767\n",
      "\t partial train loss (single batch): 0.324443\n",
      "Epoch 175/1000 train loss: [array(0.324443, dtype=float32)], val loss: 0.3229992091655731\n",
      "\t partial train loss (single batch): 0.322411\n",
      "Epoch 176/1000 train loss: [array(0.32241118, dtype=float32)], val loss: 0.3217674791812897\n",
      "\t partial train loss (single batch): 0.321228\n",
      "Epoch 177/1000 train loss: [array(0.3212276, dtype=float32)], val loss: 0.31700727343559265\n",
      "\t partial train loss (single batch): 0.321863\n",
      "Epoch 178/1000 train loss: [array(0.3218628, dtype=float32)], val loss: 0.3153426945209503\n",
      "\t partial train loss (single batch): 0.318874\n",
      "Epoch 179/1000 train loss: [array(0.31887442, dtype=float32)], val loss: 0.31331759691238403\n",
      "\t partial train loss (single batch): 0.316760\n",
      "Epoch 180/1000 train loss: [array(0.3167602, dtype=float32)], val loss: 0.310253381729126\n",
      "\t partial train loss (single batch): 0.315327\n",
      "Epoch 181/1000 train loss: [array(0.31532678, dtype=float32)], val loss: 0.31216540932655334\n",
      "\t partial train loss (single batch): 0.316434\n",
      "Epoch 182/1000 train loss: [array(0.31643412, dtype=float32)], val loss: 0.30897992849349976\n",
      "\t partial train loss (single batch): 0.311778\n",
      "Epoch 183/1000 train loss: [array(0.31177798, dtype=float32)], val loss: 0.31043750047683716\n",
      "\t partial train loss (single batch): 0.311554\n",
      "Epoch 184/1000 train loss: [array(0.31155396, dtype=float32)], val loss: 0.3118804097175598\n",
      "\t partial train loss (single batch): 0.310928\n",
      "Epoch 185/1000 train loss: [array(0.31092784, dtype=float32)], val loss: 0.30372869968414307\n",
      "\t partial train loss (single batch): 0.308529\n",
      "Epoch 186/1000 train loss: [array(0.30852887, dtype=float32)], val loss: 0.30501675605773926\n",
      "\t partial train loss (single batch): 0.310230\n",
      "Epoch 187/1000 train loss: [array(0.31023002, dtype=float32)], val loss: 0.29905080795288086\n",
      "\t partial train loss (single batch): 0.305682\n",
      "Epoch 188/1000 train loss: [array(0.30568177, dtype=float32)], val loss: 0.3087265193462372\n",
      "\t partial train loss (single batch): 0.306302\n",
      "Epoch 189/1000 train loss: [array(0.30630243, dtype=float32)], val loss: 0.30756083130836487\n",
      "\t partial train loss (single batch): 0.305796\n",
      "Epoch 190/1000 train loss: [array(0.30579627, dtype=float32)], val loss: 0.30323272943496704\n",
      "\t partial train loss (single batch): 0.303577\n",
      "Epoch 191/1000 train loss: [array(0.30357662, dtype=float32)], val loss: 0.30158090591430664\n",
      "\t partial train loss (single batch): 0.302896\n",
      "Epoch 192/1000 train loss: [array(0.30289575, dtype=float32)], val loss: 0.2977626323699951\n",
      "\t partial train loss (single batch): 0.305245\n",
      "Epoch 193/1000 train loss: [array(0.30524498, dtype=float32)], val loss: 0.3016842305660248\n",
      "\t partial train loss (single batch): 0.302383\n",
      "Epoch 194/1000 train loss: [array(0.30238283, dtype=float32)], val loss: 0.2980700433254242\n",
      "\t partial train loss (single batch): 0.300284\n",
      "Epoch 195/1000 train loss: [array(0.30028403, dtype=float32)], val loss: 0.296262264251709\n",
      "\t partial train loss (single batch): 0.298000\n",
      "Epoch 196/1000 train loss: [array(0.2979996, dtype=float32)], val loss: 0.29487326741218567\n",
      "\t partial train loss (single batch): 0.297636\n",
      "Epoch 197/1000 train loss: [array(0.2976358, dtype=float32)], val loss: 0.2954021692276001\n",
      "\t partial train loss (single batch): 0.297037\n",
      "Epoch 198/1000 train loss: [array(0.29703653, dtype=float32)], val loss: 0.29076847434043884\n",
      "\t partial train loss (single batch): 0.294217\n",
      "Epoch 199/1000 train loss: [array(0.29421663, dtype=float32)], val loss: 0.2923333942890167\n",
      "\t partial train loss (single batch): 0.295897\n",
      "Epoch 200/1000 train loss: [array(0.29589665, dtype=float32)], val loss: 0.2962111532688141\n",
      "\t partial train loss (single batch): 0.290811\n",
      "Epoch 201/1000 train loss: [array(0.29081056, dtype=float32)], val loss: 0.29304617643356323\n",
      "\t partial train loss (single batch): 0.293311\n",
      "Epoch 202/1000 train loss: [array(0.29331097, dtype=float32)], val loss: 0.2887248694896698\n",
      "\t partial train loss (single batch): 0.289454\n",
      "Epoch 203/1000 train loss: [array(0.28945398, dtype=float32)], val loss: 0.2874552309513092\n",
      "\t partial train loss (single batch): 0.291935\n",
      "Epoch 204/1000 train loss: [array(0.29193547, dtype=float32)], val loss: 0.2909679114818573\n",
      "\t partial train loss (single batch): 0.290164\n",
      "Epoch 205/1000 train loss: [array(0.29016444, dtype=float32)], val loss: 0.2874678671360016\n",
      "\t partial train loss (single batch): 0.289270\n",
      "Epoch 206/1000 train loss: [array(0.28927016, dtype=float32)], val loss: 0.2864524722099304\n",
      "\t partial train loss (single batch): 0.284314\n",
      "Epoch 207/1000 train loss: [array(0.284314, dtype=float32)], val loss: 0.28387904167175293\n",
      "\t partial train loss (single batch): 0.286436\n",
      "Epoch 208/1000 train loss: [array(0.28643563, dtype=float32)], val loss: 0.28008952736854553\n",
      "\t partial train loss (single batch): 0.284883\n",
      "Epoch 209/1000 train loss: [array(0.28488278, dtype=float32)], val loss: 0.28288841247558594\n",
      "\t partial train loss (single batch): 0.285081\n",
      "Epoch 210/1000 train loss: [array(0.28508094, dtype=float32)], val loss: 0.2812492251396179\n",
      "\t partial train loss (single batch): 0.285755\n",
      "Epoch 211/1000 train loss: [array(0.28575468, dtype=float32)], val loss: 0.28445959091186523\n",
      "\t partial train loss (single batch): 0.284219\n",
      "Epoch 212/1000 train loss: [array(0.28421924, dtype=float32)], val loss: 0.27860718965530396\n",
      "\t partial train loss (single batch): 0.282387\n",
      "Epoch 213/1000 train loss: [array(0.28238717, dtype=float32)], val loss: 0.2804069221019745\n",
      "\t partial train loss (single batch): 0.283189\n",
      "Epoch 214/1000 train loss: [array(0.28318948, dtype=float32)], val loss: 0.27903467416763306\n",
      "\t partial train loss (single batch): 0.281512\n",
      "Epoch 215/1000 train loss: [array(0.28151223, dtype=float32)], val loss: 0.27804288268089294\n",
      "\t partial train loss (single batch): 0.279314\n",
      "Epoch 216/1000 train loss: [array(0.2793142, dtype=float32)], val loss: 0.2792830467224121\n",
      "\t partial train loss (single batch): 0.279440\n",
      "Epoch 217/1000 train loss: [array(0.27944025, dtype=float32)], val loss: 0.27674514055252075\n",
      "\t partial train loss (single batch): 0.281214\n",
      "Epoch 218/1000 train loss: [array(0.28121403, dtype=float32)], val loss: 0.2774609923362732\n",
      "\t partial train loss (single batch): 0.278211\n",
      "Epoch 219/1000 train loss: [array(0.27821124, dtype=float32)], val loss: 0.27739420533180237\n",
      "\t partial train loss (single batch): 0.275434\n",
      "Epoch 220/1000 train loss: [array(0.27543435, dtype=float32)], val loss: 0.26811009645462036\n",
      "\t partial train loss (single batch): 0.278080\n",
      "Epoch 221/1000 train loss: [array(0.2780795, dtype=float32)], val loss: 0.27361559867858887\n",
      "\t partial train loss (single batch): 0.272830\n",
      "Epoch 222/1000 train loss: [array(0.27282968, dtype=float32)], val loss: 0.26993894577026367\n",
      "\t partial train loss (single batch): 0.275120\n",
      "Epoch 223/1000 train loss: [array(0.27511987, dtype=float32)], val loss: 0.27247437834739685\n",
      "\t partial train loss (single batch): 0.274484\n",
      "Epoch 224/1000 train loss: [array(0.274484, dtype=float32)], val loss: 0.272542804479599\n",
      "\t partial train loss (single batch): 0.271381\n",
      "Epoch 225/1000 train loss: [array(0.27138117, dtype=float32)], val loss: 0.27025988698005676\n",
      "\t partial train loss (single batch): 0.269944\n",
      "Epoch 226/1000 train loss: [array(0.26994407, dtype=float32)], val loss: 0.26414328813552856\n",
      "\t partial train loss (single batch): 0.267691\n",
      "Epoch 227/1000 train loss: [array(0.26769087, dtype=float32)], val loss: 0.26400238275527954\n",
      "\t partial train loss (single batch): 0.271540\n",
      "Epoch 228/1000 train loss: [array(0.27154034, dtype=float32)], val loss: 0.2682650089263916\n",
      "\t partial train loss (single batch): 0.273342\n",
      "Epoch 229/1000 train loss: [array(0.2733416, dtype=float32)], val loss: 0.264171838760376\n",
      "\t partial train loss (single batch): 0.267235\n",
      "Epoch 230/1000 train loss: [array(0.2672352, dtype=float32)], val loss: 0.26563218235969543\n",
      "\t partial train loss (single batch): 0.265660\n",
      "Epoch 231/1000 train loss: [array(0.26566043, dtype=float32)], val loss: 0.259929358959198\n",
      "\t partial train loss (single batch): 0.264875\n",
      "Epoch 232/1000 train loss: [array(0.2648746, dtype=float32)], val loss: 0.2638145089149475\n",
      "\t partial train loss (single batch): 0.267989\n",
      "Epoch 233/1000 train loss: [array(0.2679894, dtype=float32)], val loss: 0.26390206813812256\n",
      "\t partial train loss (single batch): 0.263104\n",
      "Epoch 234/1000 train loss: [array(0.26310414, dtype=float32)], val loss: 0.26124805212020874\n",
      "\t partial train loss (single batch): 0.262895\n",
      "Epoch 235/1000 train loss: [array(0.26289523, dtype=float32)], val loss: 0.2591188848018646\n",
      "\t partial train loss (single batch): 0.261299\n",
      "Epoch 236/1000 train loss: [array(0.2612988, dtype=float32)], val loss: 0.2563434839248657\n",
      "\t partial train loss (single batch): 0.266421\n",
      "Epoch 237/1000 train loss: [array(0.26642096, dtype=float32)], val loss: 0.26238909363746643\n",
      "\t partial train loss (single batch): 0.265845\n",
      "Epoch 238/1000 train loss: [array(0.26584494, dtype=float32)], val loss: 0.252691388130188\n",
      "\t partial train loss (single batch): 0.262668\n",
      "Epoch 239/1000 train loss: [array(0.26266837, dtype=float32)], val loss: 0.2581369876861572\n",
      "\t partial train loss (single batch): 0.257995\n",
      "Epoch 240/1000 train loss: [array(0.25799522, dtype=float32)], val loss: 0.2560405433177948\n",
      "\t partial train loss (single batch): 0.260599\n",
      "Epoch 241/1000 train loss: [array(0.26059866, dtype=float32)], val loss: 0.2540269196033478\n",
      "\t partial train loss (single batch): 0.256057\n",
      "Epoch 242/1000 train loss: [array(0.2560572, dtype=float32)], val loss: 0.25155726075172424\n",
      "\t partial train loss (single batch): 0.257081\n",
      "Epoch 243/1000 train loss: [array(0.25708082, dtype=float32)], val loss: 0.24989573657512665\n",
      "\t partial train loss (single batch): 0.257073\n",
      "Epoch 244/1000 train loss: [array(0.2570731, dtype=float32)], val loss: 0.2562340199947357\n",
      "\t partial train loss (single batch): 0.256130\n",
      "Epoch 245/1000 train loss: [array(0.25613013, dtype=float32)], val loss: 0.2515557110309601\n",
      "\t partial train loss (single batch): 0.257951\n",
      "Epoch 246/1000 train loss: [array(0.25795123, dtype=float32)], val loss: 0.2503158748149872\n",
      "\t partial train loss (single batch): 0.253944\n",
      "Epoch 247/1000 train loss: [array(0.25394428, dtype=float32)], val loss: 0.2475544810295105\n",
      "\t partial train loss (single batch): 0.255006\n",
      "Epoch 248/1000 train loss: [array(0.25500554, dtype=float32)], val loss: 0.25363534688949585\n",
      "\t partial train loss (single batch): 0.251572\n",
      "Epoch 249/1000 train loss: [array(0.25157228, dtype=float32)], val loss: 0.2534513771533966\n",
      "\t partial train loss (single batch): 0.247506\n",
      "Epoch 250/1000 train loss: [array(0.24750607, dtype=float32)], val loss: 0.2511207163333893\n",
      "\t partial train loss (single batch): 0.248208\n",
      "Epoch 251/1000 train loss: [array(0.24820818, dtype=float32)], val loss: 0.24882596731185913\n",
      "\t partial train loss (single batch): 0.252488\n",
      "Epoch 252/1000 train loss: [array(0.25248805, dtype=float32)], val loss: 0.2476777881383896\n",
      "\t partial train loss (single batch): 0.250206\n",
      "Epoch 253/1000 train loss: [array(0.25020623, dtype=float32)], val loss: 0.24859951436519623\n",
      "\t partial train loss (single batch): 0.249089\n",
      "Epoch 254/1000 train loss: [array(0.24908938, dtype=float32)], val loss: 0.2476087361574173\n",
      "\t partial train loss (single batch): 0.249134\n",
      "Epoch 255/1000 train loss: [array(0.2491345, dtype=float32)], val loss: 0.24371564388275146\n",
      "\t partial train loss (single batch): 0.248980\n",
      "Epoch 256/1000 train loss: [array(0.24898048, dtype=float32)], val loss: 0.24505971372127533\n",
      "\t partial train loss (single batch): 0.245032\n",
      "Epoch 257/1000 train loss: [array(0.24503236, dtype=float32)], val loss: 0.24732010066509247\n",
      "\t partial train loss (single batch): 0.245958\n",
      "Epoch 258/1000 train loss: [array(0.24595815, dtype=float32)], val loss: 0.2422434538602829\n",
      "\t partial train loss (single batch): 0.244719\n",
      "Epoch 259/1000 train loss: [array(0.24471915, dtype=float32)], val loss: 0.24185071885585785\n",
      "\t partial train loss (single batch): 0.245444\n",
      "Epoch 260/1000 train loss: [array(0.24544403, dtype=float32)], val loss: 0.24234238266944885\n",
      "\t partial train loss (single batch): 0.241454\n",
      "Epoch 261/1000 train loss: [array(0.24145356, dtype=float32)], val loss: 0.23558039963245392\n",
      "\t partial train loss (single batch): 0.240222\n",
      "Epoch 262/1000 train loss: [array(0.24022211, dtype=float32)], val loss: 0.23724567890167236\n",
      "\t partial train loss (single batch): 0.242496\n",
      "Epoch 263/1000 train loss: [array(0.24249618, dtype=float32)], val loss: 0.23796118795871735\n",
      "\t partial train loss (single batch): 0.243995\n",
      "Epoch 264/1000 train loss: [array(0.24399497, dtype=float32)], val loss: 0.23798541724681854\n",
      "\t partial train loss (single batch): 0.242973\n",
      "Epoch 265/1000 train loss: [array(0.24297267, dtype=float32)], val loss: 0.2370024174451828\n",
      "\t partial train loss (single batch): 0.242213\n",
      "Epoch 266/1000 train loss: [array(0.24221265, dtype=float32)], val loss: 0.23404332995414734\n",
      "\t partial train loss (single batch): 0.238469\n",
      "Epoch 267/1000 train loss: [array(0.23846887, dtype=float32)], val loss: 0.23503486812114716\n",
      "\t partial train loss (single batch): 0.240064\n",
      "Epoch 268/1000 train loss: [array(0.24006447, dtype=float32)], val loss: 0.23896242678165436\n",
      "\t partial train loss (single batch): 0.237870\n",
      "Epoch 269/1000 train loss: [array(0.23787032, dtype=float32)], val loss: 0.23545023798942566\n",
      "\t partial train loss (single batch): 0.236060\n",
      "Epoch 270/1000 train loss: [array(0.2360595, dtype=float32)], val loss: 0.23648667335510254\n",
      "\t partial train loss (single batch): 0.238339\n",
      "Epoch 271/1000 train loss: [array(0.23833852, dtype=float32)], val loss: 0.2315802276134491\n",
      "\t partial train loss (single batch): 0.237785\n",
      "Epoch 272/1000 train loss: [array(0.2377851, dtype=float32)], val loss: 0.23125030100345612\n",
      "\t partial train loss (single batch): 0.234889\n",
      "Epoch 273/1000 train loss: [array(0.2348887, dtype=float32)], val loss: 0.23267625272274017\n",
      "\t partial train loss (single batch): 0.236183\n",
      "Epoch 274/1000 train loss: [array(0.23618312, dtype=float32)], val loss: 0.2324901819229126\n",
      "\t partial train loss (single batch): 0.233654\n",
      "Epoch 275/1000 train loss: [array(0.2336535, dtype=float32)], val loss: 0.2354556918144226\n",
      "\t partial train loss (single batch): 0.232177\n",
      "Epoch 276/1000 train loss: [array(0.2321774, dtype=float32)], val loss: 0.2307727187871933\n",
      "\t partial train loss (single batch): 0.231174\n",
      "Epoch 277/1000 train loss: [array(0.23117365, dtype=float32)], val loss: 0.23243072628974915\n",
      "\t partial train loss (single batch): 0.232175\n",
      "Epoch 278/1000 train loss: [array(0.23217462, dtype=float32)], val loss: 0.22851765155792236\n",
      "\t partial train loss (single batch): 0.233748\n",
      "Epoch 279/1000 train loss: [array(0.23374775, dtype=float32)], val loss: 0.22359170019626617\n",
      "\t partial train loss (single batch): 0.232771\n",
      "Epoch 280/1000 train loss: [array(0.23277085, dtype=float32)], val loss: 0.23335424065589905\n",
      "\t partial train loss (single batch): 0.229865\n",
      "Epoch 281/1000 train loss: [array(0.2298654, dtype=float32)], val loss: 0.22590641677379608\n",
      "\t partial train loss (single batch): 0.230177\n",
      "Epoch 282/1000 train loss: [array(0.23017722, dtype=float32)], val loss: 0.2272164225578308\n",
      "\t partial train loss (single batch): 0.231074\n",
      "Epoch 283/1000 train loss: [array(0.23107412, dtype=float32)], val loss: 0.22518029808998108\n",
      "\t partial train loss (single batch): 0.227546\n",
      "Epoch 284/1000 train loss: [array(0.22754584, dtype=float32)], val loss: 0.2281285524368286\n",
      "\t partial train loss (single batch): 0.225902\n",
      "Epoch 285/1000 train loss: [array(0.22590165, dtype=float32)], val loss: 0.22482281923294067\n",
      "\t partial train loss (single batch): 0.226475\n",
      "Epoch 286/1000 train loss: [array(0.22647545, dtype=float32)], val loss: 0.22364220023155212\n",
      "\t partial train loss (single batch): 0.227470\n",
      "Epoch 287/1000 train loss: [array(0.22747003, dtype=float32)], val loss: 0.22214321792125702\n",
      "\t partial train loss (single batch): 0.227407\n",
      "Epoch 288/1000 train loss: [array(0.22740686, dtype=float32)], val loss: 0.22522921860218048\n",
      "\t partial train loss (single batch): 0.225674\n",
      "Epoch 289/1000 train loss: [array(0.22567385, dtype=float32)], val loss: 0.21878637373447418\n",
      "\t partial train loss (single batch): 0.226071\n",
      "Epoch 290/1000 train loss: [array(0.22607058, dtype=float32)], val loss: 0.22485242784023285\n",
      "\t partial train loss (single batch): 0.222989\n",
      "Epoch 291/1000 train loss: [array(0.22298904, dtype=float32)], val loss: 0.22230961918830872\n",
      "\t partial train loss (single batch): 0.222062\n",
      "Epoch 292/1000 train loss: [array(0.22206199, dtype=float32)], val loss: 0.22310885787010193\n",
      "\t partial train loss (single batch): 0.220532\n",
      "Epoch 293/1000 train loss: [array(0.22053209, dtype=float32)], val loss: 0.22042261064052582\n",
      "\t partial train loss (single batch): 0.224396\n",
      "Epoch 294/1000 train loss: [array(0.2243957, dtype=float32)], val loss: 0.22096900641918182\n",
      "\t partial train loss (single batch): 0.221620\n",
      "Epoch 295/1000 train loss: [array(0.22162035, dtype=float32)], val loss: 0.22293153405189514\n",
      "\t partial train loss (single batch): 0.219287\n",
      "Epoch 296/1000 train loss: [array(0.21928702, dtype=float32)], val loss: 0.2160854935646057\n",
      "\t partial train loss (single batch): 0.218866\n",
      "Epoch 297/1000 train loss: [array(0.2188665, dtype=float32)], val loss: 0.22164054214954376\n",
      "\t partial train loss (single batch): 0.222228\n",
      "Epoch 298/1000 train loss: [array(0.22222786, dtype=float32)], val loss: 0.21610665321350098\n",
      "\t partial train loss (single batch): 0.222264\n",
      "Epoch 299/1000 train loss: [array(0.22226399, dtype=float32)], val loss: 0.2151656299829483\n",
      "\t partial train loss (single batch): 0.221456\n",
      "Epoch 300/1000 train loss: [array(0.22145638, dtype=float32)], val loss: 0.2143382877111435\n",
      "\t partial train loss (single batch): 0.218236\n",
      "Epoch 301/1000 train loss: [array(0.21823612, dtype=float32)], val loss: 0.22082705795764923\n",
      "\t partial train loss (single batch): 0.218530\n",
      "Epoch 302/1000 train loss: [array(0.21852976, dtype=float32)], val loss: 0.21606484055519104\n",
      "\t partial train loss (single batch): 0.216397\n",
      "Epoch 303/1000 train loss: [array(0.21639699, dtype=float32)], val loss: 0.217196524143219\n",
      "\t partial train loss (single batch): 0.216636\n",
      "Epoch 304/1000 train loss: [array(0.2166359, dtype=float32)], val loss: 0.2162499576807022\n",
      "\t partial train loss (single batch): 0.215445\n",
      "Epoch 305/1000 train loss: [array(0.21544531, dtype=float32)], val loss: 0.21359474956989288\n",
      "\t partial train loss (single batch): 0.217244\n",
      "Epoch 306/1000 train loss: [array(0.2172437, dtype=float32)], val loss: 0.21629980206489563\n",
      "\t partial train loss (single batch): 0.213362\n",
      "Epoch 307/1000 train loss: [array(0.2133615, dtype=float32)], val loss: 0.21431724727153778\n",
      "\t partial train loss (single batch): 0.213069\n",
      "Epoch 308/1000 train loss: [array(0.21306859, dtype=float32)], val loss: 0.21042205393314362\n",
      "\t partial train loss (single batch): 0.215489\n",
      "Epoch 309/1000 train loss: [array(0.21548882, dtype=float32)], val loss: 0.21332697570323944\n",
      "\t partial train loss (single batch): 0.213709\n",
      "Epoch 310/1000 train loss: [array(0.21370906, dtype=float32)], val loss: 0.21801933646202087\n",
      "\t partial train loss (single batch): 0.215221\n",
      "Epoch 311/1000 train loss: [array(0.21522127, dtype=float32)], val loss: 0.20986200869083405\n",
      "\t partial train loss (single batch): 0.208701\n",
      "Epoch 312/1000 train loss: [array(0.20870082, dtype=float32)], val loss: 0.2085808366537094\n",
      "\t partial train loss (single batch): 0.214320\n",
      "Epoch 313/1000 train loss: [array(0.21432026, dtype=float32)], val loss: 0.20962010324001312\n",
      "\t partial train loss (single batch): 0.211842\n",
      "Epoch 314/1000 train loss: [array(0.21184224, dtype=float32)], val loss: 0.21127304434776306\n",
      "\t partial train loss (single batch): 0.210383\n",
      "Epoch 315/1000 train loss: [array(0.21038257, dtype=float32)], val loss: 0.21194948256015778\n",
      "\t partial train loss (single batch): 0.212380\n",
      "Epoch 316/1000 train loss: [array(0.21238023, dtype=float32)], val loss: 0.20592351257801056\n",
      "\t partial train loss (single batch): 0.209561\n",
      "Epoch 317/1000 train loss: [array(0.20956083, dtype=float32)], val loss: 0.20900721848011017\n",
      "\t partial train loss (single batch): 0.211345\n",
      "Epoch 318/1000 train loss: [array(0.21134454, dtype=float32)], val loss: 0.20542773604393005\n",
      "\t partial train loss (single batch): 0.212124\n",
      "Epoch 319/1000 train loss: [array(0.2121236, dtype=float32)], val loss: 0.2100108414888382\n",
      "\t partial train loss (single batch): 0.207012\n",
      "Epoch 320/1000 train loss: [array(0.20701224, dtype=float32)], val loss: 0.20897042751312256\n",
      "\t partial train loss (single batch): 0.210563\n",
      "Epoch 321/1000 train loss: [array(0.21056303, dtype=float32)], val loss: 0.20442518591880798\n",
      "\t partial train loss (single batch): 0.211400\n",
      "Epoch 322/1000 train loss: [array(0.21139985, dtype=float32)], val loss: 0.20824924111366272\n",
      "\t partial train loss (single batch): 0.206277\n",
      "Epoch 323/1000 train loss: [array(0.2062773, dtype=float32)], val loss: 0.21011412143707275\n",
      "\t partial train loss (single batch): 0.206730\n",
      "Epoch 324/1000 train loss: [array(0.20673014, dtype=float32)], val loss: 0.2063671350479126\n",
      "\t partial train loss (single batch): 0.201744\n",
      "Epoch 325/1000 train loss: [array(0.20174381, dtype=float32)], val loss: 0.20327723026275635\n",
      "\t partial train loss (single batch): 0.206499\n",
      "Epoch 326/1000 train loss: [array(0.20649858, dtype=float32)], val loss: 0.20197263360023499\n",
      "\t partial train loss (single batch): 0.207981\n",
      "Epoch 327/1000 train loss: [array(0.20798138, dtype=float32)], val loss: 0.20774343609809875\n",
      "\t partial train loss (single batch): 0.207790\n",
      "Epoch 328/1000 train loss: [array(0.20779036, dtype=float32)], val loss: 0.2030690610408783\n",
      "\t partial train loss (single batch): 0.205545\n",
      "Epoch 329/1000 train loss: [array(0.20554462, dtype=float32)], val loss: 0.2034320831298828\n",
      "\t partial train loss (single batch): 0.204029\n",
      "Epoch 330/1000 train loss: [array(0.2040292, dtype=float32)], val loss: 0.19653746485710144\n",
      "\t partial train loss (single batch): 0.207085\n",
      "Epoch 331/1000 train loss: [array(0.20708525, dtype=float32)], val loss: 0.20412078499794006\n",
      "\t partial train loss (single batch): 0.204338\n",
      "Epoch 332/1000 train loss: [array(0.20433784, dtype=float32)], val loss: 0.2032424360513687\n",
      "\t partial train loss (single batch): 0.196341\n",
      "Epoch 333/1000 train loss: [array(0.19634147, dtype=float32)], val loss: 0.20289471745491028\n",
      "\t partial train loss (single batch): 0.201907\n",
      "Epoch 334/1000 train loss: [array(0.20190729, dtype=float32)], val loss: 0.20120838284492493\n",
      "\t partial train loss (single batch): 0.203135\n",
      "Epoch 335/1000 train loss: [array(0.20313504, dtype=float32)], val loss: 0.20194388926029205\n",
      "\t partial train loss (single batch): 0.196808\n",
      "Epoch 336/1000 train loss: [array(0.19680794, dtype=float32)], val loss: 0.1996581256389618\n",
      "\t partial train loss (single batch): 0.203500\n",
      "Epoch 337/1000 train loss: [array(0.2035002, dtype=float32)], val loss: 0.19419890642166138\n",
      "\t partial train loss (single batch): 0.201939\n",
      "Epoch 338/1000 train loss: [array(0.20193937, dtype=float32)], val loss: 0.1955357939004898\n",
      "\t partial train loss (single batch): 0.197974\n",
      "Epoch 339/1000 train loss: [array(0.19797383, dtype=float32)], val loss: 0.19811546802520752\n",
      "\t partial train loss (single batch): 0.195382\n",
      "Epoch 340/1000 train loss: [array(0.19538191, dtype=float32)], val loss: 0.19753989577293396\n",
      "\t partial train loss (single batch): 0.197978\n",
      "Epoch 341/1000 train loss: [array(0.1979781, dtype=float32)], val loss: 0.19928476214408875\n",
      "\t partial train loss (single batch): 0.198574\n",
      "Epoch 342/1000 train loss: [array(0.19857356, dtype=float32)], val loss: 0.1979886293411255\n",
      "\t partial train loss (single batch): 0.199855\n",
      "Epoch 343/1000 train loss: [array(0.19985542, dtype=float32)], val loss: 0.19727279245853424\n",
      "\t partial train loss (single batch): 0.195016\n",
      "Epoch 344/1000 train loss: [array(0.1950164, dtype=float32)], val loss: 0.19370423257350922\n",
      "\t partial train loss (single batch): 0.194105\n",
      "Epoch 345/1000 train loss: [array(0.19410472, dtype=float32)], val loss: 0.19121119379997253\n",
      "\t partial train loss (single batch): 0.196705\n",
      "Epoch 346/1000 train loss: [array(0.19670545, dtype=float32)], val loss: 0.19489917159080505\n",
      "\t partial train loss (single batch): 0.199343\n",
      "Epoch 347/1000 train loss: [array(0.19934331, dtype=float32)], val loss: 0.19877706468105316\n",
      "\t partial train loss (single batch): 0.194817\n",
      "Epoch 348/1000 train loss: [array(0.19481666, dtype=float32)], val loss: 0.1965133249759674\n",
      "\t partial train loss (single batch): 0.197787\n",
      "Epoch 349/1000 train loss: [array(0.19778657, dtype=float32)], val loss: 0.19847209751605988\n",
      "\t partial train loss (single batch): 0.192916\n",
      "Epoch 350/1000 train loss: [array(0.1929158, dtype=float32)], val loss: 0.19395260512828827\n",
      "\t partial train loss (single batch): 0.192482\n",
      "Epoch 351/1000 train loss: [array(0.19248174, dtype=float32)], val loss: 0.19073624908924103\n",
      "\t partial train loss (single batch): 0.192079\n",
      "Epoch 352/1000 train loss: [array(0.19207859, dtype=float32)], val loss: 0.18814094364643097\n",
      "\t partial train loss (single batch): 0.193604\n",
      "Epoch 353/1000 train loss: [array(0.19360408, dtype=float32)], val loss: 0.19124449789524078\n",
      "\t partial train loss (single batch): 0.190861\n",
      "Epoch 354/1000 train loss: [array(0.19086102, dtype=float32)], val loss: 0.1932467818260193\n",
      "\t partial train loss (single batch): 0.191439\n",
      "Epoch 355/1000 train loss: [array(0.19143918, dtype=float32)], val loss: 0.1881222426891327\n",
      "\t partial train loss (single batch): 0.194790\n",
      "Epoch 356/1000 train loss: [array(0.19478978, dtype=float32)], val loss: 0.19257374107837677\n",
      "\t partial train loss (single batch): 0.192685\n",
      "Epoch 357/1000 train loss: [array(0.1926847, dtype=float32)], val loss: 0.19150929152965546\n",
      "\t partial train loss (single batch): 0.190620\n",
      "Epoch 358/1000 train loss: [array(0.19062027, dtype=float32)], val loss: 0.1918327361345291\n",
      "\t partial train loss (single batch): 0.191988\n",
      "Epoch 359/1000 train loss: [array(0.1919882, dtype=float32)], val loss: 0.19023330509662628\n",
      "\t partial train loss (single batch): 0.186701\n",
      "Epoch 360/1000 train loss: [array(0.18670097, dtype=float32)], val loss: 0.1866069883108139\n",
      "\t partial train loss (single batch): 0.194751\n",
      "Epoch 361/1000 train loss: [array(0.19475102, dtype=float32)], val loss: 0.18752989172935486\n",
      "\t partial train loss (single batch): 0.184256\n",
      "Epoch 362/1000 train loss: [array(0.1842559, dtype=float32)], val loss: 0.19043241441249847\n",
      "\t partial train loss (single batch): 0.189387\n",
      "Epoch 363/1000 train loss: [array(0.18938743, dtype=float32)], val loss: 0.19029666483402252\n",
      "\t partial train loss (single batch): 0.186989\n",
      "Epoch 364/1000 train loss: [array(0.18698853, dtype=float32)], val loss: 0.18809819221496582\n",
      "\t partial train loss (single batch): 0.188536\n",
      "Epoch 365/1000 train loss: [array(0.18853614, dtype=float32)], val loss: 0.19276554882526398\n",
      "\t partial train loss (single batch): 0.185214\n",
      "Epoch 366/1000 train loss: [array(0.18521442, dtype=float32)], val loss: 0.1845848113298416\n",
      "\t partial train loss (single batch): 0.188077\n",
      "Epoch 367/1000 train loss: [array(0.18807665, dtype=float32)], val loss: 0.18502280116081238\n",
      "\t partial train loss (single batch): 0.189706\n",
      "Epoch 368/1000 train loss: [array(0.189706, dtype=float32)], val loss: 0.18553581833839417\n",
      "\t partial train loss (single batch): 0.190406\n",
      "Epoch 369/1000 train loss: [array(0.19040592, dtype=float32)], val loss: 0.18661363422870636\n",
      "\t partial train loss (single batch): 0.184253\n",
      "Epoch 370/1000 train loss: [array(0.18425272, dtype=float32)], val loss: 0.1901092380285263\n",
      "\t partial train loss (single batch): 0.185302\n",
      "Epoch 371/1000 train loss: [array(0.18530157, dtype=float32)], val loss: 0.18308447301387787\n",
      "\t partial train loss (single batch): 0.185611\n",
      "Epoch 372/1000 train loss: [array(0.18561067, dtype=float32)], val loss: 0.1891307681798935\n",
      "\t partial train loss (single batch): 0.182420\n",
      "Epoch 373/1000 train loss: [array(0.18242049, dtype=float32)], val loss: 0.18986594676971436\n",
      "\t partial train loss (single batch): 0.186102\n",
      "Epoch 374/1000 train loss: [array(0.18610232, dtype=float32)], val loss: 0.185256689786911\n",
      "\t partial train loss (single batch): 0.184009\n",
      "Epoch 375/1000 train loss: [array(0.18400948, dtype=float32)], val loss: 0.18115796148777008\n",
      "\t partial train loss (single batch): 0.181650\n",
      "Epoch 376/1000 train loss: [array(0.1816502, dtype=float32)], val loss: 0.1793707013130188\n",
      "\t partial train loss (single batch): 0.180365\n",
      "Epoch 377/1000 train loss: [array(0.18036473, dtype=float32)], val loss: 0.18120327591896057\n",
      "\t partial train loss (single batch): 0.182723\n",
      "Epoch 378/1000 train loss: [array(0.1827226, dtype=float32)], val loss: 0.18218982219696045\n",
      "\t partial train loss (single batch): 0.188019\n",
      "Epoch 379/1000 train loss: [array(0.18801935, dtype=float32)], val loss: 0.18080240488052368\n",
      "\t partial train loss (single batch): 0.183411\n",
      "Epoch 380/1000 train loss: [array(0.1834108, dtype=float32)], val loss: 0.17933832108974457\n",
      "\t partial train loss (single batch): 0.179861\n",
      "Epoch 381/1000 train loss: [array(0.17986101, dtype=float32)], val loss: 0.18057085573673248\n",
      "\t partial train loss (single batch): 0.180425\n",
      "Epoch 382/1000 train loss: [array(0.18042505, dtype=float32)], val loss: 0.18367044627666473\n",
      "\t partial train loss (single batch): 0.179604\n",
      "Epoch 383/1000 train loss: [array(0.17960417, dtype=float32)], val loss: 0.17748914659023285\n",
      "\t partial train loss (single batch): 0.180979\n",
      "Epoch 384/1000 train loss: [array(0.1809793, dtype=float32)], val loss: 0.1806032657623291\n",
      "\t partial train loss (single batch): 0.181893\n",
      "Epoch 385/1000 train loss: [array(0.18189321, dtype=float32)], val loss: 0.18311932682991028\n",
      "\t partial train loss (single batch): 0.180411\n",
      "Epoch 386/1000 train loss: [array(0.1804112, dtype=float32)], val loss: 0.17926417291164398\n",
      "\t partial train loss (single batch): 0.176106\n",
      "Epoch 387/1000 train loss: [array(0.17610554, dtype=float32)], val loss: 0.18132533133029938\n",
      "\t partial train loss (single batch): 0.183474\n",
      "Epoch 388/1000 train loss: [array(0.1834736, dtype=float32)], val loss: 0.179409921169281\n",
      "\t partial train loss (single batch): 0.178046\n",
      "Epoch 389/1000 train loss: [array(0.17804572, dtype=float32)], val loss: 0.17717747390270233\n",
      "\t partial train loss (single batch): 0.178810\n",
      "Epoch 390/1000 train loss: [array(0.17880975, dtype=float32)], val loss: 0.1781097650527954\n",
      "\t partial train loss (single batch): 0.177317\n",
      "Epoch 391/1000 train loss: [array(0.17731738, dtype=float32)], val loss: 0.17896386981010437\n",
      "\t partial train loss (single batch): 0.176273\n",
      "Epoch 392/1000 train loss: [array(0.17627324, dtype=float32)], val loss: 0.18191154301166534\n",
      "\t partial train loss (single batch): 0.180299\n",
      "Epoch 393/1000 train loss: [array(0.18029895, dtype=float32)], val loss: 0.17781506478786469\n",
      "\t partial train loss (single batch): 0.178984\n",
      "Epoch 394/1000 train loss: [array(0.17898384, dtype=float32)], val loss: 0.17656974494457245\n",
      "\t partial train loss (single batch): 0.174194\n",
      "Epoch 395/1000 train loss: [array(0.17419404, dtype=float32)], val loss: 0.17318123579025269\n",
      "\t partial train loss (single batch): 0.177056\n",
      "Epoch 396/1000 train loss: [array(0.17705636, dtype=float32)], val loss: 0.17288203537464142\n",
      "\t partial train loss (single batch): 0.173556\n",
      "Epoch 397/1000 train loss: [array(0.17355643, dtype=float32)], val loss: 0.17283174395561218\n",
      "\t partial train loss (single batch): 0.175308\n",
      "Epoch 398/1000 train loss: [array(0.17530766, dtype=float32)], val loss: 0.1731356382369995\n",
      "\t partial train loss (single batch): 0.177766\n",
      "Epoch 399/1000 train loss: [array(0.17776589, dtype=float32)], val loss: 0.17664840817451477\n",
      "\t partial train loss (single batch): 0.176437\n",
      "Epoch 400/1000 train loss: [array(0.17643695, dtype=float32)], val loss: 0.17373476922512054\n",
      "\t partial train loss (single batch): 0.171481\n",
      "Epoch 401/1000 train loss: [array(0.17148139, dtype=float32)], val loss: 0.1739627867937088\n",
      "\t partial train loss (single batch): 0.176179\n",
      "Epoch 402/1000 train loss: [array(0.176179, dtype=float32)], val loss: 0.17350414395332336\n",
      "\t partial train loss (single batch): 0.174184\n",
      "Epoch 403/1000 train loss: [array(0.17418425, dtype=float32)], val loss: 0.17152677476406097\n",
      "\t partial train loss (single batch): 0.173586\n",
      "Epoch 404/1000 train loss: [array(0.17358565, dtype=float32)], val loss: 0.1707136631011963\n",
      "\t partial train loss (single batch): 0.176230\n",
      "Epoch 405/1000 train loss: [array(0.17622972, dtype=float32)], val loss: 0.17284925282001495\n",
      "\t partial train loss (single batch): 0.172628\n",
      "Epoch 406/1000 train loss: [array(0.17262816, dtype=float32)], val loss: 0.17134343087673187\n",
      "\t partial train loss (single batch): 0.177001\n",
      "Epoch 407/1000 train loss: [array(0.17700088, dtype=float32)], val loss: 0.17453263700008392\n",
      "\t partial train loss (single batch): 0.174972\n",
      "Epoch 408/1000 train loss: [array(0.17497203, dtype=float32)], val loss: 0.17264455556869507\n",
      "\t partial train loss (single batch): 0.172293\n",
      "Epoch 409/1000 train loss: [array(0.17229271, dtype=float32)], val loss: 0.17569701373577118\n",
      "\t partial train loss (single batch): 0.173931\n",
      "Epoch 410/1000 train loss: [array(0.17393076, dtype=float32)], val loss: 0.1722365915775299\n",
      "\t partial train loss (single batch): 0.174181\n",
      "Epoch 411/1000 train loss: [array(0.1741811, dtype=float32)], val loss: 0.1721658557653427\n",
      "\t partial train loss (single batch): 0.173384\n",
      "Epoch 412/1000 train loss: [array(0.1733841, dtype=float32)], val loss: 0.1721508800983429\n",
      "\t partial train loss (single batch): 0.170259\n",
      "Epoch 413/1000 train loss: [array(0.17025921, dtype=float32)], val loss: 0.17063401639461517\n",
      "\t partial train loss (single batch): 0.172662\n",
      "Epoch 414/1000 train loss: [array(0.17266157, dtype=float32)], val loss: 0.17039737105369568\n",
      "\t partial train loss (single batch): 0.172478\n",
      "Epoch 415/1000 train loss: [array(0.17247765, dtype=float32)], val loss: 0.16887374222278595\n",
      "\t partial train loss (single batch): 0.175808\n",
      "Epoch 416/1000 train loss: [array(0.17580801, dtype=float32)], val loss: 0.17456074059009552\n",
      "\t partial train loss (single batch): 0.170679\n",
      "Epoch 417/1000 train loss: [array(0.17067873, dtype=float32)], val loss: 0.16432373225688934\n",
      "\t partial train loss (single batch): 0.168655\n",
      "Epoch 418/1000 train loss: [array(0.16865525, dtype=float32)], val loss: 0.17015902698040009\n",
      "\t partial train loss (single batch): 0.168818\n",
      "Epoch 419/1000 train loss: [array(0.16881776, dtype=float32)], val loss: 0.16505789756774902\n",
      "\t partial train loss (single batch): 0.168271\n",
      "Epoch 420/1000 train loss: [array(0.1682708, dtype=float32)], val loss: 0.1673094481229782\n",
      "\t partial train loss (single batch): 0.167722\n",
      "Epoch 421/1000 train loss: [array(0.16772234, dtype=float32)], val loss: 0.16620351374149323\n",
      "\t partial train loss (single batch): 0.168860\n",
      "Epoch 422/1000 train loss: [array(0.16885959, dtype=float32)], val loss: 0.1672379970550537\n",
      "\t partial train loss (single batch): 0.170412\n",
      "Epoch 423/1000 train loss: [array(0.17041218, dtype=float32)], val loss: 0.16623343527317047\n",
      "\t partial train loss (single batch): 0.169001\n",
      "Epoch 424/1000 train loss: [array(0.16900058, dtype=float32)], val loss: 0.16858501732349396\n",
      "\t partial train loss (single batch): 0.166146\n",
      "Epoch 425/1000 train loss: [array(0.16614579, dtype=float32)], val loss: 0.16072916984558105\n",
      "\t partial train loss (single batch): 0.167872\n",
      "Epoch 426/1000 train loss: [array(0.16787231, dtype=float32)], val loss: 0.16613392531871796\n",
      "\t partial train loss (single batch): 0.167729\n",
      "Epoch 427/1000 train loss: [array(0.167729, dtype=float32)], val loss: 0.1703219711780548\n",
      "\t partial train loss (single batch): 0.168486\n",
      "Epoch 428/1000 train loss: [array(0.16848628, dtype=float32)], val loss: 0.16229237616062164\n",
      "\t partial train loss (single batch): 0.168933\n",
      "Epoch 429/1000 train loss: [array(0.16893312, dtype=float32)], val loss: 0.16866163909435272\n",
      "\t partial train loss (single batch): 0.167083\n",
      "Epoch 430/1000 train loss: [array(0.16708253, dtype=float32)], val loss: 0.1704479455947876\n",
      "\t partial train loss (single batch): 0.171179\n",
      "Epoch 431/1000 train loss: [array(0.17117897, dtype=float32)], val loss: 0.16216789186000824\n",
      "\t partial train loss (single batch): 0.164282\n",
      "Epoch 432/1000 train loss: [array(0.16428159, dtype=float32)], val loss: 0.16927647590637207\n",
      "\t partial train loss (single batch): 0.166037\n",
      "Epoch 433/1000 train loss: [array(0.16603726, dtype=float32)], val loss: 0.16815687716007233\n",
      "\t partial train loss (single batch): 0.166318\n",
      "Epoch 434/1000 train loss: [array(0.16631834, dtype=float32)], val loss: 0.1699821650981903\n",
      "\t partial train loss (single batch): 0.163771\n",
      "Epoch 435/1000 train loss: [array(0.16377056, dtype=float32)], val loss: 0.16638395190238953\n",
      "\t partial train loss (single batch): 0.162270\n",
      "Epoch 436/1000 train loss: [array(0.16227041, dtype=float32)], val loss: 0.16634513437747955\n",
      "\t partial train loss (single batch): 0.165237\n",
      "Epoch 437/1000 train loss: [array(0.16523731, dtype=float32)], val loss: 0.15961089730262756\n",
      "\t partial train loss (single batch): 0.168359\n",
      "Epoch 438/1000 train loss: [array(0.16835901, dtype=float32)], val loss: 0.16270387172698975\n",
      "\t partial train loss (single batch): 0.166110\n",
      "Epoch 439/1000 train loss: [array(0.16610993, dtype=float32)], val loss: 0.1705796718597412\n",
      "\t partial train loss (single batch): 0.163381\n",
      "Epoch 440/1000 train loss: [array(0.16338111, dtype=float32)], val loss: 0.15912173688411713\n",
      "\t partial train loss (single batch): 0.161482\n",
      "Epoch 441/1000 train loss: [array(0.16148247, dtype=float32)], val loss: 0.16903872787952423\n",
      "\t partial train loss (single batch): 0.165236\n",
      "Epoch 442/1000 train loss: [array(0.1652361, dtype=float32)], val loss: 0.16479983925819397\n",
      "\t partial train loss (single batch): 0.164335\n",
      "Epoch 443/1000 train loss: [array(0.16433531, dtype=float32)], val loss: 0.16267308592796326\n",
      "\t partial train loss (single batch): 0.164994\n",
      "Epoch 444/1000 train loss: [array(0.16499428, dtype=float32)], val loss: 0.1632278710603714\n",
      "\t partial train loss (single batch): 0.163556\n",
      "Epoch 445/1000 train loss: [array(0.16355564, dtype=float32)], val loss: 0.16580861806869507\n",
      "\t partial train loss (single batch): 0.163370\n",
      "Epoch 446/1000 train loss: [array(0.16336986, dtype=float32)], val loss: 0.15992878377437592\n",
      "\t partial train loss (single batch): 0.164431\n",
      "Epoch 447/1000 train loss: [array(0.164431, dtype=float32)], val loss: 0.16040924191474915\n",
      "\t partial train loss (single batch): 0.165577\n",
      "Epoch 448/1000 train loss: [array(0.16557683, dtype=float32)], val loss: 0.16192781925201416\n",
      "\t partial train loss (single batch): 0.166108\n",
      "Epoch 449/1000 train loss: [array(0.16610812, dtype=float32)], val loss: 0.16026946902275085\n",
      "\t partial train loss (single batch): 0.159907\n",
      "Epoch 450/1000 train loss: [array(0.15990657, dtype=float32)], val loss: 0.16292640566825867\n",
      "\t partial train loss (single batch): 0.163657\n",
      "Epoch 451/1000 train loss: [array(0.16365668, dtype=float32)], val loss: 0.1604381501674652\n",
      "\t partial train loss (single batch): 0.161953\n",
      "Epoch 452/1000 train loss: [array(0.1619533, dtype=float32)], val loss: 0.1585785448551178\n",
      "\t partial train loss (single batch): 0.164041\n",
      "Epoch 453/1000 train loss: [array(0.16404141, dtype=float32)], val loss: 0.15865753591060638\n",
      "\t partial train loss (single batch): 0.157899\n",
      "Epoch 454/1000 train loss: [array(0.15789871, dtype=float32)], val loss: 0.15995560586452484\n",
      "\t partial train loss (single batch): 0.164704\n",
      "Epoch 455/1000 train loss: [array(0.16470428, dtype=float32)], val loss: 0.16133958101272583\n",
      "\t partial train loss (single batch): 0.159381\n",
      "Epoch 456/1000 train loss: [array(0.15938087, dtype=float32)], val loss: 0.15911981463432312\n",
      "\t partial train loss (single batch): 0.161087\n",
      "Epoch 457/1000 train loss: [array(0.16108678, dtype=float32)], val loss: 0.16023249924182892\n",
      "\t partial train loss (single batch): 0.158697\n",
      "Epoch 458/1000 train loss: [array(0.15869749, dtype=float32)], val loss: 0.15947826206684113\n",
      "\t partial train loss (single batch): 0.162396\n",
      "Epoch 459/1000 train loss: [array(0.1623963, dtype=float32)], val loss: 0.15938791632652283\n",
      "\t partial train loss (single batch): 0.158318\n",
      "Epoch 460/1000 train loss: [array(0.15831795, dtype=float32)], val loss: 0.16142521798610687\n",
      "\t partial train loss (single batch): 0.159613\n",
      "Epoch 461/1000 train loss: [array(0.15961306, dtype=float32)], val loss: 0.15818694233894348\n",
      "\t partial train loss (single batch): 0.160225\n",
      "Epoch 462/1000 train loss: [array(0.16022463, dtype=float32)], val loss: 0.15668077766895294\n",
      "\t partial train loss (single batch): 0.151342\n",
      "Epoch 463/1000 train loss: [array(0.15134187, dtype=float32)], val loss: 0.1574988067150116\n",
      "\t partial train loss (single batch): 0.160158\n",
      "Epoch 464/1000 train loss: [array(0.16015817, dtype=float32)], val loss: 0.15912900865077972\n",
      "\t partial train loss (single batch): 0.158314\n",
      "Epoch 465/1000 train loss: [array(0.15831447, dtype=float32)], val loss: 0.15906108915805817\n",
      "\t partial train loss (single batch): 0.159619\n",
      "Epoch 466/1000 train loss: [array(0.15961891, dtype=float32)], val loss: 0.16323482990264893\n",
      "\t partial train loss (single batch): 0.160750\n",
      "Epoch 467/1000 train loss: [array(0.16074954, dtype=float32)], val loss: 0.15325303375720978\n",
      "\t partial train loss (single batch): 0.156100\n",
      "Epoch 468/1000 train loss: [array(0.15609962, dtype=float32)], val loss: 0.15402445197105408\n",
      "\t partial train loss (single batch): 0.156252\n",
      "Epoch 469/1000 train loss: [array(0.1562524, dtype=float32)], val loss: 0.1575687825679779\n",
      "\t partial train loss (single batch): 0.157795\n",
      "Epoch 470/1000 train loss: [array(0.15779522, dtype=float32)], val loss: 0.1550935059785843\n",
      "\t partial train loss (single batch): 0.153582\n",
      "Epoch 471/1000 train loss: [array(0.15358193, dtype=float32)], val loss: 0.15566486120224\n",
      "\t partial train loss (single batch): 0.156616\n",
      "Epoch 472/1000 train loss: [array(0.15661608, dtype=float32)], val loss: 0.15735134482383728\n",
      "\t partial train loss (single batch): 0.156203\n",
      "Epoch 473/1000 train loss: [array(0.15620303, dtype=float32)], val loss: 0.15472912788391113\n",
      "\t partial train loss (single batch): 0.156540\n",
      "Epoch 474/1000 train loss: [array(0.15653978, dtype=float32)], val loss: 0.15277308225631714\n",
      "\t partial train loss (single batch): 0.154514\n",
      "Epoch 475/1000 train loss: [array(0.15451372, dtype=float32)], val loss: 0.15825067460536957\n",
      "\t partial train loss (single batch): 0.156697\n",
      "Epoch 476/1000 train loss: [array(0.15669706, dtype=float32)], val loss: 0.15600572526454926\n",
      "\t partial train loss (single batch): 0.154588\n",
      "Epoch 477/1000 train loss: [array(0.15458804, dtype=float32)], val loss: 0.15484488010406494\n",
      "\t partial train loss (single batch): 0.152599\n",
      "Epoch 478/1000 train loss: [array(0.15259854, dtype=float32)], val loss: 0.1514773964881897\n",
      "\t partial train loss (single batch): 0.158110\n",
      "Epoch 479/1000 train loss: [array(0.15810996, dtype=float32)], val loss: 0.1566382795572281\n",
      "\t partial train loss (single batch): 0.157129\n",
      "Epoch 480/1000 train loss: [array(0.15712896, dtype=float32)], val loss: 0.152988001704216\n",
      "\t partial train loss (single batch): 0.152630\n",
      "Epoch 481/1000 train loss: [array(0.15262973, dtype=float32)], val loss: 0.1507784128189087\n",
      "\t partial train loss (single batch): 0.154570\n",
      "Epoch 482/1000 train loss: [array(0.15456995, dtype=float32)], val loss: 0.15311820805072784\n",
      "\t partial train loss (single batch): 0.148689\n",
      "Epoch 483/1000 train loss: [array(0.14868858, dtype=float32)], val loss: 0.1543494462966919\n",
      "\t partial train loss (single batch): 0.155207\n",
      "Epoch 484/1000 train loss: [array(0.15520678, dtype=float32)], val loss: 0.1514427363872528\n",
      "\t partial train loss (single batch): 0.158325\n",
      "Epoch 485/1000 train loss: [array(0.15832485, dtype=float32)], val loss: 0.15185290575027466\n",
      "\t partial train loss (single batch): 0.155782\n",
      "Epoch 486/1000 train loss: [array(0.15578192, dtype=float32)], val loss: 0.15181002020835876\n",
      "\t partial train loss (single batch): 0.154712\n",
      "Epoch 487/1000 train loss: [array(0.15471217, dtype=float32)], val loss: 0.15273906290531158\n",
      "\t partial train loss (single batch): 0.155448\n",
      "Epoch 488/1000 train loss: [array(0.1554476, dtype=float32)], val loss: 0.15413250029087067\n",
      "\t partial train loss (single batch): 0.157121\n",
      "Epoch 489/1000 train loss: [array(0.15712065, dtype=float32)], val loss: 0.15465135872364044\n",
      "\t partial train loss (single batch): 0.151277\n",
      "Epoch 490/1000 train loss: [array(0.1512774, dtype=float32)], val loss: 0.15361298620700836\n",
      "\t partial train loss (single batch): 0.152340\n",
      "Epoch 491/1000 train loss: [array(0.15234001, dtype=float32)], val loss: 0.15355633199214935\n",
      "\t partial train loss (single batch): 0.157528\n",
      "Epoch 492/1000 train loss: [array(0.15752849, dtype=float32)], val loss: 0.15430033206939697\n",
      "\t partial train loss (single batch): 0.153925\n",
      "Epoch 493/1000 train loss: [array(0.15392543, dtype=float32)], val loss: 0.15021812915802002\n",
      "\t partial train loss (single batch): 0.150282\n",
      "Epoch 494/1000 train loss: [array(0.15028195, dtype=float32)], val loss: 0.1492016762495041\n",
      "\t partial train loss (single batch): 0.154487\n",
      "Epoch 495/1000 train loss: [array(0.15448698, dtype=float32)], val loss: 0.14972934126853943\n",
      "\t partial train loss (single batch): 0.154829\n",
      "Epoch 496/1000 train loss: [array(0.1548287, dtype=float32)], val loss: 0.15114153921604156\n",
      "\t partial train loss (single batch): 0.152453\n",
      "Epoch 497/1000 train loss: [array(0.15245268, dtype=float32)], val loss: 0.14551421999931335\n",
      "\t partial train loss (single batch): 0.151312\n",
      "Epoch 498/1000 train loss: [array(0.15131234, dtype=float32)], val loss: 0.1474885791540146\n",
      "\t partial train loss (single batch): 0.151417\n",
      "Epoch 499/1000 train loss: [array(0.15141664, dtype=float32)], val loss: 0.15895457565784454\n",
      "\t partial train loss (single batch): 0.153765\n",
      "Epoch 500/1000 train loss: [array(0.15376505, dtype=float32)], val loss: 0.14702439308166504\n",
      "\t partial train loss (single batch): 0.153159\n",
      "Epoch 501/1000 train loss: [array(0.15315855, dtype=float32)], val loss: 0.15379704535007477\n",
      "\t partial train loss (single batch): 0.151717\n",
      "Epoch 502/1000 train loss: [array(0.15171672, dtype=float32)], val loss: 0.1475483477115631\n",
      "\t partial train loss (single batch): 0.146103\n",
      "Epoch 503/1000 train loss: [array(0.1461031, dtype=float32)], val loss: 0.15063172578811646\n",
      "\t partial train loss (single batch): 0.147773\n",
      "Epoch 504/1000 train loss: [array(0.14777336, dtype=float32)], val loss: 0.15644149482250214\n",
      "\t partial train loss (single batch): 0.147471\n",
      "Epoch 505/1000 train loss: [array(0.147471, dtype=float32)], val loss: 0.14511136710643768\n",
      "\t partial train loss (single batch): 0.145657\n",
      "Epoch 506/1000 train loss: [array(0.14565657, dtype=float32)], val loss: 0.15132543444633484\n",
      "\t partial train loss (single batch): 0.148535\n",
      "Epoch 507/1000 train loss: [array(0.14853485, dtype=float32)], val loss: 0.148034006357193\n",
      "\t partial train loss (single batch): 0.153216\n",
      "Epoch 508/1000 train loss: [array(0.153216, dtype=float32)], val loss: 0.15571917593479156\n",
      "\t partial train loss (single batch): 0.150314\n",
      "Epoch 509/1000 train loss: [array(0.15031436, dtype=float32)], val loss: 0.14690756797790527\n",
      "\t partial train loss (single batch): 0.152206\n",
      "Epoch 510/1000 train loss: [array(0.15220591, dtype=float32)], val loss: 0.15417803823947906\n",
      "\t partial train loss (single batch): 0.149704\n",
      "Epoch 511/1000 train loss: [array(0.14970419, dtype=float32)], val loss: 0.1526416540145874\n",
      "\t partial train loss (single batch): 0.148673\n",
      "Epoch 512/1000 train loss: [array(0.14867342, dtype=float32)], val loss: 0.1452077031135559\n",
      "\t partial train loss (single batch): 0.148940\n",
      "Epoch 513/1000 train loss: [array(0.14894001, dtype=float32)], val loss: 0.1529916226863861\n",
      "\t partial train loss (single batch): 0.148312\n",
      "Epoch 514/1000 train loss: [array(0.14831227, dtype=float32)], val loss: 0.14775097370147705\n",
      "\t partial train loss (single batch): 0.145568\n",
      "Epoch 515/1000 train loss: [array(0.14556782, dtype=float32)], val loss: 0.1443188190460205\n",
      "\t partial train loss (single batch): 0.145142\n",
      "Epoch 516/1000 train loss: [array(0.14514238, dtype=float32)], val loss: 0.1466923952102661\n",
      "\t partial train loss (single batch): 0.152559\n",
      "Epoch 517/1000 train loss: [array(0.15255855, dtype=float32)], val loss: 0.15301044285297394\n",
      "\t partial train loss (single batch): 0.146195\n",
      "Epoch 518/1000 train loss: [array(0.14619502, dtype=float32)], val loss: 0.15137003362178802\n",
      "\t partial train loss (single batch): 0.147230\n",
      "Epoch 519/1000 train loss: [array(0.14723004, dtype=float32)], val loss: 0.1519671529531479\n",
      "\t partial train loss (single batch): 0.149671\n",
      "Epoch 520/1000 train loss: [array(0.14967126, dtype=float32)], val loss: 0.14624011516571045\n",
      "\t partial train loss (single batch): 0.147144\n",
      "Epoch 521/1000 train loss: [array(0.14714402, dtype=float32)], val loss: 0.14681386947631836\n",
      "\t partial train loss (single batch): 0.147576\n",
      "Epoch 522/1000 train loss: [array(0.14757584, dtype=float32)], val loss: 0.1490151435136795\n",
      "\t partial train loss (single batch): 0.152126\n",
      "Epoch 523/1000 train loss: [array(0.15212646, dtype=float32)], val loss: 0.14668214321136475\n",
      "\t partial train loss (single batch): 0.142248\n",
      "Epoch 524/1000 train loss: [array(0.14224786, dtype=float32)], val loss: 0.149641215801239\n",
      "\t partial train loss (single batch): 0.148646\n",
      "Epoch 525/1000 train loss: [array(0.14864562, dtype=float32)], val loss: 0.1473006010055542\n",
      "\t partial train loss (single batch): 0.144094\n",
      "Epoch 526/1000 train loss: [array(0.14409423, dtype=float32)], val loss: 0.1476399004459381\n",
      "\t partial train loss (single batch): 0.143523\n",
      "Epoch 527/1000 train loss: [array(0.14352298, dtype=float32)], val loss: 0.14309938251972198\n",
      "\t partial train loss (single batch): 0.147010\n",
      "Epoch 528/1000 train loss: [array(0.14700975, dtype=float32)], val loss: 0.14688268303871155\n",
      "\t partial train loss (single batch): 0.150299\n",
      "Epoch 529/1000 train loss: [array(0.15029855, dtype=float32)], val loss: 0.14206016063690186\n",
      "\t partial train loss (single batch): 0.142136\n",
      "Epoch 530/1000 train loss: [array(0.14213574, dtype=float32)], val loss: 0.14597468078136444\n",
      "\t partial train loss (single batch): 0.143635\n",
      "Epoch 531/1000 train loss: [array(0.14363544, dtype=float32)], val loss: 0.14365023374557495\n",
      "\t partial train loss (single batch): 0.146149\n",
      "Epoch 532/1000 train loss: [array(0.14614938, dtype=float32)], val loss: 0.14710761606693268\n",
      "\t partial train loss (single batch): 0.145448\n",
      "Epoch 533/1000 train loss: [array(0.14544797, dtype=float32)], val loss: 0.14257895946502686\n",
      "\t partial train loss (single batch): 0.147243\n",
      "Epoch 534/1000 train loss: [array(0.14724278, dtype=float32)], val loss: 0.1445920765399933\n",
      "\t partial train loss (single batch): 0.153680\n",
      "Epoch 535/1000 train loss: [array(0.15367964, dtype=float32)], val loss: 0.14800743758678436\n",
      "\t partial train loss (single batch): 0.147470\n",
      "Epoch 536/1000 train loss: [array(0.14747046, dtype=float32)], val loss: 0.1478734165430069\n",
      "\t partial train loss (single batch): 0.145175\n",
      "Epoch 537/1000 train loss: [array(0.14517514, dtype=float32)], val loss: 0.14040236175060272\n",
      "\t partial train loss (single batch): 0.144599\n",
      "Epoch 538/1000 train loss: [array(0.14459875, dtype=float32)], val loss: 0.14461229741573334\n",
      "\t partial train loss (single batch): 0.143936\n",
      "Epoch 539/1000 train loss: [array(0.14393619, dtype=float32)], val loss: 0.14395774900913239\n",
      "\t partial train loss (single batch): 0.146804\n",
      "Epoch 540/1000 train loss: [array(0.14680359, dtype=float32)], val loss: 0.14552614092826843\n",
      "\t partial train loss (single batch): 0.142630\n",
      "Epoch 541/1000 train loss: [array(0.1426304, dtype=float32)], val loss: 0.14441993832588196\n",
      "\t partial train loss (single batch): 0.144996\n",
      "Epoch 542/1000 train loss: [array(0.14499614, dtype=float32)], val loss: 0.14662137627601624\n",
      "\t partial train loss (single batch): 0.144695\n",
      "Epoch 543/1000 train loss: [array(0.14469543, dtype=float32)], val loss: 0.14197775721549988\n",
      "\t partial train loss (single batch): 0.142785\n",
      "Epoch 544/1000 train loss: [array(0.14278486, dtype=float32)], val loss: 0.14302481710910797\n",
      "\t partial train loss (single batch): 0.140216\n",
      "Epoch 545/1000 train loss: [array(0.14021623, dtype=float32)], val loss: 0.1458444595336914\n",
      "\t partial train loss (single batch): 0.142501\n",
      "Epoch 546/1000 train loss: [array(0.14250138, dtype=float32)], val loss: 0.14108499884605408\n",
      "\t partial train loss (single batch): 0.141948\n",
      "Epoch 547/1000 train loss: [array(0.1419477, dtype=float32)], val loss: 0.14428286254405975\n",
      "\t partial train loss (single batch): 0.145490\n",
      "Epoch 548/1000 train loss: [array(0.14548981, dtype=float32)], val loss: 0.13921977579593658\n",
      "\t partial train loss (single batch): 0.144537\n",
      "Epoch 549/1000 train loss: [array(0.14453661, dtype=float32)], val loss: 0.14053402841091156\n",
      "\t partial train loss (single batch): 0.141290\n",
      "Epoch 550/1000 train loss: [array(0.1412903, dtype=float32)], val loss: 0.14221827685832977\n",
      "\t partial train loss (single batch): 0.138307\n",
      "Epoch 551/1000 train loss: [array(0.13830696, dtype=float32)], val loss: 0.14278149604797363\n",
      "\t partial train loss (single batch): 0.142658\n",
      "Epoch 552/1000 train loss: [array(0.14265771, dtype=float32)], val loss: 0.14404062926769257\n",
      "\t partial train loss (single batch): 0.145649\n",
      "Epoch 553/1000 train loss: [array(0.14564893, dtype=float32)], val loss: 0.13802795112133026\n",
      "\t partial train loss (single batch): 0.144433\n",
      "Epoch 554/1000 train loss: [array(0.14443314, dtype=float32)], val loss: 0.14249520003795624\n",
      "\t partial train loss (single batch): 0.141125\n",
      "Epoch 555/1000 train loss: [array(0.14112455, dtype=float32)], val loss: 0.14364510774612427\n",
      "\t partial train loss (single batch): 0.146089\n",
      "Epoch 556/1000 train loss: [array(0.14608899, dtype=float32)], val loss: 0.138422891497612\n",
      "\t partial train loss (single batch): 0.144499\n",
      "Epoch 557/1000 train loss: [array(0.14449872, dtype=float32)], val loss: 0.1439603567123413\n",
      "\t partial train loss (single batch): 0.143045\n",
      "Epoch 558/1000 train loss: [array(0.14304511, dtype=float32)], val loss: 0.144018292427063\n",
      "\t partial train loss (single batch): 0.142928\n",
      "Epoch 559/1000 train loss: [array(0.14292808, dtype=float32)], val loss: 0.13676635921001434\n",
      "\t partial train loss (single batch): 0.142393\n",
      "Epoch 560/1000 train loss: [array(0.14239313, dtype=float32)], val loss: 0.14367008209228516\n",
      "\t partial train loss (single batch): 0.139272\n",
      "Epoch 561/1000 train loss: [array(0.13927153, dtype=float32)], val loss: 0.13911603391170502\n",
      "\t partial train loss (single batch): 0.143452\n",
      "Epoch 562/1000 train loss: [array(0.1434524, dtype=float32)], val loss: 0.14176340401172638\n",
      "\t partial train loss (single batch): 0.144531\n",
      "Epoch 563/1000 train loss: [array(0.14453113, dtype=float32)], val loss: 0.14245015382766724\n",
      "\t partial train loss (single batch): 0.139224\n",
      "Epoch 564/1000 train loss: [array(0.13922441, dtype=float32)], val loss: 0.13545437157154083\n",
      "\t partial train loss (single batch): 0.138134\n",
      "Epoch 565/1000 train loss: [array(0.13813393, dtype=float32)], val loss: 0.1401909738779068\n",
      "\t partial train loss (single batch): 0.138086\n",
      "Epoch 566/1000 train loss: [array(0.1380865, dtype=float32)], val loss: 0.13826385140419006\n",
      "\t partial train loss (single batch): 0.141834\n",
      "Epoch 567/1000 train loss: [array(0.1418343, dtype=float32)], val loss: 0.1397918462753296\n",
      "\t partial train loss (single batch): 0.140177\n",
      "Epoch 568/1000 train loss: [array(0.14017667, dtype=float32)], val loss: 0.13841046392917633\n",
      "\t partial train loss (single batch): 0.143292\n",
      "Epoch 569/1000 train loss: [array(0.14329207, dtype=float32)], val loss: 0.13976874947547913\n",
      "\t partial train loss (single batch): 0.140640\n",
      "Epoch 570/1000 train loss: [array(0.1406405, dtype=float32)], val loss: 0.13873085379600525\n",
      "\t partial train loss (single batch): 0.146853\n",
      "Epoch 571/1000 train loss: [array(0.14685276, dtype=float32)], val loss: 0.13980981707572937\n",
      "\t partial train loss (single batch): 0.142143\n",
      "Epoch 572/1000 train loss: [array(0.14214267, dtype=float32)], val loss: 0.14045438170433044\n",
      "\t partial train loss (single batch): 0.136846\n",
      "Epoch 573/1000 train loss: [array(0.13684571, dtype=float32)], val loss: 0.14003600180149078\n",
      "\t partial train loss (single batch): 0.138895\n",
      "Epoch 574/1000 train loss: [array(0.13889465, dtype=float32)], val loss: 0.13449698686599731\n",
      "\t partial train loss (single batch): 0.139781\n",
      "Epoch 575/1000 train loss: [array(0.13978113, dtype=float32)], val loss: 0.13800355792045593\n",
      "\t partial train loss (single batch): 0.139818\n",
      "Epoch 576/1000 train loss: [array(0.13981754, dtype=float32)], val loss: 0.14268629252910614\n",
      "\t partial train loss (single batch): 0.138789\n",
      "Epoch 577/1000 train loss: [array(0.13878874, dtype=float32)], val loss: 0.13474497199058533\n",
      "\t partial train loss (single batch): 0.141393\n",
      "Epoch 578/1000 train loss: [array(0.1413926, dtype=float32)], val loss: 0.1417897492647171\n",
      "\t partial train loss (single batch): 0.139704\n",
      "Epoch 579/1000 train loss: [array(0.13970435, dtype=float32)], val loss: 0.1373862773180008\n",
      "\t partial train loss (single batch): 0.140775\n",
      "Epoch 580/1000 train loss: [array(0.14077526, dtype=float32)], val loss: 0.13553017377853394\n",
      "\t partial train loss (single batch): 0.138827\n",
      "Epoch 581/1000 train loss: [array(0.13882722, dtype=float32)], val loss: 0.13534098863601685\n",
      "\t partial train loss (single batch): 0.143215\n",
      "Epoch 582/1000 train loss: [array(0.14321503, dtype=float32)], val loss: 0.13691751658916473\n",
      "\t partial train loss (single batch): 0.137825\n",
      "Epoch 583/1000 train loss: [array(0.13782522, dtype=float32)], val loss: 0.13615691661834717\n",
      "\t partial train loss (single batch): 0.141527\n",
      "Epoch 584/1000 train loss: [array(0.1415271, dtype=float32)], val loss: 0.13490620255470276\n",
      "\t partial train loss (single batch): 0.136495\n",
      "Epoch 585/1000 train loss: [array(0.13649474, dtype=float32)], val loss: 0.13944672048091888\n",
      "\t partial train loss (single batch): 0.143241\n",
      "Epoch 586/1000 train loss: [array(0.14324102, dtype=float32)], val loss: 0.14058326184749603\n",
      "\t partial train loss (single batch): 0.137514\n",
      "Epoch 587/1000 train loss: [array(0.13751404, dtype=float32)], val loss: 0.13600917160511017\n",
      "\t partial train loss (single batch): 0.136077\n",
      "Epoch 588/1000 train loss: [array(0.13607699, dtype=float32)], val loss: 0.13798820972442627\n",
      "\t partial train loss (single batch): 0.140830\n",
      "Epoch 589/1000 train loss: [array(0.14082964, dtype=float32)], val loss: 0.13798683881759644\n",
      "\t partial train loss (single batch): 0.142411\n",
      "Epoch 590/1000 train loss: [array(0.1424109, dtype=float32)], val loss: 0.13757121562957764\n",
      "\t partial train loss (single batch): 0.137401\n",
      "Epoch 591/1000 train loss: [array(0.13740109, dtype=float32)], val loss: 0.1367003470659256\n",
      "\t partial train loss (single batch): 0.139097\n",
      "Epoch 592/1000 train loss: [array(0.1390974, dtype=float32)], val loss: 0.12867170572280884\n",
      "\t partial train loss (single batch): 0.136125\n",
      "Epoch 593/1000 train loss: [array(0.13612479, dtype=float32)], val loss: 0.13505667448043823\n",
      "\t partial train loss (single batch): 0.137898\n",
      "Epoch 594/1000 train loss: [array(0.1378984, dtype=float32)], val loss: 0.13972140848636627\n",
      "\t partial train loss (single batch): 0.138817\n",
      "Epoch 595/1000 train loss: [array(0.13881658, dtype=float32)], val loss: 0.13499051332473755\n",
      "\t partial train loss (single batch): 0.140407\n",
      "Epoch 596/1000 train loss: [array(0.14040667, dtype=float32)], val loss: 0.13592207431793213\n",
      "\t partial train loss (single batch): 0.133258\n",
      "Epoch 597/1000 train loss: [array(0.13325785, dtype=float32)], val loss: 0.13692151010036469\n",
      "\t partial train loss (single batch): 0.139665\n",
      "Epoch 598/1000 train loss: [array(0.13966545, dtype=float32)], val loss: 0.1357080340385437\n",
      "\t partial train loss (single batch): 0.138137\n",
      "Epoch 599/1000 train loss: [array(0.1381365, dtype=float32)], val loss: 0.135005921125412\n",
      "\t partial train loss (single batch): 0.139996\n",
      "Epoch 600/1000 train loss: [array(0.13999616, dtype=float32)], val loss: 0.13761502504348755\n",
      "\t partial train loss (single batch): 0.134508\n",
      "Epoch 601/1000 train loss: [array(0.13450842, dtype=float32)], val loss: 0.1373993456363678\n",
      "\t partial train loss (single batch): 0.134903\n",
      "Epoch 602/1000 train loss: [array(0.13490346, dtype=float32)], val loss: 0.1352560818195343\n",
      "\t partial train loss (single batch): 0.139372\n",
      "Epoch 603/1000 train loss: [array(0.13937154, dtype=float32)], val loss: 0.13662678003311157\n",
      "\t partial train loss (single batch): 0.134088\n",
      "Epoch 604/1000 train loss: [array(0.13408767, dtype=float32)], val loss: 0.1395043134689331\n",
      "\t partial train loss (single batch): 0.139439\n",
      "Epoch 605/1000 train loss: [array(0.13943876, dtype=float32)], val loss: 0.13671483099460602\n",
      "\t partial train loss (single batch): 0.134066\n",
      "Epoch 606/1000 train loss: [array(0.13406558, dtype=float32)], val loss: 0.13386288285255432\n",
      "\t partial train loss (single batch): 0.139260\n",
      "Epoch 607/1000 train loss: [array(0.1392598, dtype=float32)], val loss: 0.1360909342765808\n",
      "\t partial train loss (single batch): 0.135532\n",
      "Epoch 608/1000 train loss: [array(0.13553192, dtype=float32)], val loss: 0.13592037558555603\n",
      "\t partial train loss (single batch): 0.136062\n",
      "Epoch 609/1000 train loss: [array(0.13606204, dtype=float32)], val loss: 0.1395697444677353\n",
      "\t partial train loss (single batch): 0.135858\n",
      "Epoch 610/1000 train loss: [array(0.13585795, dtype=float32)], val loss: 0.13398441672325134\n",
      "\t partial train loss (single batch): 0.133923\n",
      "Epoch 611/1000 train loss: [array(0.13392325, dtype=float32)], val loss: 0.13398095965385437\n",
      "\t partial train loss (single batch): 0.138125\n",
      "Epoch 612/1000 train loss: [array(0.13812548, dtype=float32)], val loss: 0.1376684308052063\n",
      "\t partial train loss (single batch): 0.136177\n",
      "Epoch 613/1000 train loss: [array(0.13617699, dtype=float32)], val loss: 0.1337302029132843\n",
      "\t partial train loss (single batch): 0.131838\n",
      "Epoch 614/1000 train loss: [array(0.13183814, dtype=float32)], val loss: 0.12952008843421936\n",
      "\t partial train loss (single batch): 0.137482\n",
      "Epoch 615/1000 train loss: [array(0.13748243, dtype=float32)], val loss: 0.1360151320695877\n",
      "\t partial train loss (single batch): 0.137086\n",
      "Epoch 616/1000 train loss: [array(0.13708642, dtype=float32)], val loss: 0.13080884516239166\n",
      "\t partial train loss (single batch): 0.132537\n",
      "Epoch 617/1000 train loss: [array(0.13253693, dtype=float32)], val loss: 0.1380530595779419\n",
      "\t partial train loss (single batch): 0.132871\n",
      "Epoch 618/1000 train loss: [array(0.13287136, dtype=float32)], val loss: 0.13123473525047302\n",
      "\t partial train loss (single batch): 0.135634\n",
      "Epoch 619/1000 train loss: [array(0.1356344, dtype=float32)], val loss: 0.1352609097957611\n",
      "\t partial train loss (single batch): 0.133668\n",
      "Epoch 620/1000 train loss: [array(0.13366777, dtype=float32)], val loss: 0.13231836259365082\n",
      "\t partial train loss (single batch): 0.134713\n",
      "Epoch 621/1000 train loss: [array(0.13471307, dtype=float32)], val loss: 0.13539588451385498\n",
      "\t partial train loss (single batch): 0.137398\n",
      "Epoch 622/1000 train loss: [array(0.13739832, dtype=float32)], val loss: 0.13032694160938263\n",
      "\t partial train loss (single batch): 0.134067\n",
      "Epoch 623/1000 train loss: [array(0.13406715, dtype=float32)], val loss: 0.1353505551815033\n",
      "\t partial train loss (single batch): 0.139329\n",
      "Epoch 624/1000 train loss: [array(0.13932863, dtype=float32)], val loss: 0.13080580532550812\n",
      "\t partial train loss (single batch): 0.132144\n",
      "Epoch 625/1000 train loss: [array(0.13214399, dtype=float32)], val loss: 0.131156325340271\n",
      "\t partial train loss (single batch): 0.134530\n",
      "Epoch 626/1000 train loss: [array(0.13452952, dtype=float32)], val loss: 0.12851440906524658\n",
      "\t partial train loss (single batch): 0.134870\n",
      "Epoch 627/1000 train loss: [array(0.13486978, dtype=float32)], val loss: 0.13242536783218384\n",
      "\t partial train loss (single batch): 0.135488\n",
      "Epoch 628/1000 train loss: [array(0.1354882, dtype=float32)], val loss: 0.12988319993019104\n",
      "\t partial train loss (single batch): 0.134729\n",
      "Epoch 629/1000 train loss: [array(0.13472895, dtype=float32)], val loss: 0.12837935984134674\n",
      "\t partial train loss (single batch): 0.134341\n",
      "Epoch 630/1000 train loss: [array(0.13434097, dtype=float32)], val loss: 0.1360584944486618\n",
      "\t partial train loss (single batch): 0.133838\n",
      "Epoch 631/1000 train loss: [array(0.13383825, dtype=float32)], val loss: 0.13205908238887787\n",
      "\t partial train loss (single batch): 0.135435\n",
      "Epoch 632/1000 train loss: [array(0.1354353, dtype=float32)], val loss: 0.13200223445892334\n",
      "\t partial train loss (single batch): 0.134525\n",
      "Epoch 633/1000 train loss: [array(0.13452545, dtype=float32)], val loss: 0.12763991951942444\n",
      "\t partial train loss (single batch): 0.133222\n",
      "Epoch 634/1000 train loss: [array(0.13322228, dtype=float32)], val loss: 0.13152505457401276\n",
      "\t partial train loss (single batch): 0.135012\n",
      "Epoch 635/1000 train loss: [array(0.13501249, dtype=float32)], val loss: 0.1312200129032135\n",
      "\t partial train loss (single batch): 0.129869\n",
      "Epoch 636/1000 train loss: [array(0.12986939, dtype=float32)], val loss: 0.1288774013519287\n",
      "\t partial train loss (single batch): 0.128743\n",
      "Epoch 637/1000 train loss: [array(0.12874304, dtype=float32)], val loss: 0.1297517716884613\n",
      "\t partial train loss (single batch): 0.133375\n",
      "Epoch 638/1000 train loss: [array(0.13337457, dtype=float32)], val loss: 0.12875241041183472\n",
      "\t partial train loss (single batch): 0.135673\n",
      "Epoch 639/1000 train loss: [array(0.13567324, dtype=float32)], val loss: 0.1297019124031067\n",
      "\t partial train loss (single batch): 0.131474\n",
      "Epoch 640/1000 train loss: [array(0.13147406, dtype=float32)], val loss: 0.13123132288455963\n",
      "\t partial train loss (single batch): 0.132865\n",
      "Epoch 641/1000 train loss: [array(0.13286458, dtype=float32)], val loss: 0.13098746538162231\n",
      "\t partial train loss (single batch): 0.133047\n",
      "Epoch 642/1000 train loss: [array(0.13304737, dtype=float32)], val loss: 0.13153453171253204\n",
      "\t partial train loss (single batch): 0.128740\n",
      "Epoch 643/1000 train loss: [array(0.12873968, dtype=float32)], val loss: 0.12904350459575653\n",
      "\t partial train loss (single batch): 0.131614\n",
      "Epoch 644/1000 train loss: [array(0.13161375, dtype=float32)], val loss: 0.13267453014850616\n",
      "\t partial train loss (single batch): 0.133708\n",
      "Epoch 645/1000 train loss: [array(0.13370822, dtype=float32)], val loss: 0.1285776048898697\n",
      "\t partial train loss (single batch): 0.131492\n",
      "Epoch 646/1000 train loss: [array(0.13149238, dtype=float32)], val loss: 0.1347247213125229\n",
      "\t partial train loss (single batch): 0.134001\n",
      "Epoch 647/1000 train loss: [array(0.13400057, dtype=float32)], val loss: 0.13064588606357574\n",
      "\t partial train loss (single batch): 0.133230\n",
      "Epoch 648/1000 train loss: [array(0.13322993, dtype=float32)], val loss: 0.1263790726661682\n",
      "\t partial train loss (single batch): 0.131427\n",
      "Epoch 649/1000 train loss: [array(0.13142745, dtype=float32)], val loss: 0.1359875351190567\n",
      "\t partial train loss (single batch): 0.132147\n",
      "Epoch 650/1000 train loss: [array(0.13214716, dtype=float32)], val loss: 0.13006632030010223\n",
      "\t partial train loss (single batch): 0.130381\n",
      "Epoch 651/1000 train loss: [array(0.13038085, dtype=float32)], val loss: 0.13076326251029968\n",
      "\t partial train loss (single batch): 0.133653\n",
      "Epoch 652/1000 train loss: [array(0.13365257, dtype=float32)], val loss: 0.12653511762619019\n",
      "\t partial train loss (single batch): 0.127672\n",
      "Epoch 653/1000 train loss: [array(0.12767151, dtype=float32)], val loss: 0.12764960527420044\n",
      "\t partial train loss (single batch): 0.133011\n",
      "Epoch 654/1000 train loss: [array(0.13301091, dtype=float32)], val loss: 0.12724065780639648\n",
      "\t partial train loss (single batch): 0.129951\n",
      "Epoch 655/1000 train loss: [array(0.1299508, dtype=float32)], val loss: 0.13176469504833221\n",
      "\t partial train loss (single batch): 0.128973\n",
      "Epoch 656/1000 train loss: [array(0.12897308, dtype=float32)], val loss: 0.12979236245155334\n",
      "\t partial train loss (single batch): 0.132959\n",
      "Epoch 657/1000 train loss: [array(0.13295905, dtype=float32)], val loss: 0.1279006153345108\n",
      "\t partial train loss (single batch): 0.128896\n",
      "Epoch 658/1000 train loss: [array(0.12889642, dtype=float32)], val loss: 0.12540562450885773\n",
      "\t partial train loss (single batch): 0.132542\n",
      "Epoch 659/1000 train loss: [array(0.13254167, dtype=float32)], val loss: 0.12970852851867676\n",
      "\t partial train loss (single batch): 0.130454\n",
      "Epoch 660/1000 train loss: [array(0.13045405, dtype=float32)], val loss: 0.13012999296188354\n",
      "\t partial train loss (single batch): 0.132188\n",
      "Epoch 661/1000 train loss: [array(0.13218783, dtype=float32)], val loss: 0.13025349378585815\n",
      "\t partial train loss (single batch): 0.136812\n",
      "Epoch 662/1000 train loss: [array(0.13681236, dtype=float32)], val loss: 0.12717200815677643\n",
      "\t partial train loss (single batch): 0.130899\n",
      "Epoch 663/1000 train loss: [array(0.13089888, dtype=float32)], val loss: 0.13552139699459076\n",
      "\t partial train loss (single batch): 0.127822\n",
      "Epoch 664/1000 train loss: [array(0.12782168, dtype=float32)], val loss: 0.12514126300811768\n",
      "\t partial train loss (single batch): 0.130430\n",
      "Epoch 665/1000 train loss: [array(0.13042967, dtype=float32)], val loss: 0.1310931295156479\n",
      "\t partial train loss (single batch): 0.132215\n",
      "Epoch 666/1000 train loss: [array(0.1322155, dtype=float32)], val loss: 0.12940576672554016\n",
      "\t partial train loss (single batch): 0.132327\n",
      "Epoch 667/1000 train loss: [array(0.1323274, dtype=float32)], val loss: 0.12271320819854736\n",
      "\t partial train loss (single batch): 0.128682\n",
      "Epoch 668/1000 train loss: [array(0.12868245, dtype=float32)], val loss: 0.12803038954734802\n",
      "\t partial train loss (single batch): 0.131483\n",
      "Epoch 669/1000 train loss: [array(0.13148324, dtype=float32)], val loss: 0.12562713027000427\n",
      "\t partial train loss (single batch): 0.133695\n",
      "Epoch 670/1000 train loss: [array(0.13369504, dtype=float32)], val loss: 0.13232992589473724\n",
      "\t partial train loss (single batch): 0.133693\n",
      "Epoch 671/1000 train loss: [array(0.13369267, dtype=float32)], val loss: 0.13261032104492188\n",
      "\t partial train loss (single batch): 0.132411\n",
      "Epoch 672/1000 train loss: [array(0.13241136, dtype=float32)], val loss: 0.12732335925102234\n",
      "\t partial train loss (single batch): 0.129778\n",
      "Epoch 673/1000 train loss: [array(0.12977757, dtype=float32)], val loss: 0.128944531083107\n",
      "\t partial train loss (single batch): 0.129625\n",
      "Epoch 674/1000 train loss: [array(0.1296252, dtype=float32)], val loss: 0.12959016859531403\n",
      "\t partial train loss (single batch): 0.129603\n",
      "Epoch 675/1000 train loss: [array(0.12960336, dtype=float32)], val loss: 0.13514237105846405\n",
      "\t partial train loss (single batch): 0.131894\n",
      "Epoch 676/1000 train loss: [array(0.13189414, dtype=float32)], val loss: 0.12621699273586273\n",
      "\t partial train loss (single batch): 0.133281\n",
      "Epoch 677/1000 train loss: [array(0.133281, dtype=float32)], val loss: 0.12517333030700684\n",
      "\t partial train loss (single batch): 0.131055\n",
      "Epoch 678/1000 train loss: [array(0.13105509, dtype=float32)], val loss: 0.12676575779914856\n",
      "\t partial train loss (single batch): 0.130270\n",
      "Epoch 679/1000 train loss: [array(0.13027008, dtype=float32)], val loss: 0.13206540048122406\n",
      "\t partial train loss (single batch): 0.128841\n",
      "Epoch 680/1000 train loss: [array(0.12884095, dtype=float32)], val loss: 0.12779343128204346\n",
      "\t partial train loss (single batch): 0.127012\n",
      "Epoch 681/1000 train loss: [array(0.12701231, dtype=float32)], val loss: 0.12868189811706543\n",
      "\t partial train loss (single batch): 0.126817\n",
      "Epoch 682/1000 train loss: [array(0.12681672, dtype=float32)], val loss: 0.12693409621715546\n",
      "\t partial train loss (single batch): 0.125025\n",
      "Epoch 683/1000 train loss: [array(0.12502517, dtype=float32)], val loss: 0.12803441286087036\n",
      "\t partial train loss (single batch): 0.130005\n",
      "Epoch 684/1000 train loss: [array(0.13000545, dtype=float32)], val loss: 0.1270984709262848\n",
      "\t partial train loss (single batch): 0.130729\n",
      "Epoch 685/1000 train loss: [array(0.1307295, dtype=float32)], val loss: 0.12578429281711578\n",
      "\t partial train loss (single batch): 0.132704\n",
      "Epoch 686/1000 train loss: [array(0.13270359, dtype=float32)], val loss: 0.12946149706840515\n",
      "\t partial train loss (single batch): 0.129670\n",
      "Epoch 687/1000 train loss: [array(0.1296697, dtype=float32)], val loss: 0.12718510627746582\n",
      "\t partial train loss (single batch): 0.125687\n",
      "Epoch 688/1000 train loss: [array(0.12568721, dtype=float32)], val loss: 0.13108070194721222\n",
      "\t partial train loss (single batch): 0.129515\n",
      "Epoch 689/1000 train loss: [array(0.1295145, dtype=float32)], val loss: 0.12313830852508545\n",
      "\t partial train loss (single batch): 0.130609\n",
      "Epoch 690/1000 train loss: [array(0.13060892, dtype=float32)], val loss: 0.12641377747058868\n",
      "\t partial train loss (single batch): 0.130690\n",
      "Epoch 691/1000 train loss: [array(0.13069034, dtype=float32)], val loss: 0.12956267595291138\n",
      "\t partial train loss (single batch): 0.132991\n",
      "Epoch 692/1000 train loss: [array(0.13299061, dtype=float32)], val loss: 0.1304255574941635\n",
      "\t partial train loss (single batch): 0.128202\n",
      "Epoch 693/1000 train loss: [array(0.12820184, dtype=float32)], val loss: 0.12563782930374146\n",
      "\t partial train loss (single batch): 0.127961\n",
      "Epoch 694/1000 train loss: [array(0.12796104, dtype=float32)], val loss: 0.12822166085243225\n",
      "\t partial train loss (single batch): 0.123229\n",
      "Epoch 695/1000 train loss: [array(0.12322909, dtype=float32)], val loss: 0.13175028562545776\n",
      "\t partial train loss (single batch): 0.126173\n",
      "Epoch 696/1000 train loss: [array(0.12617278, dtype=float32)], val loss: 0.1270349621772766\n",
      "\t partial train loss (single batch): 0.129657\n",
      "Epoch 697/1000 train loss: [array(0.12965667, dtype=float32)], val loss: 0.12530502676963806\n",
      "\t partial train loss (single batch): 0.125671\n",
      "Epoch 698/1000 train loss: [array(0.12567145, dtype=float32)], val loss: 0.1300497204065323\n",
      "\t partial train loss (single batch): 0.133659\n",
      "Epoch 699/1000 train loss: [array(0.13365877, dtype=float32)], val loss: 0.12994341552257538\n",
      "\t partial train loss (single batch): 0.128942\n",
      "Epoch 700/1000 train loss: [array(0.12894225, dtype=float32)], val loss: 0.12853732705116272\n",
      "\t partial train loss (single batch): 0.128444\n",
      "Epoch 701/1000 train loss: [array(0.1284441, dtype=float32)], val loss: 0.131110280752182\n",
      "\t partial train loss (single batch): 0.127202\n",
      "Epoch 702/1000 train loss: [array(0.1272024, dtype=float32)], val loss: 0.123725026845932\n",
      "\t partial train loss (single batch): 0.130734\n",
      "Epoch 703/1000 train loss: [array(0.13073353, dtype=float32)], val loss: 0.12800179421901703\n",
      "\t partial train loss (single batch): 0.129246\n",
      "Epoch 704/1000 train loss: [array(0.1292459, dtype=float32)], val loss: 0.12323878705501556\n",
      "\t partial train loss (single batch): 0.125617\n",
      "Epoch 705/1000 train loss: [array(0.1256173, dtype=float32)], val loss: 0.12681294977664948\n",
      "\t partial train loss (single batch): 0.124676\n",
      "Epoch 706/1000 train loss: [array(0.12467603, dtype=float32)], val loss: 0.12391868233680725\n",
      "\t partial train loss (single batch): 0.126539\n",
      "Epoch 707/1000 train loss: [array(0.12653852, dtype=float32)], val loss: 0.1271963268518448\n",
      "\t partial train loss (single batch): 0.130788\n",
      "Epoch 708/1000 train loss: [array(0.1307882, dtype=float32)], val loss: 0.13068389892578125\n",
      "\t partial train loss (single batch): 0.123565\n",
      "Epoch 709/1000 train loss: [array(0.12356503, dtype=float32)], val loss: 0.12858480215072632\n",
      "\t partial train loss (single batch): 0.127307\n",
      "Epoch 710/1000 train loss: [array(0.12730746, dtype=float32)], val loss: 0.1322559118270874\n",
      "\t partial train loss (single batch): 0.128917\n",
      "Epoch 711/1000 train loss: [array(0.12891738, dtype=float32)], val loss: 0.1277540624141693\n",
      "\t partial train loss (single batch): 0.125595\n",
      "Epoch 712/1000 train loss: [array(0.1255953, dtype=float32)], val loss: 0.1252979040145874\n",
      "\t partial train loss (single batch): 0.124170\n",
      "Epoch 713/1000 train loss: [array(0.12416955, dtype=float32)], val loss: 0.12440644204616547\n",
      "\t partial train loss (single batch): 0.129771\n",
      "Epoch 714/1000 train loss: [array(0.1297705, dtype=float32)], val loss: 0.1289817839860916\n",
      "\t partial train loss (single batch): 0.123627\n",
      "Epoch 715/1000 train loss: [array(0.12362691, dtype=float32)], val loss: 0.12659864127635956\n",
      "\t partial train loss (single batch): 0.127265\n",
      "Epoch 716/1000 train loss: [array(0.12726486, dtype=float32)], val loss: 0.12556171417236328\n",
      "\t partial train loss (single batch): 0.126530\n",
      "Epoch 717/1000 train loss: [array(0.1265305, dtype=float32)], val loss: 0.12453161925077438\n",
      "\t partial train loss (single batch): 0.126671\n",
      "Epoch 718/1000 train loss: [array(0.1266713, dtype=float32)], val loss: 0.12120652943849564\n",
      "\t partial train loss (single batch): 0.124430\n",
      "Epoch 719/1000 train loss: [array(0.1244297, dtype=float32)], val loss: 0.12632562220096588\n",
      "\t partial train loss (single batch): 0.128903\n",
      "Epoch 720/1000 train loss: [array(0.1289029, dtype=float32)], val loss: 0.12704437971115112\n",
      "\t partial train loss (single batch): 0.128649\n",
      "Epoch 721/1000 train loss: [array(0.12864862, dtype=float32)], val loss: 0.12374362349510193\n",
      "\t partial train loss (single batch): 0.128022\n",
      "Epoch 722/1000 train loss: [array(0.12802167, dtype=float32)], val loss: 0.12920883297920227\n",
      "\t partial train loss (single batch): 0.125317\n",
      "Epoch 723/1000 train loss: [array(0.12531726, dtype=float32)], val loss: 0.12240549176931381\n",
      "\t partial train loss (single batch): 0.123358\n",
      "Epoch 724/1000 train loss: [array(0.12335806, dtype=float32)], val loss: 0.11974938958883286\n",
      "\t partial train loss (single batch): 0.122917\n",
      "Epoch 725/1000 train loss: [array(0.12291654, dtype=float32)], val loss: 0.11955958604812622\n",
      "\t partial train loss (single batch): 0.127630\n",
      "Epoch 726/1000 train loss: [array(0.12762958, dtype=float32)], val loss: 0.12432755529880524\n",
      "\t partial train loss (single batch): 0.123998\n",
      "Epoch 727/1000 train loss: [array(0.12399819, dtype=float32)], val loss: 0.12450306117534637\n",
      "\t partial train loss (single batch): 0.122962\n",
      "Epoch 728/1000 train loss: [array(0.12296204, dtype=float32)], val loss: 0.12661203742027283\n",
      "\t partial train loss (single batch): 0.124678\n",
      "Epoch 729/1000 train loss: [array(0.12467792, dtype=float32)], val loss: 0.12306489795446396\n",
      "\t partial train loss (single batch): 0.124602\n",
      "Epoch 730/1000 train loss: [array(0.12460169, dtype=float32)], val loss: 0.12498734146356583\n",
      "\t partial train loss (single batch): 0.122006\n",
      "Epoch 731/1000 train loss: [array(0.12200629, dtype=float32)], val loss: 0.11544126272201538\n",
      "\t partial train loss (single batch): 0.122503\n",
      "Epoch 732/1000 train loss: [array(0.12250318, dtype=float32)], val loss: 0.12439857423305511\n",
      "\t partial train loss (single batch): 0.122695\n",
      "Epoch 733/1000 train loss: [array(0.12269457, dtype=float32)], val loss: 0.12234452366828918\n",
      "\t partial train loss (single batch): 0.121727\n",
      "Epoch 734/1000 train loss: [array(0.12172736, dtype=float32)], val loss: 0.12517687678337097\n",
      "\t partial train loss (single batch): 0.124428\n",
      "Epoch 735/1000 train loss: [array(0.12442812, dtype=float32)], val loss: 0.1211862564086914\n",
      "\t partial train loss (single batch): 0.124760\n",
      "Epoch 736/1000 train loss: [array(0.1247603, dtype=float32)], val loss: 0.12262821942567825\n",
      "\t partial train loss (single batch): 0.122510\n",
      "Epoch 737/1000 train loss: [array(0.12250951, dtype=float32)], val loss: 0.12121465057134628\n",
      "\t partial train loss (single batch): 0.125074\n",
      "Epoch 738/1000 train loss: [array(0.1250737, dtype=float32)], val loss: 0.12902648746967316\n",
      "\t partial train loss (single batch): 0.124665\n",
      "Epoch 739/1000 train loss: [array(0.12466468, dtype=float32)], val loss: 0.12596046924591064\n",
      "\t partial train loss (single batch): 0.122931\n",
      "Epoch 740/1000 train loss: [array(0.12293068, dtype=float32)], val loss: 0.1232161819934845\n",
      "\t partial train loss (single batch): 0.123319\n",
      "Epoch 741/1000 train loss: [array(0.12331857, dtype=float32)], val loss: 0.12633182108402252\n",
      "\t partial train loss (single batch): 0.122724\n",
      "Epoch 742/1000 train loss: [array(0.1227245, dtype=float32)], val loss: 0.1239621713757515\n",
      "\t partial train loss (single batch): 0.123796\n",
      "Epoch 743/1000 train loss: [array(0.12379636, dtype=float32)], val loss: 0.11937040090560913\n",
      "\t partial train loss (single batch): 0.122671\n",
      "Epoch 744/1000 train loss: [array(0.12267116, dtype=float32)], val loss: 0.12304720282554626\n",
      "\t partial train loss (single batch): 0.124714\n",
      "Epoch 745/1000 train loss: [array(0.12471378, dtype=float32)], val loss: 0.12192385643720627\n",
      "\t partial train loss (single batch): 0.120520\n",
      "Epoch 746/1000 train loss: [array(0.12051957, dtype=float32)], val loss: 0.12419210374355316\n",
      "\t partial train loss (single batch): 0.125247\n",
      "Epoch 747/1000 train loss: [array(0.1252474, dtype=float32)], val loss: 0.12127792835235596\n",
      "\t partial train loss (single batch): 0.119643\n",
      "Epoch 748/1000 train loss: [array(0.119643, dtype=float32)], val loss: 0.12169106304645538\n",
      "\t partial train loss (single batch): 0.126088\n",
      "Epoch 749/1000 train loss: [array(0.12608765, dtype=float32)], val loss: 0.1280856430530548\n",
      "\t partial train loss (single batch): 0.121110\n",
      "Epoch 750/1000 train loss: [array(0.12111031, dtype=float32)], val loss: 0.12043756991624832\n",
      "\t partial train loss (single batch): 0.123175\n",
      "Epoch 751/1000 train loss: [array(0.12317457, dtype=float32)], val loss: 0.12847746908664703\n",
      "\t partial train loss (single batch): 0.125807\n",
      "Epoch 752/1000 train loss: [array(0.1258072, dtype=float32)], val loss: 0.12366682291030884\n",
      "\t partial train loss (single batch): 0.126060\n",
      "Epoch 753/1000 train loss: [array(0.1260599, dtype=float32)], val loss: 0.123957060277462\n",
      "\t partial train loss (single batch): 0.127566\n",
      "Epoch 754/1000 train loss: [array(0.12756576, dtype=float32)], val loss: 0.128041073679924\n",
      "\t partial train loss (single batch): 0.122646\n",
      "Epoch 755/1000 train loss: [array(0.1226461, dtype=float32)], val loss: 0.1207638531923294\n",
      "\t partial train loss (single batch): 0.126214\n",
      "Epoch 756/1000 train loss: [array(0.12621392, dtype=float32)], val loss: 0.12054657191038132\n",
      "\t partial train loss (single batch): 0.122008\n",
      "Epoch 757/1000 train loss: [array(0.12200844, dtype=float32)], val loss: 0.12182993441820145\n",
      "\t partial train loss (single batch): 0.131858\n",
      "Epoch 758/1000 train loss: [array(0.13185826, dtype=float32)], val loss: 0.1258200854063034\n",
      "\t partial train loss (single batch): 0.122848\n",
      "Epoch 759/1000 train loss: [array(0.12284774, dtype=float32)], val loss: 0.1200157180428505\n",
      "\t partial train loss (single batch): 0.122148\n",
      "Epoch 760/1000 train loss: [array(0.12214779, dtype=float32)], val loss: 0.1263328343629837\n",
      "\t partial train loss (single batch): 0.120414\n",
      "Epoch 761/1000 train loss: [array(0.12041358, dtype=float32)], val loss: 0.126797616481781\n",
      "\t partial train loss (single batch): 0.118462\n",
      "Epoch 762/1000 train loss: [array(0.11846211, dtype=float32)], val loss: 0.12109766900539398\n",
      "\t partial train loss (single batch): 0.128777\n",
      "Epoch 763/1000 train loss: [array(0.12877667, dtype=float32)], val loss: 0.12141420692205429\n",
      "\t partial train loss (single batch): 0.121566\n",
      "Epoch 764/1000 train loss: [array(0.12156642, dtype=float32)], val loss: 0.12136021256446838\n",
      "\t partial train loss (single batch): 0.123422\n",
      "Epoch 765/1000 train loss: [array(0.12342165, dtype=float32)], val loss: 0.11718202382326126\n",
      "\t partial train loss (single batch): 0.124717\n",
      "Epoch 766/1000 train loss: [array(0.12471651, dtype=float32)], val loss: 0.12149447202682495\n",
      "\t partial train loss (single batch): 0.125799\n",
      "Epoch 767/1000 train loss: [array(0.12579934, dtype=float32)], val loss: 0.1236102432012558\n",
      "\t partial train loss (single batch): 0.124401\n",
      "Epoch 768/1000 train loss: [array(0.12440147, dtype=float32)], val loss: 0.12392747402191162\n",
      "\t partial train loss (single batch): 0.124381\n",
      "Epoch 769/1000 train loss: [array(0.12438093, dtype=float32)], val loss: 0.1229865625500679\n",
      "\t partial train loss (single batch): 0.123050\n",
      "Epoch 770/1000 train loss: [array(0.12305015, dtype=float32)], val loss: 0.1202274039387703\n",
      "\t partial train loss (single batch): 0.125659\n",
      "Epoch 771/1000 train loss: [array(0.12565887, dtype=float32)], val loss: 0.11625689268112183\n",
      "\t partial train loss (single batch): 0.123649\n",
      "Epoch 772/1000 train loss: [array(0.12364917, dtype=float32)], val loss: 0.11854492872953415\n",
      "\t partial train loss (single batch): 0.116587\n",
      "Epoch 773/1000 train loss: [array(0.11658739, dtype=float32)], val loss: 0.1214539110660553\n",
      "\t partial train loss (single batch): 0.122355\n",
      "Epoch 774/1000 train loss: [array(0.12235454, dtype=float32)], val loss: 0.12140392512083054\n",
      "\t partial train loss (single batch): 0.122011\n",
      "Epoch 775/1000 train loss: [array(0.12201077, dtype=float32)], val loss: 0.12086323648691177\n",
      "\t partial train loss (single batch): 0.123078\n",
      "Epoch 776/1000 train loss: [array(0.12307753, dtype=float32)], val loss: 0.12264399975538254\n",
      "\t partial train loss (single batch): 0.121227\n",
      "Epoch 777/1000 train loss: [array(0.12122744, dtype=float32)], val loss: 0.12167074531316757\n",
      "\t partial train loss (single batch): 0.120845\n",
      "Epoch 778/1000 train loss: [array(0.12084519, dtype=float32)], val loss: 0.1213727742433548\n",
      "\t partial train loss (single batch): 0.120873\n",
      "Epoch 779/1000 train loss: [array(0.12087295, dtype=float32)], val loss: 0.11994700878858566\n",
      "\t partial train loss (single batch): 0.119973\n",
      "Epoch 780/1000 train loss: [array(0.1199725, dtype=float32)], val loss: 0.11631771177053452\n",
      "\t partial train loss (single batch): 0.122168\n",
      "Epoch 781/1000 train loss: [array(0.12216774, dtype=float32)], val loss: 0.12301836162805557\n",
      "\t partial train loss (single batch): 0.121671\n",
      "Epoch 782/1000 train loss: [array(0.12167129, dtype=float32)], val loss: 0.12230628728866577\n",
      "\t partial train loss (single batch): 0.121361\n",
      "Epoch 783/1000 train loss: [array(0.12136085, dtype=float32)], val loss: 0.11897410452365875\n",
      "\t partial train loss (single batch): 0.120108\n",
      "Epoch 784/1000 train loss: [array(0.12010798, dtype=float32)], val loss: 0.12182534486055374\n",
      "\t partial train loss (single batch): 0.122556\n",
      "Epoch 785/1000 train loss: [array(0.12255649, dtype=float32)], val loss: 0.11797428131103516\n",
      "\t partial train loss (single batch): 0.121658\n",
      "Epoch 786/1000 train loss: [array(0.12165777, dtype=float32)], val loss: 0.11902255564928055\n",
      "\t partial train loss (single batch): 0.124546\n",
      "Epoch 787/1000 train loss: [array(0.12454644, dtype=float32)], val loss: 0.11996225267648697\n",
      "\t partial train loss (single batch): 0.122440\n",
      "Epoch 788/1000 train loss: [array(0.12243956, dtype=float32)], val loss: 0.11946726590394974\n",
      "\t partial train loss (single batch): 0.128113\n",
      "Epoch 789/1000 train loss: [array(0.12811325, dtype=float32)], val loss: 0.12183672934770584\n",
      "\t partial train loss (single batch): 0.120751\n",
      "Epoch 790/1000 train loss: [array(0.12075069, dtype=float32)], val loss: 0.12078732997179031\n",
      "\t partial train loss (single batch): 0.123046\n",
      "Epoch 791/1000 train loss: [array(0.12304559, dtype=float32)], val loss: 0.12384891510009766\n",
      "\t partial train loss (single batch): 0.126775\n",
      "Epoch 792/1000 train loss: [array(0.1267751, dtype=float32)], val loss: 0.11950718611478806\n",
      "\t partial train loss (single batch): 0.121926\n",
      "Epoch 793/1000 train loss: [array(0.12192564, dtype=float32)], val loss: 0.12399368733167648\n",
      "\t partial train loss (single batch): 0.127753\n",
      "Epoch 794/1000 train loss: [array(0.12775308, dtype=float32)], val loss: 0.12208738923072815\n",
      "\t partial train loss (single batch): 0.124412\n",
      "Epoch 795/1000 train loss: [array(0.12441188, dtype=float32)], val loss: 0.12258271872997284\n",
      "\t partial train loss (single batch): 0.116814\n",
      "Epoch 796/1000 train loss: [array(0.11681397, dtype=float32)], val loss: 0.1223750188946724\n",
      "\t partial train loss (single batch): 0.121904\n",
      "Epoch 797/1000 train loss: [array(0.12190364, dtype=float32)], val loss: 0.12201828509569168\n",
      "\t partial train loss (single batch): 0.122263\n",
      "Epoch 798/1000 train loss: [array(0.12226264, dtype=float32)], val loss: 0.11803543567657471\n",
      "\t partial train loss (single batch): 0.117883\n",
      "Epoch 799/1000 train loss: [array(0.11788286, dtype=float32)], val loss: 0.12351565808057785\n",
      "\t partial train loss (single batch): 0.123388\n",
      "Epoch 800/1000 train loss: [array(0.12338809, dtype=float32)], val loss: 0.12310877442359924\n",
      "\t partial train loss (single batch): 0.122528\n",
      "Epoch 801/1000 train loss: [array(0.12252848, dtype=float32)], val loss: 0.11774059385061264\n",
      "\t partial train loss (single batch): 0.119026\n",
      "Epoch 802/1000 train loss: [array(0.11902636, dtype=float32)], val loss: 0.12314946204423904\n",
      "\t partial train loss (single batch): 0.124742\n",
      "Epoch 803/1000 train loss: [array(0.12474176, dtype=float32)], val loss: 0.1209200918674469\n",
      "\t partial train loss (single batch): 0.123476\n",
      "Epoch 804/1000 train loss: [array(0.12347575, dtype=float32)], val loss: 0.11910117417573929\n",
      "\t partial train loss (single batch): 0.121299\n",
      "Epoch 805/1000 train loss: [array(0.12129878, dtype=float32)], val loss: 0.12073428928852081\n",
      "\t partial train loss (single batch): 0.118281\n",
      "Epoch 806/1000 train loss: [array(0.11828092, dtype=float32)], val loss: 0.12079858034849167\n",
      "\t partial train loss (single batch): 0.120209\n",
      "Epoch 807/1000 train loss: [array(0.12020928, dtype=float32)], val loss: 0.11981562525033951\n",
      "\t partial train loss (single batch): 0.124650\n",
      "Epoch 808/1000 train loss: [array(0.12464978, dtype=float32)], val loss: 0.11883760988712311\n",
      "\t partial train loss (single batch): 0.120248\n",
      "Epoch 809/1000 train loss: [array(0.12024799, dtype=float32)], val loss: 0.1217598170042038\n",
      "\t partial train loss (single batch): 0.121353\n",
      "Epoch 810/1000 train loss: [array(0.12135313, dtype=float32)], val loss: 0.11876906454563141\n",
      "\t partial train loss (single batch): 0.121406\n",
      "Epoch 811/1000 train loss: [array(0.12140569, dtype=float32)], val loss: 0.11712829768657684\n",
      "\t partial train loss (single batch): 0.121350\n",
      "Epoch 812/1000 train loss: [array(0.12135007, dtype=float32)], val loss: 0.12384501099586487\n",
      "\t partial train loss (single batch): 0.118321\n",
      "Epoch 813/1000 train loss: [array(0.1183212, dtype=float32)], val loss: 0.11790484935045242\n",
      "\t partial train loss (single batch): 0.119591\n",
      "Epoch 814/1000 train loss: [array(0.11959063, dtype=float32)], val loss: 0.12073130160570145\n",
      "\t partial train loss (single batch): 0.120527\n",
      "Epoch 815/1000 train loss: [array(0.12052705, dtype=float32)], val loss: 0.1230672225356102\n",
      "\t partial train loss (single batch): 0.120644\n",
      "Epoch 816/1000 train loss: [array(0.12064442, dtype=float32)], val loss: 0.11920840293169022\n",
      "\t partial train loss (single batch): 0.122723\n",
      "Epoch 817/1000 train loss: [array(0.12272266, dtype=float32)], val loss: 0.11626271158456802\n",
      "\t partial train loss (single batch): 0.117149\n",
      "Epoch 818/1000 train loss: [array(0.11714887, dtype=float32)], val loss: 0.12137369066476822\n",
      "\t partial train loss (single batch): 0.114848\n",
      "Epoch 819/1000 train loss: [array(0.11484806, dtype=float32)], val loss: 0.11538320779800415\n",
      "\t partial train loss (single batch): 0.117021\n",
      "Epoch 820/1000 train loss: [array(0.11702105, dtype=float32)], val loss: 0.11837873607873917\n",
      "\t partial train loss (single batch): 0.118024\n",
      "Epoch 821/1000 train loss: [array(0.11802352, dtype=float32)], val loss: 0.11908784508705139\n",
      "\t partial train loss (single batch): 0.117430\n",
      "Epoch 822/1000 train loss: [array(0.11743018, dtype=float32)], val loss: 0.11676984280347824\n",
      "\t partial train loss (single batch): 0.118373\n",
      "Epoch 823/1000 train loss: [array(0.11837292, dtype=float32)], val loss: 0.11915256828069687\n",
      "\t partial train loss (single batch): 0.123546\n",
      "Epoch 824/1000 train loss: [array(0.12354627, dtype=float32)], val loss: 0.11360107362270355\n",
      "\t partial train loss (single batch): 0.121743\n",
      "Epoch 825/1000 train loss: [array(0.12174252, dtype=float32)], val loss: 0.11538758873939514\n",
      "\t partial train loss (single batch): 0.122059\n",
      "Epoch 826/1000 train loss: [array(0.12205861, dtype=float32)], val loss: 0.12019506096839905\n",
      "\t partial train loss (single batch): 0.120275\n",
      "Epoch 827/1000 train loss: [array(0.12027468, dtype=float32)], val loss: 0.11790012568235397\n",
      "\t partial train loss (single batch): 0.120330\n",
      "Epoch 828/1000 train loss: [array(0.12033023, dtype=float32)], val loss: 0.11826661229133606\n",
      "\t partial train loss (single batch): 0.119333\n",
      "Epoch 829/1000 train loss: [array(0.11933313, dtype=float32)], val loss: 0.11837138235569\n",
      "\t partial train loss (single batch): 0.121210\n",
      "Epoch 830/1000 train loss: [array(0.12120968, dtype=float32)], val loss: 0.12006375938653946\n",
      "\t partial train loss (single batch): 0.118946\n",
      "Epoch 831/1000 train loss: [array(0.11894626, dtype=float32)], val loss: 0.11669909209012985\n",
      "\t partial train loss (single batch): 0.118461\n",
      "Epoch 832/1000 train loss: [array(0.11846073, dtype=float32)], val loss: 0.1178402230143547\n",
      "\t partial train loss (single batch): 0.121037\n",
      "Epoch 833/1000 train loss: [array(0.12103657, dtype=float32)], val loss: 0.11668726801872253\n",
      "\t partial train loss (single batch): 0.119552\n",
      "Epoch 834/1000 train loss: [array(0.11955179, dtype=float32)], val loss: 0.11551839113235474\n",
      "\t partial train loss (single batch): 0.115810\n",
      "Epoch 835/1000 train loss: [array(0.11580987, dtype=float32)], val loss: 0.1163165420293808\n",
      "\t partial train loss (single batch): 0.120678\n",
      "Epoch 836/1000 train loss: [array(0.1206784, dtype=float32)], val loss: 0.1180645301938057\n",
      "\t partial train loss (single batch): 0.117215\n",
      "Epoch 837/1000 train loss: [array(0.11721541, dtype=float32)], val loss: 0.11665444821119308\n",
      "\t partial train loss (single batch): 0.121044\n",
      "Epoch 838/1000 train loss: [array(0.121044, dtype=float32)], val loss: 0.11781840771436691\n",
      "\t partial train loss (single batch): 0.118886\n",
      "Epoch 839/1000 train loss: [array(0.11888594, dtype=float32)], val loss: 0.11765898764133453\n",
      "\t partial train loss (single batch): 0.116958\n",
      "Epoch 840/1000 train loss: [array(0.11695823, dtype=float32)], val loss: 0.11748666316270828\n",
      "\t partial train loss (single batch): 0.118052\n",
      "Epoch 841/1000 train loss: [array(0.11805197, dtype=float32)], val loss: 0.11768361181020737\n",
      "\t partial train loss (single batch): 0.120439\n",
      "Epoch 842/1000 train loss: [array(0.12043925, dtype=float32)], val loss: 0.11465626955032349\n",
      "\t partial train loss (single batch): 0.116037\n",
      "Epoch 843/1000 train loss: [array(0.11603659, dtype=float32)], val loss: 0.1171908974647522\n",
      "\t partial train loss (single batch): 0.122859\n",
      "Epoch 844/1000 train loss: [array(0.12285887, dtype=float32)], val loss: 0.11527735739946365\n",
      "\t partial train loss (single batch): 0.114490\n",
      "Epoch 845/1000 train loss: [array(0.11448987, dtype=float32)], val loss: 0.11502993851900101\n",
      "\t partial train loss (single batch): 0.120326\n",
      "Epoch 846/1000 train loss: [array(0.12032562, dtype=float32)], val loss: 0.11756528913974762\n",
      "\t partial train loss (single batch): 0.114230\n",
      "Epoch 847/1000 train loss: [array(0.11423037, dtype=float32)], val loss: 0.11680632084608078\n",
      "\t partial train loss (single batch): 0.116591\n",
      "Epoch 848/1000 train loss: [array(0.11659063, dtype=float32)], val loss: 0.11755740642547607\n",
      "\t partial train loss (single batch): 0.115350\n",
      "Epoch 849/1000 train loss: [array(0.11535025, dtype=float32)], val loss: 0.11950191855430603\n",
      "\t partial train loss (single batch): 0.120529\n",
      "Epoch 850/1000 train loss: [array(0.12052867, dtype=float32)], val loss: 0.12221301347017288\n",
      "\t partial train loss (single batch): 0.116353\n",
      "Epoch 851/1000 train loss: [array(0.11635279, dtype=float32)], val loss: 0.11691831797361374\n",
      "\t partial train loss (single batch): 0.119422\n",
      "Epoch 852/1000 train loss: [array(0.11942202, dtype=float32)], val loss: 0.11659955978393555\n",
      "\t partial train loss (single batch): 0.116766\n",
      "Epoch 853/1000 train loss: [array(0.11676626, dtype=float32)], val loss: 0.12030491232872009\n",
      "\t partial train loss (single batch): 0.119158\n",
      "Epoch 854/1000 train loss: [array(0.11915755, dtype=float32)], val loss: 0.117816261947155\n",
      "\t partial train loss (single batch): 0.117706\n",
      "Epoch 855/1000 train loss: [array(0.11770634, dtype=float32)], val loss: 0.11909373104572296\n",
      "\t partial train loss (single batch): 0.115386\n",
      "Epoch 856/1000 train loss: [array(0.11538559, dtype=float32)], val loss: 0.12438450753688812\n",
      "\t partial train loss (single batch): 0.119809\n",
      "Epoch 857/1000 train loss: [array(0.11980926, dtype=float32)], val loss: 0.11781186610460281\n",
      "\t partial train loss (single batch): 0.116374\n",
      "Epoch 858/1000 train loss: [array(0.11637352, dtype=float32)], val loss: 0.1214623749256134\n",
      "\t partial train loss (single batch): 0.123210\n",
      "Epoch 859/1000 train loss: [array(0.12321013, dtype=float32)], val loss: 0.12267882376909256\n",
      "\t partial train loss (single batch): 0.118736\n",
      "Epoch 860/1000 train loss: [array(0.11873585, dtype=float32)], val loss: 0.11698253452777863\n",
      "\t partial train loss (single batch): 0.120278\n",
      "Epoch 861/1000 train loss: [array(0.12027811, dtype=float32)], val loss: 0.11697350442409515\n",
      "\t partial train loss (single batch): 0.115507\n",
      "Epoch 862/1000 train loss: [array(0.11550707, dtype=float32)], val loss: 0.11518879979848862\n",
      "\t partial train loss (single batch): 0.119199\n",
      "Epoch 863/1000 train loss: [array(0.11919915, dtype=float32)], val loss: 0.11540348827838898\n",
      "\t partial train loss (single batch): 0.116263\n",
      "Epoch 864/1000 train loss: [array(0.11626323, dtype=float32)], val loss: 0.11574980616569519\n",
      "\t partial train loss (single batch): 0.120232\n",
      "Epoch 865/1000 train loss: [array(0.12023204, dtype=float32)], val loss: 0.11221200227737427\n",
      "\t partial train loss (single batch): 0.114615\n",
      "Epoch 866/1000 train loss: [array(0.11461538, dtype=float32)], val loss: 0.11612924933433533\n",
      "\t partial train loss (single batch): 0.117770\n",
      "Epoch 867/1000 train loss: [array(0.1177702, dtype=float32)], val loss: 0.11405236274003983\n",
      "\t partial train loss (single batch): 0.116726\n",
      "Epoch 868/1000 train loss: [array(0.11672633, dtype=float32)], val loss: 0.109169140458107\n",
      "\t partial train loss (single batch): 0.119616\n",
      "Epoch 869/1000 train loss: [array(0.11961629, dtype=float32)], val loss: 0.1217045858502388\n",
      "\t partial train loss (single batch): 0.119181\n",
      "Epoch 870/1000 train loss: [array(0.11918083, dtype=float32)], val loss: 0.11281709372997284\n",
      "\t partial train loss (single batch): 0.119766\n",
      "Epoch 871/1000 train loss: [array(0.11976621, dtype=float32)], val loss: 0.11630179733037949\n",
      "\t partial train loss (single batch): 0.115998\n",
      "Epoch 872/1000 train loss: [array(0.11599826, dtype=float32)], val loss: 0.11779773235321045\n",
      "\t partial train loss (single batch): 0.114389\n",
      "Epoch 873/1000 train loss: [array(0.11438907, dtype=float32)], val loss: 0.11972329765558243\n",
      "\t partial train loss (single batch): 0.120461\n",
      "Epoch 874/1000 train loss: [array(0.12046126, dtype=float32)], val loss: 0.11329663544893265\n",
      "\t partial train loss (single batch): 0.116190\n",
      "Epoch 875/1000 train loss: [array(0.11619003, dtype=float32)], val loss: 0.11505463719367981\n",
      "\t partial train loss (single batch): 0.118570\n",
      "Epoch 876/1000 train loss: [array(0.11857035, dtype=float32)], val loss: 0.11194542795419693\n",
      "\t partial train loss (single batch): 0.116753\n",
      "Epoch 877/1000 train loss: [array(0.11675335, dtype=float32)], val loss: 0.11916830390691757\n",
      "\t partial train loss (single batch): 0.118878\n",
      "Epoch 878/1000 train loss: [array(0.11887772, dtype=float32)], val loss: 0.11851701140403748\n",
      "\t partial train loss (single batch): 0.117680\n",
      "Epoch 879/1000 train loss: [array(0.11768005, dtype=float32)], val loss: 0.10907478630542755\n",
      "\t partial train loss (single batch): 0.113293\n",
      "Epoch 880/1000 train loss: [array(0.11329272, dtype=float32)], val loss: 0.11757205426692963\n",
      "\t partial train loss (single batch): 0.113126\n",
      "Epoch 881/1000 train loss: [array(0.11312559, dtype=float32)], val loss: 0.1203731894493103\n",
      "\t partial train loss (single batch): 0.117701\n",
      "Epoch 882/1000 train loss: [array(0.11770135, dtype=float32)], val loss: 0.11630529165267944\n",
      "\t partial train loss (single batch): 0.116332\n",
      "Epoch 883/1000 train loss: [array(0.11633158, dtype=float32)], val loss: 0.11598892509937286\n",
      "\t partial train loss (single batch): 0.115646\n",
      "Epoch 884/1000 train loss: [array(0.11564603, dtype=float32)], val loss: 0.11946189403533936\n",
      "\t partial train loss (single batch): 0.113875\n",
      "Epoch 885/1000 train loss: [array(0.11387521, dtype=float32)], val loss: 0.11521870642900467\n",
      "\t partial train loss (single batch): 0.118688\n",
      "Epoch 886/1000 train loss: [array(0.11868789, dtype=float32)], val loss: 0.11481916904449463\n",
      "\t partial train loss (single batch): 0.116328\n",
      "Epoch 887/1000 train loss: [array(0.11632795, dtype=float32)], val loss: 0.11578492820262909\n",
      "\t partial train loss (single batch): 0.116040\n",
      "Epoch 888/1000 train loss: [array(0.11604013, dtype=float32)], val loss: 0.11520915478467941\n",
      "\t partial train loss (single batch): 0.116867\n",
      "Epoch 889/1000 train loss: [array(0.11686701, dtype=float32)], val loss: 0.11871957033872604\n",
      "\t partial train loss (single batch): 0.117305\n",
      "Epoch 890/1000 train loss: [array(0.11730505, dtype=float32)], val loss: 0.11161352694034576\n",
      "\t partial train loss (single batch): 0.112552\n",
      "Epoch 891/1000 train loss: [array(0.11255153, dtype=float32)], val loss: 0.1196867898106575\n",
      "\t partial train loss (single batch): 0.116125\n",
      "Epoch 892/1000 train loss: [array(0.11612479, dtype=float32)], val loss: 0.11798067390918732\n",
      "\t partial train loss (single batch): 0.117380\n",
      "Epoch 893/1000 train loss: [array(0.11737983, dtype=float32)], val loss: 0.11791791766881943\n",
      "\t partial train loss (single batch): 0.114852\n",
      "Epoch 894/1000 train loss: [array(0.11485217, dtype=float32)], val loss: 0.11194536834955215\n",
      "\t partial train loss (single batch): 0.115807\n",
      "Epoch 895/1000 train loss: [array(0.11580662, dtype=float32)], val loss: 0.11316592991352081\n",
      "\t partial train loss (single batch): 0.117125\n",
      "Epoch 896/1000 train loss: [array(0.11712472, dtype=float32)], val loss: 0.11606912314891815\n",
      "\t partial train loss (single batch): 0.115648\n",
      "Epoch 897/1000 train loss: [array(0.11564757, dtype=float32)], val loss: 0.11355370283126831\n",
      "\t partial train loss (single batch): 0.116445\n",
      "Epoch 898/1000 train loss: [array(0.11644462, dtype=float32)], val loss: 0.1171688362956047\n",
      "\t partial train loss (single batch): 0.116843\n",
      "Epoch 899/1000 train loss: [array(0.11684307, dtype=float32)], val loss: 0.11517204344272614\n",
      "\t partial train loss (single batch): 0.117314\n",
      "Epoch 900/1000 train loss: [array(0.11731414, dtype=float32)], val loss: 0.11563427746295929\n",
      "\t partial train loss (single batch): 0.116295\n",
      "Epoch 901/1000 train loss: [array(0.11629512, dtype=float32)], val loss: 0.11759501695632935\n",
      "\t partial train loss (single batch): 0.119197\n",
      "Epoch 902/1000 train loss: [array(0.11919736, dtype=float32)], val loss: 0.11575428396463394\n",
      "\t partial train loss (single batch): 0.118879\n",
      "Epoch 903/1000 train loss: [array(0.11887854, dtype=float32)], val loss: 0.12350349873304367\n",
      "\t partial train loss (single batch): 0.118322\n",
      "Epoch 904/1000 train loss: [array(0.11832234, dtype=float32)], val loss: 0.11796028912067413\n",
      "\t partial train loss (single batch): 0.115621\n",
      "Epoch 905/1000 train loss: [array(0.11562051, dtype=float32)], val loss: 0.11014057695865631\n",
      "\t partial train loss (single batch): 0.117830\n",
      "Epoch 906/1000 train loss: [array(0.11783033, dtype=float32)], val loss: 0.11335653066635132\n",
      "\t partial train loss (single batch): 0.116661\n",
      "Epoch 907/1000 train loss: [array(0.11666097, dtype=float32)], val loss: 0.11221640557050705\n",
      "\t partial train loss (single batch): 0.114842\n",
      "Epoch 908/1000 train loss: [array(0.11484229, dtype=float32)], val loss: 0.11507871001958847\n",
      "\t partial train loss (single batch): 0.121067\n",
      "Epoch 909/1000 train loss: [array(0.12106702, dtype=float32)], val loss: 0.11468252539634705\n",
      "\t partial train loss (single batch): 0.116980\n",
      "Epoch 910/1000 train loss: [array(0.11698017, dtype=float32)], val loss: 0.11497823894023895\n",
      "\t partial train loss (single batch): 0.115885\n",
      "Epoch 911/1000 train loss: [array(0.11588542, dtype=float32)], val loss: 0.11749153584241867\n",
      "\t partial train loss (single batch): 0.117553\n",
      "Epoch 912/1000 train loss: [array(0.1175533, dtype=float32)], val loss: 0.11267516016960144\n",
      "\t partial train loss (single batch): 0.119707\n",
      "Epoch 913/1000 train loss: [array(0.11970736, dtype=float32)], val loss: 0.11427795886993408\n",
      "\t partial train loss (single batch): 0.112998\n",
      "Epoch 914/1000 train loss: [array(0.11299771, dtype=float32)], val loss: 0.1169765368103981\n",
      "\t partial train loss (single batch): 0.114386\n",
      "Epoch 915/1000 train loss: [array(0.11438609, dtype=float32)], val loss: 0.12031683325767517\n",
      "\t partial train loss (single batch): 0.117917\n",
      "Epoch 916/1000 train loss: [array(0.11791714, dtype=float32)], val loss: 0.11114814132452011\n",
      "\t partial train loss (single batch): 0.113858\n",
      "Epoch 917/1000 train loss: [array(0.11385793, dtype=float32)], val loss: 0.11511057615280151\n",
      "\t partial train loss (single batch): 0.118353\n",
      "Epoch 918/1000 train loss: [array(0.1183534, dtype=float32)], val loss: 0.11810709536075592\n",
      "\t partial train loss (single batch): 0.111053\n",
      "Epoch 919/1000 train loss: [array(0.11105289, dtype=float32)], val loss: 0.1179111897945404\n",
      "\t partial train loss (single batch): 0.114763\n",
      "Epoch 920/1000 train loss: [array(0.11476287, dtype=float32)], val loss: 0.11699974536895752\n",
      "\t partial train loss (single batch): 0.112589\n",
      "Epoch 921/1000 train loss: [array(0.11258905, dtype=float32)], val loss: 0.12001828849315643\n",
      "\t partial train loss (single batch): 0.119497\n",
      "Epoch 922/1000 train loss: [array(0.11949662, dtype=float32)], val loss: 0.1144888624548912\n",
      "\t partial train loss (single batch): 0.113949\n",
      "Epoch 923/1000 train loss: [array(0.11394867, dtype=float32)], val loss: 0.11271551996469498\n",
      "\t partial train loss (single batch): 0.117936\n",
      "Epoch 924/1000 train loss: [array(0.11793573, dtype=float32)], val loss: 0.11294344812631607\n",
      "\t partial train loss (single batch): 0.115154\n",
      "Epoch 925/1000 train loss: [array(0.11515421, dtype=float32)], val loss: 0.11194613575935364\n",
      "\t partial train loss (single batch): 0.117901\n",
      "Epoch 926/1000 train loss: [array(0.11790056, dtype=float32)], val loss: 0.11519452184438705\n",
      "\t partial train loss (single batch): 0.118630\n",
      "Epoch 927/1000 train loss: [array(0.11862992, dtype=float32)], val loss: 0.11355874687433243\n",
      "\t partial train loss (single batch): 0.113247\n",
      "Epoch 928/1000 train loss: [array(0.11324686, dtype=float32)], val loss: 0.12012258172035217\n",
      "\t partial train loss (single batch): 0.115414\n",
      "Epoch 929/1000 train loss: [array(0.11541399, dtype=float32)], val loss: 0.11645320057868958\n",
      "\t partial train loss (single batch): 0.118082\n",
      "Epoch 930/1000 train loss: [array(0.11808197, dtype=float32)], val loss: 0.11126623302698135\n",
      "\t partial train loss (single batch): 0.113709\n",
      "Epoch 931/1000 train loss: [array(0.11370903, dtype=float32)], val loss: 0.12070639431476593\n",
      "\t partial train loss (single batch): 0.116110\n",
      "Epoch 932/1000 train loss: [array(0.11610991, dtype=float32)], val loss: 0.11804003268480301\n",
      "\t partial train loss (single batch): 0.118074\n",
      "Epoch 933/1000 train loss: [array(0.1180735, dtype=float32)], val loss: 0.11127371340990067\n",
      "\t partial train loss (single batch): 0.117759\n",
      "Epoch 934/1000 train loss: [array(0.1177587, dtype=float32)], val loss: 0.1147729828953743\n",
      "\t partial train loss (single batch): 0.115418\n",
      "Epoch 935/1000 train loss: [array(0.1154183, dtype=float32)], val loss: 0.11440224200487137\n",
      "\t partial train loss (single batch): 0.115496\n",
      "Epoch 936/1000 train loss: [array(0.11549649, dtype=float32)], val loss: 0.11151039600372314\n",
      "\t partial train loss (single batch): 0.113244\n",
      "Epoch 937/1000 train loss: [array(0.11324403, dtype=float32)], val loss: 0.11507680267095566\n",
      "\t partial train loss (single batch): 0.113782\n",
      "Epoch 938/1000 train loss: [array(0.11378177, dtype=float32)], val loss: 0.11365988105535507\n",
      "\t partial train loss (single batch): 0.113451\n",
      "Epoch 939/1000 train loss: [array(0.11345068, dtype=float32)], val loss: 0.11325009167194366\n",
      "\t partial train loss (single batch): 0.111750\n",
      "Epoch 940/1000 train loss: [array(0.11174993, dtype=float32)], val loss: 0.11466382443904877\n",
      "\t partial train loss (single batch): 0.112781\n",
      "Epoch 941/1000 train loss: [array(0.11278128, dtype=float32)], val loss: 0.11171796172857285\n",
      "\t partial train loss (single batch): 0.114702\n",
      "Epoch 942/1000 train loss: [array(0.11470225, dtype=float32)], val loss: 0.11147300899028778\n",
      "\t partial train loss (single batch): 0.114313\n",
      "Epoch 943/1000 train loss: [array(0.11431336, dtype=float32)], val loss: 0.10959959030151367\n",
      "\t partial train loss (single batch): 0.118417\n",
      "Epoch 944/1000 train loss: [array(0.11841697, dtype=float32)], val loss: 0.11291945725679398\n",
      "\t partial train loss (single batch): 0.116008\n",
      "Epoch 945/1000 train loss: [array(0.11600818, dtype=float32)], val loss: 0.11682236939668655\n",
      "\t partial train loss (single batch): 0.116938\n",
      "Epoch 946/1000 train loss: [array(0.11693761, dtype=float32)], val loss: 0.1125403493642807\n",
      "\t partial train loss (single batch): 0.116605\n",
      "Epoch 947/1000 train loss: [array(0.11660508, dtype=float32)], val loss: 0.11487989127635956\n",
      "\t partial train loss (single batch): 0.119760\n",
      "Epoch 948/1000 train loss: [array(0.11976044, dtype=float32)], val loss: 0.1135815680027008\n",
      "\t partial train loss (single batch): 0.114421\n",
      "Epoch 949/1000 train loss: [array(0.11442147, dtype=float32)], val loss: 0.1060669869184494\n",
      "\t partial train loss (single batch): 0.114314\n",
      "Epoch 950/1000 train loss: [array(0.11431411, dtype=float32)], val loss: 0.10976050794124603\n",
      "\t partial train loss (single batch): 0.113509\n",
      "Epoch 951/1000 train loss: [array(0.1135087, dtype=float32)], val loss: 0.1081870049238205\n",
      "\t partial train loss (single batch): 0.113208\n",
      "Epoch 952/1000 train loss: [array(0.11320805, dtype=float32)], val loss: 0.11724402010440826\n",
      "\t partial train loss (single batch): 0.113137\n",
      "Epoch 953/1000 train loss: [array(0.11313686, dtype=float32)], val loss: 0.11237388104200363\n",
      "\t partial train loss (single batch): 0.112506\n",
      "Epoch 954/1000 train loss: [array(0.11250564, dtype=float32)], val loss: 0.11464744806289673\n",
      "\t partial train loss (single batch): 0.112316\n",
      "Epoch 955/1000 train loss: [array(0.11231585, dtype=float32)], val loss: 0.11464208364486694\n",
      "\t partial train loss (single batch): 0.111815\n",
      "Epoch 956/1000 train loss: [array(0.11181485, dtype=float32)], val loss: 0.11548233032226562\n",
      "\t partial train loss (single batch): 0.110758\n",
      "Epoch 957/1000 train loss: [array(0.11075798, dtype=float32)], val loss: 0.1107390969991684\n",
      "\t partial train loss (single batch): 0.118196\n",
      "Epoch 958/1000 train loss: [array(0.11819596, dtype=float32)], val loss: 0.11152171343564987\n",
      "\t partial train loss (single batch): 0.118681\n",
      "Epoch 959/1000 train loss: [array(0.11868054, dtype=float32)], val loss: 0.1139087975025177\n",
      "\t partial train loss (single batch): 0.113310\n",
      "Epoch 960/1000 train loss: [array(0.11330996, dtype=float32)], val loss: 0.10840782523155212\n",
      "\t partial train loss (single batch): 0.110872\n",
      "Epoch 961/1000 train loss: [array(0.11087172, dtype=float32)], val loss: 0.11693241447210312\n",
      "\t partial train loss (single batch): 0.117034\n",
      "Epoch 962/1000 train loss: [array(0.11703427, dtype=float32)], val loss: 0.11270999908447266\n",
      "\t partial train loss (single batch): 0.112455\n",
      "Epoch 963/1000 train loss: [array(0.1124548, dtype=float32)], val loss: 0.11247304826974869\n",
      "\t partial train loss (single batch): 0.110950\n",
      "Epoch 964/1000 train loss: [array(0.11094977, dtype=float32)], val loss: 0.1164548322558403\n",
      "\t partial train loss (single batch): 0.116993\n",
      "Epoch 965/1000 train loss: [array(0.11699348, dtype=float32)], val loss: 0.11063630878925323\n",
      "\t partial train loss (single batch): 0.114627\n",
      "Epoch 966/1000 train loss: [array(0.11462691, dtype=float32)], val loss: 0.1102215051651001\n",
      "\t partial train loss (single batch): 0.113060\n",
      "Epoch 967/1000 train loss: [array(0.11306006, dtype=float32)], val loss: 0.11482877284288406\n",
      "\t partial train loss (single batch): 0.113456\n",
      "Epoch 968/1000 train loss: [array(0.11345634, dtype=float32)], val loss: 0.10752928256988525\n",
      "\t partial train loss (single batch): 0.114199\n",
      "Epoch 969/1000 train loss: [array(0.11419903, dtype=float32)], val loss: 0.11221709847450256\n",
      "\t partial train loss (single batch): 0.112281\n",
      "Epoch 970/1000 train loss: [array(0.11228091, dtype=float32)], val loss: 0.11167869716882706\n",
      "\t partial train loss (single batch): 0.120872\n",
      "Epoch 971/1000 train loss: [array(0.1208723, dtype=float32)], val loss: 0.11686095595359802\n",
      "\t partial train loss (single batch): 0.112750\n",
      "Epoch 972/1000 train loss: [array(0.11275034, dtype=float32)], val loss: 0.1083751767873764\n",
      "\t partial train loss (single batch): 0.112465\n",
      "Epoch 973/1000 train loss: [array(0.11246455, dtype=float32)], val loss: 0.11066493391990662\n",
      "\t partial train loss (single batch): 0.116236\n",
      "Epoch 974/1000 train loss: [array(0.11623574, dtype=float32)], val loss: 0.11472660303115845\n",
      "\t partial train loss (single batch): 0.114489\n",
      "Epoch 975/1000 train loss: [array(0.11448882, dtype=float32)], val loss: 0.10835158824920654\n",
      "\t partial train loss (single batch): 0.112194\n",
      "Epoch 976/1000 train loss: [array(0.11219361, dtype=float32)], val loss: 0.11325308680534363\n",
      "\t partial train loss (single batch): 0.113176\n",
      "Epoch 977/1000 train loss: [array(0.11317613, dtype=float32)], val loss: 0.11519268900156021\n",
      "\t partial train loss (single batch): 0.112502\n",
      "Epoch 978/1000 train loss: [array(0.11250182, dtype=float32)], val loss: 0.11514091491699219\n",
      "\t partial train loss (single batch): 0.119103\n",
      "Epoch 979/1000 train loss: [array(0.11910298, dtype=float32)], val loss: 0.11119059473276138\n",
      "\t partial train loss (single batch): 0.115736\n",
      "Epoch 980/1000 train loss: [array(0.11573566, dtype=float32)], val loss: 0.11336888372898102\n",
      "\t partial train loss (single batch): 0.113172\n",
      "Epoch 981/1000 train loss: [array(0.11317164, dtype=float32)], val loss: 0.10980421304702759\n",
      "\t partial train loss (single batch): 0.117242\n",
      "Epoch 982/1000 train loss: [array(0.11724227, dtype=float32)], val loss: 0.11424831300973892\n",
      "\t partial train loss (single batch): 0.112675\n",
      "Epoch 983/1000 train loss: [array(0.11267462, dtype=float32)], val loss: 0.111238032579422\n",
      "\t partial train loss (single batch): 0.114884\n",
      "Epoch 984/1000 train loss: [array(0.11488375, dtype=float32)], val loss: 0.10574819892644882\n",
      "\t partial train loss (single batch): 0.114003\n",
      "Epoch 985/1000 train loss: [array(0.11400265, dtype=float32)], val loss: 0.11386553943157196\n",
      "\t partial train loss (single batch): 0.111135\n",
      "Epoch 986/1000 train loss: [array(0.11113549, dtype=float32)], val loss: 0.11221287399530411\n",
      "\t partial train loss (single batch): 0.111941\n",
      "Epoch 987/1000 train loss: [array(0.11194102, dtype=float32)], val loss: 0.11178791522979736\n",
      "\t partial train loss (single batch): 0.107086\n",
      "Epoch 988/1000 train loss: [array(0.1070864, dtype=float32)], val loss: 0.11242139339447021\n",
      "\t partial train loss (single batch): 0.118245\n",
      "Epoch 989/1000 train loss: [array(0.11824493, dtype=float32)], val loss: 0.11371562629938126\n",
      "\t partial train loss (single batch): 0.112635\n",
      "Epoch 990/1000 train loss: [array(0.11263517, dtype=float32)], val loss: 0.10944685339927673\n",
      "\t partial train loss (single batch): 0.111662\n",
      "Epoch 991/1000 train loss: [array(0.11166175, dtype=float32)], val loss: 0.1115444079041481\n",
      "\t partial train loss (single batch): 0.116701\n",
      "Epoch 992/1000 train loss: [array(0.11670123, dtype=float32)], val loss: 0.11775993555784225\n",
      "\t partial train loss (single batch): 0.113372\n",
      "Epoch 993/1000 train loss: [array(0.11337224, dtype=float32)], val loss: 0.1130228117108345\n",
      "\t partial train loss (single batch): 0.117331\n",
      "Epoch 994/1000 train loss: [array(0.11733085, dtype=float32)], val loss: 0.11200258880853653\n",
      "\t partial train loss (single batch): 0.111631\n",
      "Epoch 995/1000 train loss: [array(0.11163101, dtype=float32)], val loss: 0.11254570633172989\n",
      "\t partial train loss (single batch): 0.115011\n",
      "Epoch 996/1000 train loss: [array(0.11501087, dtype=float32)], val loss: 0.11663369089365005\n",
      "\t partial train loss (single batch): 0.111672\n",
      "Epoch 997/1000 train loss: [array(0.11167233, dtype=float32)], val loss: 0.11349697411060333\n",
      "\t partial train loss (single batch): 0.109249\n",
      "Epoch 998/1000 train loss: [array(0.10924898, dtype=float32)], val loss: 0.11408820748329163\n",
      "\t partial train loss (single batch): 0.115261\n",
      "Epoch 999/1000 train loss: [array(0.11526095, dtype=float32)], val loss: 0.11108971387147903\n",
      "\t partial train loss (single batch): 0.114165\n",
      "Epoch 1000/1000 train loss: [array(0.11416537, dtype=float32)], val loss: 0.11300583928823471\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import random \n",
    "num_epochs = 1000\n",
    "history={'train_loss':[],'val_loss':[]}\n",
    "for epoch in range(num_epochs):\n",
    "   img_train, labels_train = generator.get_random_batch(training=True)\n",
    "   img_test, labels_test = generator.get_random_batch(training=False)\n",
    "   #print(img_train[0])\n",
    "   #img = img_train[0]\n",
    "   #plt.imshow(np.array(img).reshape((28, 28)))\n",
    "\n",
    "\n",
    "   img_train = torch.tensor(img_train)\n",
    "   img_train = img_train.type(torch.FloatTensor)\n",
    "   img_test = torch.tensor(img_test)\n",
    "   img_test = img_test.type(torch.FloatTensor)\n",
    "   train_loss = train_epoch(encoder,decoder,loss_fn,optim, img_train)\n",
    "   val_loss = test_epoch(encoder,decoder,loss_fn, img_test)\n",
    "   print(f\"Epoch {epoch+1}/{num_epochs} train loss: {train_loss}, val loss: {val_loss}\")\n",
    "   #print('\\n EPOCH {}/{} \\t train loss {:.3f} \\t val loss {:.3f}'.format(epoch + 1, num_epochs,train_loss,val_loss))\n",
    "   history['train_loss'].append(train_loss)\n",
    "   history['val_loss'].append(val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "plt.semilogy(history['train_loss'], label='Train')\n",
    "plt.semilogy(history['val_loss'], label='Valid')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Average Loss')\n",
    "#plt.grid()\n",
    "plt.legend()\n",
    "#plt.title('loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1440x288 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "z = np.random.randn(100, 8)\n",
    "z = torch.Tensor(z)\n",
    "with torch.no_grad():\n",
    "    x_hat = decoder(z)\n",
    "n = 10\n",
    "plt.figure(figsize=(20, 4))\n",
    "for i in range(1, n + 1):\n",
    "    # Display original\n",
    "    ax = plt.subplot(2, n, i)\n",
    "    plt.imshow(np.array(x_hat[i]).reshape((28, 28)))\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-26 13:16:38.226882: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/usr/local/anaconda3/envs/tensorflow/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'generator' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/w2/4g3c4yrn38g2nwwdhjdlx6fc0000gn/T/ipykernel_13877/2529427843.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mverification_net\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVerificationNet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVerificationNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mforce_learn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"../models/verification_model\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_random_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'generator' is not defined"
     ]
    }
   ],
   "source": [
    "from verification_net import VerificationNet\n",
    "net = VerificationNet(force_learn=False, file_name=\"../models/verification_model\")\n",
    "img, labels = generator.get_random_batch(training=False, batch_size=1000)\n",
    "x = torch.Tensor(img)\n",
    "x = x.type(torch.FloatTensor)\n",
    "\n",
    "with torch.no_grad():\n",
    "    auto_encoded_imgs = np.array(decoder(encoder(x)))\n",
    "auto_encoded_imgs = auto_encoded_imgs.swapaxes(1, 2)\n",
    "auto_encoded_imgs = auto_encoded_imgs.swapaxes(2, 3)\n",
    "img = img.swapaxes(1, 2)\n",
    "img = img.swapaxes(2, 3)\n",
    "\n",
    "_, acc_ae = net.check_predictability(data=auto_encoded_imgs, correct_labels=labels)\n",
    "print(f\"Accuracy AE: {100 * acc_ae:.2f}%\")\n",
    "_, acc_orig = net.check_predictability(data=img, correct_labels=labels)\n",
    "print(f\"Accuracy Original: {100 * acc_orig:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "49ff9a1d55cbad36b515c3ded8837e12145fab330794be4b4ac6e95d3772d975"
  },
  "kernelspec": {
   "display_name": "Python 3.8.0 ('tensorflow')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

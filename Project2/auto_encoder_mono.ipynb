{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "from stacked_mnist import StackedMNISTData, DataMode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, encoded_space_dim, fc2_input_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        ### Convolutional section\n",
    "        self.encoder_cnn = nn.Sequential(\n",
    "            # First convolutional layer\n",
    "            nn.Conv2d(1, 8, 3, stride=2, padding=1),\n",
    "            # nn.BatchNorm2d(8),\n",
    "            nn.ReLU(True),\n",
    "            # Second convolutional layer\n",
    "            nn.Conv2d(8, 16, 3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(True),\n",
    "            # Third convolutional layer\n",
    "            nn.Conv2d(16, 32, 3, stride=2, padding=0),\n",
    "            # nn.BatchNorm2d(32),\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "\n",
    "        ### Flatten layer\n",
    "        self.flatten = nn.Flatten(start_dim=1)\n",
    "\n",
    "        ### Linear section\n",
    "        self.encoder_lin = nn.Sequential(\n",
    "            # First linear layer\n",
    "            nn.Linear(3 * 3 * 32, 128),\n",
    "            nn.ReLU(True),\n",
    "            # Second linear layer\n",
    "            nn.Linear(128, encoded_space_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply convolutions\n",
    "        x = self.encoder_cnn(x)\n",
    "        # Flatten\n",
    "        x = self.flatten(x)\n",
    "        # # Apply linear layers\n",
    "        x = self.encoder_lin(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, encoded_space_dim,fc2_input_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        ### Linear section\n",
    "        self.decoder_lin = nn.Sequential(\n",
    "            # First linear layer\n",
    "            nn.Linear(encoded_space_dim, 128),\n",
    "            nn.ReLU(True),\n",
    "            # Second linear layer\n",
    "            nn.Linear(128, 3 * 3 * 32),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "\n",
    "        ### Unflatten\n",
    "        self.unflatten = nn.Unflatten(dim=1, unflattened_size=(32, 3, 3))\n",
    "\n",
    "        ### Convolutional section\n",
    "        self.decoder_conv = nn.Sequential(\n",
    "            # First transposed convolution\n",
    "            nn.ConvTranspose2d(32, 16, 3, stride=2, output_padding=0),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(True),\n",
    "            # Second transposed convolution\n",
    "            nn.ConvTranspose2d(16, 8, 3, stride=2, padding=1, output_padding=1),\n",
    "            nn.BatchNorm2d(8),\n",
    "            nn.ReLU(True),\n",
    "            # Third transposed convolution\n",
    "            nn.ConvTranspose2d(8, 1, 3, stride=2, padding=1, output_padding=1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Apply linear layers\n",
    "        x = self.decoder_lin(x)\n",
    "        # Unflatten\n",
    "        x = self.unflatten(x)\n",
    "        # Apply transposed convolutions\n",
    "        x = self.decoder_conv(x)\n",
    "        # Apply a sigmoid to force the output to be between 0 and 1 (valid pixel values)\n",
    "        x = torch.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = 8\n",
    "encoder = Encoder(encoded_space_dim=d,fc2_input_dim=128)\n",
    "decoder = Decoder(encoded_space_dim=d,fc2_input_dim=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.BCELoss()\n",
    "lr= 0.001\n",
    "params_to_optimize = [\n",
    "    {'params': encoder.parameters()},\n",
    "    {'params': decoder.parameters()}\n",
    "]\n",
    "optim = torch.optim.Adam(params_to_optimize, lr=lr, weight_decay=1e-05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Training function\n",
    "def train_epoch(encoder, decoder, loss_fn, optimizer, x):\n",
    "    # Set train mode for both the encoder and the decoder\n",
    "    encoder.train()\n",
    "    decoder.train()\n",
    "    train_loss = []\n",
    "\n",
    "    # Encode data\n",
    "    encoded_data = encoder(x)\n",
    "    # Decode data\n",
    "    decoded_data = decoder(encoded_data)\n",
    "    # Evaluate loss\n",
    "    loss = loss_fn(decoded_data, x)\n",
    "    # Backward pass\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    # Print batch loss\n",
    "    print('\\t partial train loss (single batch): %f' % (loss.data))\n",
    "    train_loss.append(loss.detach().numpy())\n",
    "\n",
    "    return train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Testing function\n",
    "def test_epoch(encoder, decoder, loss_fn, x):\n",
    "    # Set evaluation mode for encoder and decoder\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    with torch.no_grad(): # No need to track the gradients\n",
    "        # Define the lists to store the outputs for each batch\n",
    "        conc_out = []\n",
    "        conc_label = []\n",
    "        # Encode data\n",
    "        encoded_data = encoder(x)\n",
    "        # Decode data\n",
    "        decoded_data = decoder(encoded_data)\n",
    "        # Append the network output and the original image to the lists\n",
    "        conc_out.append(decoded_data)\n",
    "        conc_label.append(x)\n",
    "        # Create a single tensor with all the values in the lists\n",
    "        conc_out = torch.cat(conc_out)\n",
    "        conc_label = torch.cat(conc_label) \n",
    "        # Evaluate global loss\n",
    "        val_loss = loss_fn(conc_out, conc_label)\n",
    "    return val_loss.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = StackedMNISTData(mode=DataMode.MONO_BINARY_COMPLETE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t partial train loss (single batch): 0.347533\n",
      "Epoch 1/1000 train loss: [array(0.3475328, dtype=float32)], val loss: 0.31359922885894775\n",
      "\t partial train loss (single batch): 0.340137\n",
      "Epoch 2/1000 train loss: [array(0.34013706, dtype=float32)], val loss: 0.3117157816886902\n",
      "\t partial train loss (single batch): 0.334783\n",
      "Epoch 3/1000 train loss: [array(0.33478326, dtype=float32)], val loss: 0.3106595575809479\n",
      "\t partial train loss (single batch): 0.328165\n",
      "Epoch 4/1000 train loss: [array(0.32816452, dtype=float32)], val loss: 0.3093099594116211\n",
      "\t partial train loss (single batch): 0.323389\n",
      "Epoch 5/1000 train loss: [array(0.32338902, dtype=float32)], val loss: 0.3082239329814911\n",
      "\t partial train loss (single batch): 0.319580\n",
      "Epoch 6/1000 train loss: [array(0.3195805, dtype=float32)], val loss: 0.3071628510951996\n",
      "\t partial train loss (single batch): 0.316517\n",
      "Epoch 7/1000 train loss: [array(0.31651658, dtype=float32)], val loss: 0.3054042458534241\n",
      "\t partial train loss (single batch): 0.312444\n",
      "Epoch 8/1000 train loss: [array(0.31244352, dtype=float32)], val loss: 0.3047295808792114\n",
      "\t partial train loss (single batch): 0.310438\n",
      "Epoch 9/1000 train loss: [array(0.31043828, dtype=float32)], val loss: 0.30430707335472107\n",
      "\t partial train loss (single batch): 0.307195\n",
      "Epoch 10/1000 train loss: [array(0.3071948, dtype=float32)], val loss: 0.3022320866584778\n",
      "\t partial train loss (single batch): 0.305076\n",
      "Epoch 11/1000 train loss: [array(0.30507636, dtype=float32)], val loss: 0.30224213004112244\n",
      "\t partial train loss (single batch): 0.302197\n",
      "Epoch 12/1000 train loss: [array(0.30219653, dtype=float32)], val loss: 0.3002353310585022\n",
      "\t partial train loss (single batch): 0.300686\n",
      "Epoch 13/1000 train loss: [array(0.30068648, dtype=float32)], val loss: 0.2998347580432892\n",
      "\t partial train loss (single batch): 0.298212\n",
      "Epoch 14/1000 train loss: [array(0.29821205, dtype=float32)], val loss: 0.2989497184753418\n",
      "\t partial train loss (single batch): 0.295629\n",
      "Epoch 15/1000 train loss: [array(0.29562935, dtype=float32)], val loss: 0.2969987392425537\n",
      "\t partial train loss (single batch): 0.294493\n",
      "Epoch 16/1000 train loss: [array(0.2944926, dtype=float32)], val loss: 0.29529842734336853\n",
      "\t partial train loss (single batch): 0.292398\n",
      "Epoch 17/1000 train loss: [array(0.29239774, dtype=float32)], val loss: 0.29399073123931885\n",
      "\t partial train loss (single batch): 0.290308\n",
      "Epoch 18/1000 train loss: [array(0.29030848, dtype=float32)], val loss: 0.29341673851013184\n",
      "\t partial train loss (single batch): 0.287756\n",
      "Epoch 19/1000 train loss: [array(0.2877562, dtype=float32)], val loss: 0.2911117970943451\n",
      "\t partial train loss (single batch): 0.286777\n",
      "Epoch 20/1000 train loss: [array(0.28677672, dtype=float32)], val loss: 0.2904995083808899\n",
      "\t partial train loss (single batch): 0.284729\n",
      "Epoch 21/1000 train loss: [array(0.28472885, dtype=float32)], val loss: 0.28883877396583557\n",
      "\t partial train loss (single batch): 0.283003\n",
      "Epoch 22/1000 train loss: [array(0.28300312, dtype=float32)], val loss: 0.2875039577484131\n",
      "\t partial train loss (single batch): 0.281758\n",
      "Epoch 23/1000 train loss: [array(0.28175756, dtype=float32)], val loss: 0.2858024537563324\n",
      "\t partial train loss (single batch): 0.279278\n",
      "Epoch 24/1000 train loss: [array(0.2792779, dtype=float32)], val loss: 0.2841968834400177\n",
      "\t partial train loss (single batch): 0.277541\n",
      "Epoch 25/1000 train loss: [array(0.2775405, dtype=float32)], val loss: 0.28259387612342834\n",
      "\t partial train loss (single batch): 0.276816\n",
      "Epoch 26/1000 train loss: [array(0.27681592, dtype=float32)], val loss: 0.28095293045043945\n",
      "\t partial train loss (single batch): 0.275648\n",
      "Epoch 27/1000 train loss: [array(0.27564824, dtype=float32)], val loss: 0.27907121181488037\n",
      "\t partial train loss (single batch): 0.273534\n",
      "Epoch 28/1000 train loss: [array(0.27353373, dtype=float32)], val loss: 0.27774113416671753\n",
      "\t partial train loss (single batch): 0.272033\n",
      "Epoch 29/1000 train loss: [array(0.27203348, dtype=float32)], val loss: 0.2761099338531494\n",
      "\t partial train loss (single batch): 0.269889\n",
      "Epoch 30/1000 train loss: [array(0.26988885, dtype=float32)], val loss: 0.2742293179035187\n",
      "\t partial train loss (single batch): 0.269021\n",
      "Epoch 31/1000 train loss: [array(0.26902062, dtype=float32)], val loss: 0.27232497930526733\n",
      "\t partial train loss (single batch): 0.267471\n",
      "Epoch 32/1000 train loss: [array(0.26747116, dtype=float32)], val loss: 0.2708173990249634\n",
      "\t partial train loss (single batch): 0.266035\n",
      "Epoch 33/1000 train loss: [array(0.26603523, dtype=float32)], val loss: 0.26936936378479004\n",
      "\t partial train loss (single batch): 0.263996\n",
      "Epoch 34/1000 train loss: [array(0.26399645, dtype=float32)], val loss: 0.2679089903831482\n",
      "\t partial train loss (single batch): 0.262548\n",
      "Epoch 35/1000 train loss: [array(0.26254836, dtype=float32)], val loss: 0.266160786151886\n",
      "\t partial train loss (single batch): 0.261339\n",
      "Epoch 36/1000 train loss: [array(0.26133928, dtype=float32)], val loss: 0.26450562477111816\n",
      "\t partial train loss (single batch): 0.259489\n",
      "Epoch 37/1000 train loss: [array(0.25948885, dtype=float32)], val loss: 0.2631688714027405\n",
      "\t partial train loss (single batch): 0.258206\n",
      "Epoch 38/1000 train loss: [array(0.25820598, dtype=float32)], val loss: 0.26178833842277527\n",
      "\t partial train loss (single batch): 0.256821\n",
      "Epoch 39/1000 train loss: [array(0.25682122, dtype=float32)], val loss: 0.26051849126815796\n",
      "\t partial train loss (single batch): 0.255327\n",
      "Epoch 40/1000 train loss: [array(0.25532663, dtype=float32)], val loss: 0.25855445861816406\n",
      "\t partial train loss (single batch): 0.253985\n",
      "Epoch 41/1000 train loss: [array(0.25398532, dtype=float32)], val loss: 0.25775614380836487\n",
      "\t partial train loss (single batch): 0.251706\n",
      "Epoch 42/1000 train loss: [array(0.25170597, dtype=float32)], val loss: 0.25643303990364075\n",
      "\t partial train loss (single batch): 0.250387\n",
      "Epoch 43/1000 train loss: [array(0.25038734, dtype=float32)], val loss: 0.25509533286094666\n",
      "\t partial train loss (single batch): 0.249170\n",
      "Epoch 44/1000 train loss: [array(0.24917042, dtype=float32)], val loss: 0.25350937247276306\n",
      "\t partial train loss (single batch): 0.247191\n",
      "Epoch 45/1000 train loss: [array(0.24719132, dtype=float32)], val loss: 0.2522790729999542\n",
      "\t partial train loss (single batch): 0.245737\n",
      "Epoch 46/1000 train loss: [array(0.24573688, dtype=float32)], val loss: 0.25102463364601135\n",
      "\t partial train loss (single batch): 0.244441\n",
      "Epoch 47/1000 train loss: [array(0.24444053, dtype=float32)], val loss: 0.24916641414165497\n",
      "\t partial train loss (single batch): 0.242535\n",
      "Epoch 48/1000 train loss: [array(0.24253477, dtype=float32)], val loss: 0.24825583398342133\n",
      "\t partial train loss (single batch): 0.241128\n",
      "Epoch 49/1000 train loss: [array(0.24112828, dtype=float32)], val loss: 0.2471168488264084\n",
      "\t partial train loss (single batch): 0.239715\n",
      "Epoch 50/1000 train loss: [array(0.23971483, dtype=float32)], val loss: 0.24544671177864075\n",
      "\t partial train loss (single batch): 0.238130\n",
      "Epoch 51/1000 train loss: [array(0.23812985, dtype=float32)], val loss: 0.24414248764514923\n",
      "\t partial train loss (single batch): 0.236438\n",
      "Epoch 52/1000 train loss: [array(0.2364383, dtype=float32)], val loss: 0.24255764484405518\n",
      "\t partial train loss (single batch): 0.235284\n",
      "Epoch 53/1000 train loss: [array(0.23528442, dtype=float32)], val loss: 0.24137096107006073\n",
      "\t partial train loss (single batch): 0.233121\n",
      "Epoch 54/1000 train loss: [array(0.23312117, dtype=float32)], val loss: 0.23962447047233582\n",
      "\t partial train loss (single batch): 0.231391\n",
      "Epoch 55/1000 train loss: [array(0.23139122, dtype=float32)], val loss: 0.23816831409931183\n",
      "\t partial train loss (single batch): 0.229893\n",
      "Epoch 56/1000 train loss: [array(0.22989309, dtype=float32)], val loss: 0.23612536489963531\n",
      "\t partial train loss (single batch): 0.228775\n",
      "Epoch 57/1000 train loss: [array(0.22877476, dtype=float32)], val loss: 0.23470056056976318\n",
      "\t partial train loss (single batch): 0.227200\n",
      "Epoch 58/1000 train loss: [array(0.2271997, dtype=float32)], val loss: 0.23332597315311432\n",
      "\t partial train loss (single batch): 0.225757\n",
      "Epoch 59/1000 train loss: [array(0.2257574, dtype=float32)], val loss: 0.2311503291130066\n",
      "\t partial train loss (single batch): 0.224292\n",
      "Epoch 60/1000 train loss: [array(0.22429204, dtype=float32)], val loss: 0.22994819283485413\n",
      "\t partial train loss (single batch): 0.222955\n",
      "Epoch 61/1000 train loss: [array(0.22295535, dtype=float32)], val loss: 0.22766204178333282\n",
      "\t partial train loss (single batch): 0.221002\n",
      "Epoch 62/1000 train loss: [array(0.22100227, dtype=float32)], val loss: 0.22630272805690765\n",
      "\t partial train loss (single batch): 0.219461\n",
      "Epoch 63/1000 train loss: [array(0.21946132, dtype=float32)], val loss: 0.224792018532753\n",
      "\t partial train loss (single batch): 0.218248\n",
      "Epoch 64/1000 train loss: [array(0.2182483, dtype=float32)], val loss: 0.22291426360607147\n",
      "\t partial train loss (single batch): 0.216597\n",
      "Epoch 65/1000 train loss: [array(0.21659675, dtype=float32)], val loss: 0.22066402435302734\n",
      "\t partial train loss (single batch): 0.214965\n",
      "Epoch 66/1000 train loss: [array(0.21496454, dtype=float32)], val loss: 0.21905305981636047\n",
      "\t partial train loss (single batch): 0.213440\n",
      "Epoch 67/1000 train loss: [array(0.21343958, dtype=float32)], val loss: 0.21758359670639038\n",
      "\t partial train loss (single batch): 0.212005\n",
      "Epoch 68/1000 train loss: [array(0.2120049, dtype=float32)], val loss: 0.21617506444454193\n",
      "\t partial train loss (single batch): 0.210578\n",
      "Epoch 69/1000 train loss: [array(0.21057823, dtype=float32)], val loss: 0.21447612345218658\n",
      "\t partial train loss (single batch): 0.208822\n",
      "Epoch 70/1000 train loss: [array(0.2088218, dtype=float32)], val loss: 0.21306458115577698\n",
      "\t partial train loss (single batch): 0.207429\n",
      "Epoch 71/1000 train loss: [array(0.20742899, dtype=float32)], val loss: 0.21081624925136566\n",
      "\t partial train loss (single batch): 0.205772\n",
      "Epoch 72/1000 train loss: [array(0.20577209, dtype=float32)], val loss: 0.209390789270401\n",
      "\t partial train loss (single batch): 0.204270\n",
      "Epoch 73/1000 train loss: [array(0.20426951, dtype=float32)], val loss: 0.20802317559719086\n",
      "\t partial train loss (single batch): 0.203008\n",
      "Epoch 74/1000 train loss: [array(0.20300822, dtype=float32)], val loss: 0.20658519864082336\n",
      "\t partial train loss (single batch): 0.201515\n",
      "Epoch 75/1000 train loss: [array(0.201515, dtype=float32)], val loss: 0.20478178560733795\n",
      "\t partial train loss (single batch): 0.199760\n",
      "Epoch 76/1000 train loss: [array(0.19975989, dtype=float32)], val loss: 0.20315831899642944\n",
      "\t partial train loss (single batch): 0.198265\n",
      "Epoch 77/1000 train loss: [array(0.19826545, dtype=float32)], val loss: 0.20181632041931152\n",
      "\t partial train loss (single batch): 0.196669\n",
      "Epoch 78/1000 train loss: [array(0.1966688, dtype=float32)], val loss: 0.1999284029006958\n",
      "\t partial train loss (single batch): 0.195425\n",
      "Epoch 79/1000 train loss: [array(0.19542539, dtype=float32)], val loss: 0.1979338824748993\n",
      "\t partial train loss (single batch): 0.193797\n",
      "Epoch 80/1000 train loss: [array(0.19379729, dtype=float32)], val loss: 0.19647683203220367\n",
      "\t partial train loss (single batch): 0.192242\n",
      "Epoch 81/1000 train loss: [array(0.1922421, dtype=float32)], val loss: 0.1951788365840912\n",
      "\t partial train loss (single batch): 0.190696\n",
      "Epoch 82/1000 train loss: [array(0.19069639, dtype=float32)], val loss: 0.19393578171730042\n",
      "\t partial train loss (single batch): 0.189445\n",
      "Epoch 83/1000 train loss: [array(0.18944506, dtype=float32)], val loss: 0.1921510100364685\n",
      "\t partial train loss (single batch): 0.187978\n",
      "Epoch 84/1000 train loss: [array(0.18797779, dtype=float32)], val loss: 0.19010962545871735\n",
      "\t partial train loss (single batch): 0.186692\n",
      "Epoch 85/1000 train loss: [array(0.18669216, dtype=float32)], val loss: 0.18890321254730225\n",
      "\t partial train loss (single batch): 0.185250\n",
      "Epoch 86/1000 train loss: [array(0.18525018, dtype=float32)], val loss: 0.18784165382385254\n",
      "\t partial train loss (single batch): 0.183542\n",
      "Epoch 87/1000 train loss: [array(0.1835416, dtype=float32)], val loss: 0.18696612119674683\n",
      "\t partial train loss (single batch): 0.182741\n",
      "Epoch 88/1000 train loss: [array(0.1827415, dtype=float32)], val loss: 0.18475861847400665\n",
      "\t partial train loss (single batch): 0.181580\n",
      "Epoch 89/1000 train loss: [array(0.18158002, dtype=float32)], val loss: 0.1832934021949768\n",
      "\t partial train loss (single batch): 0.179709\n",
      "Epoch 90/1000 train loss: [array(0.17970912, dtype=float32)], val loss: 0.18177838623523712\n",
      "\t partial train loss (single batch): 0.178392\n",
      "Epoch 91/1000 train loss: [array(0.17839245, dtype=float32)], val loss: 0.18045058846473694\n",
      "\t partial train loss (single batch): 0.176751\n",
      "Epoch 92/1000 train loss: [array(0.1767507, dtype=float32)], val loss: 0.17891374230384827\n",
      "\t partial train loss (single batch): 0.176052\n",
      "Epoch 93/1000 train loss: [array(0.17605229, dtype=float32)], val loss: 0.17721520364284515\n",
      "\t partial train loss (single batch): 0.174364\n",
      "Epoch 94/1000 train loss: [array(0.17436421, dtype=float32)], val loss: 0.17649409174919128\n",
      "\t partial train loss (single batch): 0.173058\n",
      "Epoch 95/1000 train loss: [array(0.17305838, dtype=float32)], val loss: 0.17505283653736115\n",
      "\t partial train loss (single batch): 0.172344\n",
      "Epoch 96/1000 train loss: [array(0.17234391, dtype=float32)], val loss: 0.17375719547271729\n",
      "\t partial train loss (single batch): 0.170752\n",
      "Epoch 97/1000 train loss: [array(0.170752, dtype=float32)], val loss: 0.17273636162281036\n",
      "\t partial train loss (single batch): 0.169537\n",
      "Epoch 98/1000 train loss: [array(0.16953717, dtype=float32)], val loss: 0.17113874852657318\n",
      "\t partial train loss (single batch): 0.168431\n",
      "Epoch 99/1000 train loss: [array(0.16843079, dtype=float32)], val loss: 0.16963094472885132\n",
      "\t partial train loss (single batch): 0.166515\n",
      "Epoch 100/1000 train loss: [array(0.16651452, dtype=float32)], val loss: 0.16704431176185608\n",
      "\t partial train loss (single batch): 0.165614\n",
      "Epoch 101/1000 train loss: [array(0.16561376, dtype=float32)], val loss: 0.16763998568058014\n",
      "\t partial train loss (single batch): 0.164773\n",
      "Epoch 102/1000 train loss: [array(0.16477293, dtype=float32)], val loss: 0.16599516570568085\n",
      "\t partial train loss (single batch): 0.163067\n",
      "Epoch 103/1000 train loss: [array(0.16306719, dtype=float32)], val loss: 0.16405802965164185\n",
      "\t partial train loss (single batch): 0.161521\n",
      "Epoch 104/1000 train loss: [array(0.16152123, dtype=float32)], val loss: 0.16351541876792908\n",
      "\t partial train loss (single batch): 0.160713\n",
      "Epoch 105/1000 train loss: [array(0.16071261, dtype=float32)], val loss: 0.1624000370502472\n",
      "\t partial train loss (single batch): 0.159451\n",
      "Epoch 106/1000 train loss: [array(0.15945065, dtype=float32)], val loss: 0.16105128824710846\n",
      "\t partial train loss (single batch): 0.158336\n",
      "Epoch 107/1000 train loss: [array(0.15833564, dtype=float32)], val loss: 0.15959595143795013\n",
      "\t partial train loss (single batch): 0.156825\n",
      "Epoch 108/1000 train loss: [array(0.15682468, dtype=float32)], val loss: 0.15838447213172913\n",
      "\t partial train loss (single batch): 0.156274\n",
      "Epoch 109/1000 train loss: [array(0.15627386, dtype=float32)], val loss: 0.15739333629608154\n",
      "\t partial train loss (single batch): 0.155276\n",
      "Epoch 110/1000 train loss: [array(0.15527639, dtype=float32)], val loss: 0.15678325295448303\n",
      "\t partial train loss (single batch): 0.154052\n",
      "Epoch 111/1000 train loss: [array(0.15405163, dtype=float32)], val loss: 0.15566082298755646\n",
      "\t partial train loss (single batch): 0.152444\n",
      "Epoch 112/1000 train loss: [array(0.15244398, dtype=float32)], val loss: 0.153959259390831\n",
      "\t partial train loss (single batch): 0.151497\n",
      "Epoch 113/1000 train loss: [array(0.15149748, dtype=float32)], val loss: 0.1527455598115921\n",
      "\t partial train loss (single batch): 0.150682\n",
      "Epoch 114/1000 train loss: [array(0.15068185, dtype=float32)], val loss: 0.15212970972061157\n",
      "\t partial train loss (single batch): 0.149512\n",
      "Epoch 115/1000 train loss: [array(0.1495122, dtype=float32)], val loss: 0.15031296014785767\n",
      "\t partial train loss (single batch): 0.148585\n",
      "Epoch 116/1000 train loss: [array(0.14858466, dtype=float32)], val loss: 0.14874394237995148\n",
      "\t partial train loss (single batch): 0.147361\n",
      "Epoch 117/1000 train loss: [array(0.14736092, dtype=float32)], val loss: 0.14820554852485657\n",
      "\t partial train loss (single batch): 0.146791\n",
      "Epoch 118/1000 train loss: [array(0.14679106, dtype=float32)], val loss: 0.14671185612678528\n",
      "\t partial train loss (single batch): 0.145877\n",
      "Epoch 119/1000 train loss: [array(0.14587669, dtype=float32)], val loss: 0.14651915431022644\n",
      "\t partial train loss (single batch): 0.143909\n",
      "Epoch 120/1000 train loss: [array(0.14390865, dtype=float32)], val loss: 0.14556433260440826\n",
      "\t partial train loss (single batch): 0.143364\n",
      "Epoch 121/1000 train loss: [array(0.1433642, dtype=float32)], val loss: 0.1441846638917923\n",
      "\t partial train loss (single batch): 0.142376\n",
      "Epoch 122/1000 train loss: [array(0.14237553, dtype=float32)], val loss: 0.14320258796215057\n",
      "\t partial train loss (single batch): 0.141531\n",
      "Epoch 123/1000 train loss: [array(0.14153118, dtype=float32)], val loss: 0.14256533980369568\n",
      "\t partial train loss (single batch): 0.140518\n",
      "Epoch 124/1000 train loss: [array(0.14051804, dtype=float32)], val loss: 0.1402892917394638\n",
      "\t partial train loss (single batch): 0.139406\n",
      "Epoch 125/1000 train loss: [array(0.13940588, dtype=float32)], val loss: 0.1394142210483551\n",
      "\t partial train loss (single batch): 0.138758\n",
      "Epoch 126/1000 train loss: [array(0.1387583, dtype=float32)], val loss: 0.13966025412082672\n",
      "\t partial train loss (single batch): 0.138206\n",
      "Epoch 127/1000 train loss: [array(0.1382064, dtype=float32)], val loss: 0.138221874833107\n",
      "\t partial train loss (single batch): 0.136666\n",
      "Epoch 128/1000 train loss: [array(0.1366664, dtype=float32)], val loss: 0.13517902791500092\n",
      "\t partial train loss (single batch): 0.136290\n",
      "Epoch 129/1000 train loss: [array(0.13628985, dtype=float32)], val loss: 0.1355445683002472\n",
      "\t partial train loss (single batch): 0.134410\n",
      "Epoch 130/1000 train loss: [array(0.13441011, dtype=float32)], val loss: 0.13513050973415375\n",
      "\t partial train loss (single batch): 0.133814\n",
      "Epoch 131/1000 train loss: [array(0.13381363, dtype=float32)], val loss: 0.13365837931632996\n",
      "\t partial train loss (single batch): 0.133538\n",
      "Epoch 132/1000 train loss: [array(0.13353771, dtype=float32)], val loss: 0.1327798217535019\n",
      "\t partial train loss (single batch): 0.132340\n",
      "Epoch 133/1000 train loss: [array(0.13233978, dtype=float32)], val loss: 0.13113147020339966\n",
      "\t partial train loss (single batch): 0.131087\n",
      "Epoch 134/1000 train loss: [array(0.13108735, dtype=float32)], val loss: 0.12976737320423126\n",
      "\t partial train loss (single batch): 0.130041\n",
      "Epoch 135/1000 train loss: [array(0.13004053, dtype=float32)], val loss: 0.12937651574611664\n",
      "\t partial train loss (single batch): 0.130083\n",
      "Epoch 136/1000 train loss: [array(0.13008264, dtype=float32)], val loss: 0.12919338047504425\n",
      "\t partial train loss (single batch): 0.129316\n",
      "Epoch 137/1000 train loss: [array(0.12931643, dtype=float32)], val loss: 0.1276530772447586\n",
      "\t partial train loss (single batch): 0.128256\n",
      "Epoch 138/1000 train loss: [array(0.12825555, dtype=float32)], val loss: 0.1280066817998886\n",
      "\t partial train loss (single batch): 0.127013\n",
      "Epoch 139/1000 train loss: [array(0.12701254, dtype=float32)], val loss: 0.1269908845424652\n",
      "\t partial train loss (single batch): 0.126135\n",
      "Epoch 140/1000 train loss: [array(0.12613548, dtype=float32)], val loss: 0.12526822090148926\n",
      "\t partial train loss (single batch): 0.125639\n",
      "Epoch 141/1000 train loss: [array(0.12563896, dtype=float32)], val loss: 0.12439371645450592\n",
      "\t partial train loss (single batch): 0.123972\n",
      "Epoch 142/1000 train loss: [array(0.1239718, dtype=float32)], val loss: 0.12485714256763458\n",
      "\t partial train loss (single batch): 0.123266\n",
      "Epoch 143/1000 train loss: [array(0.12326632, dtype=float32)], val loss: 0.124275341629982\n",
      "\t partial train loss (single batch): 0.122407\n",
      "Epoch 144/1000 train loss: [array(0.12240662, dtype=float32)], val loss: 0.12159144133329391\n",
      "\t partial train loss (single batch): 0.121885\n",
      "Epoch 145/1000 train loss: [array(0.12188512, dtype=float32)], val loss: 0.12138678133487701\n",
      "\t partial train loss (single batch): 0.120489\n",
      "Epoch 146/1000 train loss: [array(0.12048864, dtype=float32)], val loss: 0.12040464580059052\n",
      "\t partial train loss (single batch): 0.120356\n",
      "Epoch 147/1000 train loss: [array(0.12035631, dtype=float32)], val loss: 0.12110966444015503\n",
      "\t partial train loss (single batch): 0.119161\n",
      "Epoch 148/1000 train loss: [array(0.11916097, dtype=float32)], val loss: 0.11905834823846817\n",
      "\t partial train loss (single batch): 0.118549\n",
      "Epoch 149/1000 train loss: [array(0.11854941, dtype=float32)], val loss: 0.11872530728578568\n",
      "\t partial train loss (single batch): 0.117392\n",
      "Epoch 150/1000 train loss: [array(0.117392, dtype=float32)], val loss: 0.11644045263528824\n",
      "\t partial train loss (single batch): 0.116342\n",
      "Epoch 151/1000 train loss: [array(0.11634196, dtype=float32)], val loss: 0.11689921468496323\n",
      "\t partial train loss (single batch): 0.115361\n",
      "Epoch 152/1000 train loss: [array(0.11536111, dtype=float32)], val loss: 0.11467446386814117\n",
      "\t partial train loss (single batch): 0.115558\n",
      "Epoch 153/1000 train loss: [array(0.11555815, dtype=float32)], val loss: 0.1145775243639946\n",
      "\t partial train loss (single batch): 0.114689\n",
      "Epoch 154/1000 train loss: [array(0.1146889, dtype=float32)], val loss: 0.11304501444101334\n",
      "\t partial train loss (single batch): 0.113945\n",
      "Epoch 155/1000 train loss: [array(0.11394529, dtype=float32)], val loss: 0.1134682297706604\n",
      "\t partial train loss (single batch): 0.113661\n",
      "Epoch 156/1000 train loss: [array(0.11366097, dtype=float32)], val loss: 0.1126820296049118\n",
      "\t partial train loss (single batch): 0.111920\n",
      "Epoch 157/1000 train loss: [array(0.11191979, dtype=float32)], val loss: 0.11108560115098953\n",
      "\t partial train loss (single batch): 0.111729\n",
      "Epoch 158/1000 train loss: [array(0.11172928, dtype=float32)], val loss: 0.11131765693426132\n",
      "\t partial train loss (single batch): 0.111280\n",
      "Epoch 159/1000 train loss: [array(0.1112799, dtype=float32)], val loss: 0.10883566737174988\n",
      "\t partial train loss (single batch): 0.110199\n",
      "Epoch 160/1000 train loss: [array(0.11019903, dtype=float32)], val loss: 0.10849110782146454\n",
      "\t partial train loss (single batch): 0.109748\n",
      "Epoch 161/1000 train loss: [array(0.10974774, dtype=float32)], val loss: 0.10857921838760376\n",
      "\t partial train loss (single batch): 0.109404\n",
      "Epoch 162/1000 train loss: [array(0.10940398, dtype=float32)], val loss: 0.1071571558713913\n",
      "\t partial train loss (single batch): 0.106937\n",
      "Epoch 163/1000 train loss: [array(0.1069366, dtype=float32)], val loss: 0.10739834606647491\n",
      "\t partial train loss (single batch): 0.107093\n",
      "Epoch 164/1000 train loss: [array(0.10709266, dtype=float32)], val loss: 0.10625883936882019\n",
      "\t partial train loss (single batch): 0.105927\n",
      "Epoch 165/1000 train loss: [array(0.10592714, dtype=float32)], val loss: 0.10520390421152115\n",
      "\t partial train loss (single batch): 0.104877\n",
      "Epoch 166/1000 train loss: [array(0.10487735, dtype=float32)], val loss: 0.10543667525053024\n",
      "\t partial train loss (single batch): 0.104814\n",
      "Epoch 167/1000 train loss: [array(0.10481402, dtype=float32)], val loss: 0.1054239273071289\n",
      "\t partial train loss (single batch): 0.104169\n",
      "Epoch 168/1000 train loss: [array(0.10416911, dtype=float32)], val loss: 0.10379950702190399\n",
      "\t partial train loss (single batch): 0.104328\n",
      "Epoch 169/1000 train loss: [array(0.10432761, dtype=float32)], val loss: 0.10475943237543106\n",
      "\t partial train loss (single batch): 0.102605\n",
      "Epoch 170/1000 train loss: [array(0.10260502, dtype=float32)], val loss: 0.10202563554048538\n",
      "\t partial train loss (single batch): 0.103034\n",
      "Epoch 171/1000 train loss: [array(0.1030336, dtype=float32)], val loss: 0.10196804255247116\n",
      "\t partial train loss (single batch): 0.102384\n",
      "Epoch 172/1000 train loss: [array(0.1023837, dtype=float32)], val loss: 0.10109373182058334\n",
      "\t partial train loss (single batch): 0.101122\n",
      "Epoch 173/1000 train loss: [array(0.10112157, dtype=float32)], val loss: 0.10048940032720566\n",
      "\t partial train loss (single batch): 0.101157\n",
      "Epoch 174/1000 train loss: [array(0.10115676, dtype=float32)], val loss: 0.10003818571567535\n",
      "\t partial train loss (single batch): 0.100741\n",
      "Epoch 175/1000 train loss: [array(0.1007411, dtype=float32)], val loss: 0.09905599057674408\n",
      "\t partial train loss (single batch): 0.099094\n",
      "Epoch 176/1000 train loss: [array(0.09909416, dtype=float32)], val loss: 0.09814727306365967\n",
      "\t partial train loss (single batch): 0.099068\n",
      "Epoch 177/1000 train loss: [array(0.099068, dtype=float32)], val loss: 0.0969020426273346\n",
      "\t partial train loss (single batch): 0.098551\n",
      "Epoch 178/1000 train loss: [array(0.09855062, dtype=float32)], val loss: 0.09774655103683472\n",
      "\t partial train loss (single batch): 0.097697\n",
      "Epoch 179/1000 train loss: [array(0.09769711, dtype=float32)], val loss: 0.09577330946922302\n",
      "\t partial train loss (single batch): 0.097011\n",
      "Epoch 180/1000 train loss: [array(0.09701124, dtype=float32)], val loss: 0.09474987536668777\n",
      "\t partial train loss (single batch): 0.097143\n",
      "Epoch 181/1000 train loss: [array(0.09714337, dtype=float32)], val loss: 0.09534432739019394\n",
      "\t partial train loss (single batch): 0.096405\n",
      "Epoch 182/1000 train loss: [array(0.09640528, dtype=float32)], val loss: 0.09425539523363113\n",
      "\t partial train loss (single batch): 0.096093\n",
      "Epoch 183/1000 train loss: [array(0.09609264, dtype=float32)], val loss: 0.09409528225660324\n",
      "\t partial train loss (single batch): 0.094350\n",
      "Epoch 184/1000 train loss: [array(0.09434983, dtype=float32)], val loss: 0.09303710609674454\n",
      "\t partial train loss (single batch): 0.095118\n",
      "Epoch 185/1000 train loss: [array(0.09511762, dtype=float32)], val loss: 0.0934867262840271\n",
      "\t partial train loss (single batch): 0.093805\n",
      "Epoch 186/1000 train loss: [array(0.0938051, dtype=float32)], val loss: 0.09433009475469589\n",
      "\t partial train loss (single batch): 0.094021\n",
      "Epoch 187/1000 train loss: [array(0.09402096, dtype=float32)], val loss: 0.0918266549706459\n",
      "\t partial train loss (single batch): 0.093549\n",
      "Epoch 188/1000 train loss: [array(0.09354856, dtype=float32)], val loss: 0.0908207818865776\n",
      "\t partial train loss (single batch): 0.094119\n",
      "Epoch 189/1000 train loss: [array(0.09411895, dtype=float32)], val loss: 0.09258095175027847\n",
      "\t partial train loss (single batch): 0.092724\n",
      "Epoch 190/1000 train loss: [array(0.09272412, dtype=float32)], val loss: 0.09009724855422974\n",
      "\t partial train loss (single batch): 0.091367\n",
      "Epoch 191/1000 train loss: [array(0.09136668, dtype=float32)], val loss: 0.09010258316993713\n",
      "\t partial train loss (single batch): 0.090729\n",
      "Epoch 192/1000 train loss: [array(0.09072874, dtype=float32)], val loss: 0.08930584043264389\n",
      "\t partial train loss (single batch): 0.092439\n",
      "Epoch 193/1000 train loss: [array(0.0924393, dtype=float32)], val loss: 0.09048353135585785\n",
      "\t partial train loss (single batch): 0.090707\n",
      "Epoch 194/1000 train loss: [array(0.09070667, dtype=float32)], val loss: 0.08902790397405624\n",
      "\t partial train loss (single batch): 0.090145\n",
      "Epoch 195/1000 train loss: [array(0.09014534, dtype=float32)], val loss: 0.08890938013792038\n",
      "\t partial train loss (single batch): 0.089547\n",
      "Epoch 196/1000 train loss: [array(0.089547, dtype=float32)], val loss: 0.08793313056230545\n",
      "\t partial train loss (single batch): 0.089257\n",
      "Epoch 197/1000 train loss: [array(0.08925656, dtype=float32)], val loss: 0.08593979477882385\n",
      "\t partial train loss (single batch): 0.089591\n",
      "Epoch 198/1000 train loss: [array(0.08959052, dtype=float32)], val loss: 0.08744091540575027\n",
      "\t partial train loss (single batch): 0.089323\n",
      "Epoch 199/1000 train loss: [array(0.08932289, dtype=float32)], val loss: 0.0881258100271225\n",
      "\t partial train loss (single batch): 0.087855\n",
      "Epoch 200/1000 train loss: [array(0.08785501, dtype=float32)], val loss: 0.08597144484519958\n",
      "\t partial train loss (single batch): 0.087003\n",
      "Epoch 201/1000 train loss: [array(0.08700348, dtype=float32)], val loss: 0.08649700880050659\n",
      "\t partial train loss (single batch): 0.088112\n",
      "Epoch 202/1000 train loss: [array(0.088112, dtype=float32)], val loss: 0.08691175282001495\n",
      "\t partial train loss (single batch): 0.086235\n",
      "Epoch 203/1000 train loss: [array(0.0862352, dtype=float32)], val loss: 0.08592033386230469\n",
      "\t partial train loss (single batch): 0.086863\n",
      "Epoch 204/1000 train loss: [array(0.08686259, dtype=float32)], val loss: 0.08497348427772522\n",
      "\t partial train loss (single batch): 0.085647\n",
      "Epoch 205/1000 train loss: [array(0.08564663, dtype=float32)], val loss: 0.08601541072130203\n",
      "\t partial train loss (single batch): 0.085270\n",
      "Epoch 206/1000 train loss: [array(0.08527017, dtype=float32)], val loss: 0.08411414921283722\n",
      "\t partial train loss (single batch): 0.084605\n",
      "Epoch 207/1000 train loss: [array(0.08460535, dtype=float32)], val loss: 0.08481258898973465\n",
      "\t partial train loss (single batch): 0.086334\n",
      "Epoch 208/1000 train loss: [array(0.08633394, dtype=float32)], val loss: 0.08470704406499863\n",
      "\t partial train loss (single batch): 0.085546\n",
      "Epoch 209/1000 train loss: [array(0.0855457, dtype=float32)], val loss: 0.08278811722993851\n",
      "\t partial train loss (single batch): 0.083706\n",
      "Epoch 210/1000 train loss: [array(0.08370625, dtype=float32)], val loss: 0.08329284191131592\n",
      "\t partial train loss (single batch): 0.084345\n",
      "Epoch 211/1000 train loss: [array(0.08434521, dtype=float32)], val loss: 0.08280134946107864\n",
      "\t partial train loss (single batch): 0.084924\n",
      "Epoch 212/1000 train loss: [array(0.084924, dtype=float32)], val loss: 0.0833960473537445\n",
      "\t partial train loss (single batch): 0.083650\n",
      "Epoch 213/1000 train loss: [array(0.08364966, dtype=float32)], val loss: 0.08387059718370438\n",
      "\t partial train loss (single batch): 0.083167\n",
      "Epoch 214/1000 train loss: [array(0.08316732, dtype=float32)], val loss: 0.08282245695590973\n",
      "\t partial train loss (single batch): 0.083775\n",
      "Epoch 215/1000 train loss: [array(0.08377476, dtype=float32)], val loss: 0.08377252519130707\n",
      "\t partial train loss (single batch): 0.082197\n",
      "Epoch 216/1000 train loss: [array(0.08219713, dtype=float32)], val loss: 0.08279500901699066\n",
      "\t partial train loss (single batch): 0.082458\n",
      "Epoch 217/1000 train loss: [array(0.08245818, dtype=float32)], val loss: 0.0805194228887558\n",
      "\t partial train loss (single batch): 0.080863\n",
      "Epoch 218/1000 train loss: [array(0.08086285, dtype=float32)], val loss: 0.08137024939060211\n",
      "\t partial train loss (single batch): 0.081200\n",
      "Epoch 219/1000 train loss: [array(0.08119976, dtype=float32)], val loss: 0.08109395951032639\n",
      "\t partial train loss (single batch): 0.081317\n",
      "Epoch 220/1000 train loss: [array(0.0813175, dtype=float32)], val loss: 0.0816766694188118\n",
      "\t partial train loss (single batch): 0.081907\n",
      "Epoch 221/1000 train loss: [array(0.08190709, dtype=float32)], val loss: 0.07968831807374954\n",
      "\t partial train loss (single batch): 0.080328\n",
      "Epoch 222/1000 train loss: [array(0.08032822, dtype=float32)], val loss: 0.08122027665376663\n",
      "\t partial train loss (single batch): 0.080602\n",
      "Epoch 223/1000 train loss: [array(0.08060209, dtype=float32)], val loss: 0.0809057205915451\n",
      "\t partial train loss (single batch): 0.079929\n",
      "Epoch 224/1000 train loss: [array(0.07992893, dtype=float32)], val loss: 0.07943026721477509\n",
      "\t partial train loss (single batch): 0.080073\n",
      "Epoch 225/1000 train loss: [array(0.08007255, dtype=float32)], val loss: 0.07948297262191772\n",
      "\t partial train loss (single batch): 0.079640\n",
      "Epoch 226/1000 train loss: [array(0.07964002, dtype=float32)], val loss: 0.07929856330156326\n",
      "\t partial train loss (single batch): 0.078940\n",
      "Epoch 227/1000 train loss: [array(0.07893985, dtype=float32)], val loss: 0.0792282223701477\n",
      "\t partial train loss (single batch): 0.079833\n",
      "Epoch 228/1000 train loss: [array(0.07983314, dtype=float32)], val loss: 0.07720062881708145\n",
      "\t partial train loss (single batch): 0.078691\n",
      "Epoch 229/1000 train loss: [array(0.07869122, dtype=float32)], val loss: 0.07865388691425323\n",
      "\t partial train loss (single batch): 0.078744\n",
      "Epoch 230/1000 train loss: [array(0.07874369, dtype=float32)], val loss: 0.07694719731807709\n",
      "\t partial train loss (single batch): 0.076764\n",
      "Epoch 231/1000 train loss: [array(0.07676391, dtype=float32)], val loss: 0.07824302464723587\n",
      "\t partial train loss (single batch): 0.077891\n",
      "Epoch 232/1000 train loss: [array(0.07789058, dtype=float32)], val loss: 0.07943321019411087\n",
      "\t partial train loss (single batch): 0.077450\n",
      "Epoch 233/1000 train loss: [array(0.07745014, dtype=float32)], val loss: 0.07784443348646164\n",
      "\t partial train loss (single batch): 0.077039\n",
      "Epoch 234/1000 train loss: [array(0.07703934, dtype=float32)], val loss: 0.07601211965084076\n",
      "\t partial train loss (single batch): 0.076604\n",
      "Epoch 235/1000 train loss: [array(0.07660446, dtype=float32)], val loss: 0.07617568224668503\n",
      "\t partial train loss (single batch): 0.076773\n",
      "Epoch 236/1000 train loss: [array(0.07677285, dtype=float32)], val loss: 0.07728958874940872\n",
      "\t partial train loss (single batch): 0.076942\n",
      "Epoch 237/1000 train loss: [array(0.07694167, dtype=float32)], val loss: 0.07721251994371414\n",
      "\t partial train loss (single batch): 0.075417\n",
      "Epoch 238/1000 train loss: [array(0.07541745, dtype=float32)], val loss: 0.07583016157150269\n",
      "\t partial train loss (single batch): 0.075672\n",
      "Epoch 239/1000 train loss: [array(0.07567245, dtype=float32)], val loss: 0.07594852149486542\n",
      "\t partial train loss (single batch): 0.077000\n",
      "Epoch 240/1000 train loss: [array(0.07700027, dtype=float32)], val loss: 0.07514170557260513\n",
      "\t partial train loss (single batch): 0.076539\n",
      "Epoch 241/1000 train loss: [array(0.07653919, dtype=float32)], val loss: 0.07489754259586334\n",
      "\t partial train loss (single batch): 0.075691\n",
      "Epoch 242/1000 train loss: [array(0.07569063, dtype=float32)], val loss: 0.07600925117731094\n",
      "\t partial train loss (single batch): 0.076027\n",
      "Epoch 243/1000 train loss: [array(0.07602719, dtype=float32)], val loss: 0.07419486343860626\n",
      "\t partial train loss (single batch): 0.075152\n",
      "Epoch 244/1000 train loss: [array(0.07515228, dtype=float32)], val loss: 0.07521310448646545\n",
      "\t partial train loss (single batch): 0.075172\n",
      "Epoch 245/1000 train loss: [array(0.07517231, dtype=float32)], val loss: 0.07356731593608856\n",
      "\t partial train loss (single batch): 0.074817\n",
      "Epoch 246/1000 train loss: [array(0.07481698, dtype=float32)], val loss: 0.07445637136697769\n",
      "\t partial train loss (single batch): 0.073666\n",
      "Epoch 247/1000 train loss: [array(0.07366558, dtype=float32)], val loss: 0.07380955666303635\n",
      "\t partial train loss (single batch): 0.075252\n",
      "Epoch 248/1000 train loss: [array(0.075252, dtype=float32)], val loss: 0.07376549392938614\n",
      "\t partial train loss (single batch): 0.073942\n",
      "Epoch 249/1000 train loss: [array(0.07394156, dtype=float32)], val loss: 0.07293384522199631\n",
      "\t partial train loss (single batch): 0.073891\n",
      "Epoch 250/1000 train loss: [array(0.07389109, dtype=float32)], val loss: 0.07371097058057785\n",
      "\t partial train loss (single batch): 0.072785\n",
      "Epoch 251/1000 train loss: [array(0.07278537, dtype=float32)], val loss: 0.07351724058389664\n",
      "\t partial train loss (single batch): 0.074697\n",
      "Epoch 252/1000 train loss: [array(0.07469679, dtype=float32)], val loss: 0.07293112576007843\n",
      "\t partial train loss (single batch): 0.073533\n",
      "Epoch 253/1000 train loss: [array(0.07353267, dtype=float32)], val loss: 0.07214508205652237\n",
      "\t partial train loss (single batch): 0.072741\n",
      "Epoch 254/1000 train loss: [array(0.07274119, dtype=float32)], val loss: 0.07361316680908203\n",
      "\t partial train loss (single batch): 0.073276\n",
      "Epoch 255/1000 train loss: [array(0.07327623, dtype=float32)], val loss: 0.073489248752594\n",
      "\t partial train loss (single batch): 0.073035\n",
      "Epoch 256/1000 train loss: [array(0.07303549, dtype=float32)], val loss: 0.07216263562440872\n",
      "\t partial train loss (single batch): 0.073505\n",
      "Epoch 257/1000 train loss: [array(0.07350488, dtype=float32)], val loss: 0.07132259011268616\n",
      "\t partial train loss (single batch): 0.072074\n",
      "Epoch 258/1000 train loss: [array(0.07207415, dtype=float32)], val loss: 0.07305638492107391\n",
      "\t partial train loss (single batch): 0.071840\n",
      "Epoch 259/1000 train loss: [array(0.07183962, dtype=float32)], val loss: 0.07036033272743225\n",
      "\t partial train loss (single batch): 0.069997\n",
      "Epoch 260/1000 train loss: [array(0.06999671, dtype=float32)], val loss: 0.07316254079341888\n",
      "\t partial train loss (single batch): 0.071340\n",
      "Epoch 261/1000 train loss: [array(0.07134037, dtype=float32)], val loss: 0.0734025314450264\n",
      "\t partial train loss (single batch): 0.073366\n",
      "Epoch 262/1000 train loss: [array(0.07336587, dtype=float32)], val loss: 0.0726999044418335\n",
      "\t partial train loss (single batch): 0.071404\n",
      "Epoch 263/1000 train loss: [array(0.0714042, dtype=float32)], val loss: 0.07093647122383118\n",
      "\t partial train loss (single batch): 0.071587\n",
      "Epoch 264/1000 train loss: [array(0.07158665, dtype=float32)], val loss: 0.07188539952039719\n",
      "\t partial train loss (single batch): 0.071636\n",
      "Epoch 265/1000 train loss: [array(0.07163622, dtype=float32)], val loss: 0.06888478994369507\n",
      "\t partial train loss (single batch): 0.070579\n",
      "Epoch 266/1000 train loss: [array(0.07057928, dtype=float32)], val loss: 0.07114633917808533\n",
      "\t partial train loss (single batch): 0.071890\n",
      "Epoch 267/1000 train loss: [array(0.07188974, dtype=float32)], val loss: 0.0711015909910202\n",
      "\t partial train loss (single batch): 0.069143\n",
      "Epoch 268/1000 train loss: [array(0.06914268, dtype=float32)], val loss: 0.0727398470044136\n",
      "\t partial train loss (single batch): 0.070561\n",
      "Epoch 269/1000 train loss: [array(0.07056119, dtype=float32)], val loss: 0.06978403031826019\n",
      "\t partial train loss (single batch): 0.070816\n",
      "Epoch 270/1000 train loss: [array(0.07081585, dtype=float32)], val loss: 0.07065843045711517\n",
      "\t partial train loss (single batch): 0.071115\n",
      "Epoch 271/1000 train loss: [array(0.07111536, dtype=float32)], val loss: 0.0709298774600029\n",
      "\t partial train loss (single batch): 0.071358\n",
      "Epoch 272/1000 train loss: [array(0.0713577, dtype=float32)], val loss: 0.06982561200857162\n",
      "\t partial train loss (single batch): 0.069101\n",
      "Epoch 273/1000 train loss: [array(0.06910057, dtype=float32)], val loss: 0.070527084171772\n",
      "\t partial train loss (single batch): 0.069526\n",
      "Epoch 274/1000 train loss: [array(0.06952565, dtype=float32)], val loss: 0.07018496841192245\n",
      "\t partial train loss (single batch): 0.069086\n",
      "Epoch 275/1000 train loss: [array(0.06908593, dtype=float32)], val loss: 0.06956864148378372\n",
      "\t partial train loss (single batch): 0.069910\n",
      "Epoch 276/1000 train loss: [array(0.06991034, dtype=float32)], val loss: 0.06887425482273102\n",
      "\t partial train loss (single batch): 0.068379\n",
      "Epoch 277/1000 train loss: [array(0.06837858, dtype=float32)], val loss: 0.06984879821538925\n",
      "\t partial train loss (single batch): 0.069854\n",
      "Epoch 278/1000 train loss: [array(0.06985375, dtype=float32)], val loss: 0.06995133310556412\n",
      "\t partial train loss (single batch): 0.069998\n",
      "Epoch 279/1000 train loss: [array(0.06999832, dtype=float32)], val loss: 0.06793294847011566\n",
      "\t partial train loss (single batch): 0.068179\n",
      "Epoch 280/1000 train loss: [array(0.0681794, dtype=float32)], val loss: 0.06971515715122223\n",
      "\t partial train loss (single batch): 0.068163\n",
      "Epoch 281/1000 train loss: [array(0.0681634, dtype=float32)], val loss: 0.06845138221979141\n",
      "\t partial train loss (single batch): 0.067544\n",
      "Epoch 282/1000 train loss: [array(0.06754415, dtype=float32)], val loss: 0.06833039969205856\n",
      "\t partial train loss (single batch): 0.067250\n",
      "Epoch 283/1000 train loss: [array(0.06724999, dtype=float32)], val loss: 0.06785673648118973\n",
      "\t partial train loss (single batch): 0.068302\n",
      "Epoch 284/1000 train loss: [array(0.06830173, dtype=float32)], val loss: 0.06813804805278778\n",
      "\t partial train loss (single batch): 0.067838\n",
      "Epoch 285/1000 train loss: [array(0.06783771, dtype=float32)], val loss: 0.06914719939231873\n",
      "\t partial train loss (single batch): 0.068573\n",
      "Epoch 286/1000 train loss: [array(0.06857292, dtype=float32)], val loss: 0.06740424036979675\n",
      "\t partial train loss (single batch): 0.066520\n",
      "Epoch 287/1000 train loss: [array(0.06651977, dtype=float32)], val loss: 0.06777751445770264\n",
      "\t partial train loss (single batch): 0.067420\n",
      "Epoch 288/1000 train loss: [array(0.06741973, dtype=float32)], val loss: 0.0688534826040268\n",
      "\t partial train loss (single batch): 0.068269\n",
      "Epoch 289/1000 train loss: [array(0.0682686, dtype=float32)], val loss: 0.066805899143219\n",
      "\t partial train loss (single batch): 0.067882\n",
      "Epoch 290/1000 train loss: [array(0.06788186, dtype=float32)], val loss: 0.06904040277004242\n",
      "\t partial train loss (single batch): 0.067439\n",
      "Epoch 291/1000 train loss: [array(0.06743853, dtype=float32)], val loss: 0.06615398079156876\n",
      "\t partial train loss (single batch): 0.067516\n",
      "Epoch 292/1000 train loss: [array(0.06751579, dtype=float32)], val loss: 0.0676129087805748\n",
      "\t partial train loss (single batch): 0.067940\n",
      "Epoch 293/1000 train loss: [array(0.0679404, dtype=float32)], val loss: 0.06644846498966217\n",
      "\t partial train loss (single batch): 0.066181\n",
      "Epoch 294/1000 train loss: [array(0.06618145, dtype=float32)], val loss: 0.06777323782444\n",
      "\t partial train loss (single batch): 0.067589\n",
      "Epoch 295/1000 train loss: [array(0.06758896, dtype=float32)], val loss: 0.06735765933990479\n",
      "\t partial train loss (single batch): 0.067276\n",
      "Epoch 296/1000 train loss: [array(0.0672756, dtype=float32)], val loss: 0.06575623899698257\n",
      "\t partial train loss (single batch): 0.066521\n",
      "Epoch 297/1000 train loss: [array(0.06652056, dtype=float32)], val loss: 0.06633646786212921\n",
      "\t partial train loss (single batch): 0.066506\n",
      "Epoch 298/1000 train loss: [array(0.06650563, dtype=float32)], val loss: 0.06691146641969681\n",
      "\t partial train loss (single batch): 0.066397\n",
      "Epoch 299/1000 train loss: [array(0.06639723, dtype=float32)], val loss: 0.06791085749864578\n",
      "\t partial train loss (single batch): 0.066973\n",
      "Epoch 300/1000 train loss: [array(0.06697315, dtype=float32)], val loss: 0.06635265052318573\n",
      "\t partial train loss (single batch): 0.065628\n",
      "Epoch 301/1000 train loss: [array(0.06562832, dtype=float32)], val loss: 0.0665951520204544\n",
      "\t partial train loss (single batch): 0.066353\n",
      "Epoch 302/1000 train loss: [array(0.06635345, dtype=float32)], val loss: 0.06884518265724182\n",
      "\t partial train loss (single batch): 0.065719\n",
      "Epoch 303/1000 train loss: [array(0.06571856, dtype=float32)], val loss: 0.06543644517660141\n",
      "\t partial train loss (single batch): 0.066390\n",
      "Epoch 304/1000 train loss: [array(0.06639025, dtype=float32)], val loss: 0.06683097779750824\n",
      "\t partial train loss (single batch): 0.065970\n",
      "Epoch 305/1000 train loss: [array(0.06596968, dtype=float32)], val loss: 0.06581079959869385\n",
      "\t partial train loss (single batch): 0.065126\n",
      "Epoch 306/1000 train loss: [array(0.06512588, dtype=float32)], val loss: 0.06542721390724182\n",
      "\t partial train loss (single batch): 0.065188\n",
      "Epoch 307/1000 train loss: [array(0.06518811, dtype=float32)], val loss: 0.06598693132400513\n",
      "\t partial train loss (single batch): 0.064768\n",
      "Epoch 308/1000 train loss: [array(0.0647675, dtype=float32)], val loss: 0.06485996395349503\n",
      "\t partial train loss (single batch): 0.065096\n",
      "Epoch 309/1000 train loss: [array(0.0650956, dtype=float32)], val loss: 0.06387018412351608\n",
      "\t partial train loss (single batch): 0.065251\n",
      "Epoch 310/1000 train loss: [array(0.06525056, dtype=float32)], val loss: 0.0655658021569252\n",
      "\t partial train loss (single batch): 0.063496\n",
      "Epoch 311/1000 train loss: [array(0.0634963, dtype=float32)], val loss: 0.06636014580726624\n",
      "\t partial train loss (single batch): 0.064325\n",
      "Epoch 312/1000 train loss: [array(0.06432515, dtype=float32)], val loss: 0.06506945192813873\n",
      "\t partial train loss (single batch): 0.064986\n",
      "Epoch 313/1000 train loss: [array(0.06498583, dtype=float32)], val loss: 0.064161017537117\n",
      "\t partial train loss (single batch): 0.063688\n",
      "Epoch 314/1000 train loss: [array(0.06368821, dtype=float32)], val loss: 0.06511525809764862\n",
      "\t partial train loss (single batch): 0.064684\n",
      "Epoch 315/1000 train loss: [array(0.06468378, dtype=float32)], val loss: 0.06534533947706223\n",
      "\t partial train loss (single batch): 0.065843\n",
      "Epoch 316/1000 train loss: [array(0.06584276, dtype=float32)], val loss: 0.06683570891618729\n",
      "\t partial train loss (single batch): 0.065134\n",
      "Epoch 317/1000 train loss: [array(0.06513383, dtype=float32)], val loss: 0.06501241028308868\n",
      "\t partial train loss (single batch): 0.063938\n",
      "Epoch 318/1000 train loss: [array(0.06393848, dtype=float32)], val loss: 0.06418690830469131\n",
      "\t partial train loss (single batch): 0.062964\n",
      "Epoch 319/1000 train loss: [array(0.06296358, dtype=float32)], val loss: 0.06431680917739868\n",
      "\t partial train loss (single batch): 0.063003\n",
      "Epoch 320/1000 train loss: [array(0.06300309, dtype=float32)], val loss: 0.06306441128253937\n",
      "\t partial train loss (single batch): 0.064649\n",
      "Epoch 321/1000 train loss: [array(0.06464893, dtype=float32)], val loss: 0.06289975345134735\n",
      "\t partial train loss (single batch): 0.065692\n",
      "Epoch 322/1000 train loss: [array(0.06569194, dtype=float32)], val loss: 0.06501621752977371\n",
      "\t partial train loss (single batch): 0.063311\n",
      "Epoch 323/1000 train loss: [array(0.0633112, dtype=float32)], val loss: 0.06467463821172714\n",
      "\t partial train loss (single batch): 0.063342\n",
      "Epoch 324/1000 train loss: [array(0.06334157, dtype=float32)], val loss: 0.06235335394740105\n",
      "\t partial train loss (single batch): 0.063312\n",
      "Epoch 325/1000 train loss: [array(0.06331191, dtype=float32)], val loss: 0.06235450133681297\n",
      "\t partial train loss (single batch): 0.063674\n",
      "Epoch 326/1000 train loss: [array(0.06367379, dtype=float32)], val loss: 0.063756063580513\n",
      "\t partial train loss (single batch): 0.063076\n",
      "Epoch 327/1000 train loss: [array(0.06307644, dtype=float32)], val loss: 0.06509286165237427\n",
      "\t partial train loss (single batch): 0.064309\n",
      "Epoch 328/1000 train loss: [array(0.06430884, dtype=float32)], val loss: 0.06385035067796707\n",
      "\t partial train loss (single batch): 0.063076\n",
      "Epoch 329/1000 train loss: [array(0.06307607, dtype=float32)], val loss: 0.06334801018238068\n",
      "\t partial train loss (single batch): 0.064384\n",
      "Epoch 330/1000 train loss: [array(0.06438423, dtype=float32)], val loss: 0.06359441578388214\n",
      "\t partial train loss (single batch): 0.062566\n",
      "Epoch 331/1000 train loss: [array(0.06256605, dtype=float32)], val loss: 0.0620514377951622\n",
      "\t partial train loss (single batch): 0.063625\n",
      "Epoch 332/1000 train loss: [array(0.06362543, dtype=float32)], val loss: 0.061758313328027725\n",
      "\t partial train loss (single batch): 0.063752\n",
      "Epoch 333/1000 train loss: [array(0.06375244, dtype=float32)], val loss: 0.0631561204791069\n",
      "\t partial train loss (single batch): 0.062819\n",
      "Epoch 334/1000 train loss: [array(0.06281904, dtype=float32)], val loss: 0.06312438100576401\n",
      "\t partial train loss (single batch): 0.063908\n",
      "Epoch 335/1000 train loss: [array(0.06390828, dtype=float32)], val loss: 0.063417948782444\n",
      "\t partial train loss (single batch): 0.063994\n",
      "Epoch 336/1000 train loss: [array(0.06399445, dtype=float32)], val loss: 0.06215711683034897\n",
      "\t partial train loss (single batch): 0.061749\n",
      "Epoch 337/1000 train loss: [array(0.06174863, dtype=float32)], val loss: 0.0613863579928875\n",
      "\t partial train loss (single batch): 0.063304\n",
      "Epoch 338/1000 train loss: [array(0.06330405, dtype=float32)], val loss: 0.06267274171113968\n",
      "\t partial train loss (single batch): 0.062692\n",
      "Epoch 339/1000 train loss: [array(0.06269188, dtype=float32)], val loss: 0.062007781118154526\n",
      "\t partial train loss (single batch): 0.062577\n",
      "Epoch 340/1000 train loss: [array(0.0625772, dtype=float32)], val loss: 0.0634838417172432\n",
      "\t partial train loss (single batch): 0.062803\n",
      "Epoch 341/1000 train loss: [array(0.06280328, dtype=float32)], val loss: 0.06314471364021301\n",
      "\t partial train loss (single batch): 0.062195\n",
      "Epoch 342/1000 train loss: [array(0.06219516, dtype=float32)], val loss: 0.06338417530059814\n",
      "\t partial train loss (single batch): 0.063399\n",
      "Epoch 343/1000 train loss: [array(0.06339853, dtype=float32)], val loss: 0.05928988382220268\n",
      "\t partial train loss (single batch): 0.062874\n",
      "Epoch 344/1000 train loss: [array(0.06287384, dtype=float32)], val loss: 0.06000715494155884\n",
      "\t partial train loss (single batch): 0.061691\n",
      "Epoch 345/1000 train loss: [array(0.06169057, dtype=float32)], val loss: 0.0635802373290062\n",
      "\t partial train loss (single batch): 0.061475\n",
      "Epoch 346/1000 train loss: [array(0.06147484, dtype=float32)], val loss: 0.06145668402314186\n",
      "\t partial train loss (single batch): 0.061480\n",
      "Epoch 347/1000 train loss: [array(0.06148015, dtype=float32)], val loss: 0.062421612441539764\n",
      "\t partial train loss (single batch): 0.062722\n",
      "Epoch 348/1000 train loss: [array(0.06272224, dtype=float32)], val loss: 0.061342474073171616\n",
      "\t partial train loss (single batch): 0.060896\n",
      "Epoch 349/1000 train loss: [array(0.06089602, dtype=float32)], val loss: 0.061485596001148224\n",
      "\t partial train loss (single batch): 0.063920\n",
      "Epoch 350/1000 train loss: [array(0.06392042, dtype=float32)], val loss: 0.062229953706264496\n",
      "\t partial train loss (single batch): 0.062444\n",
      "Epoch 351/1000 train loss: [array(0.06244367, dtype=float32)], val loss: 0.05927957966923714\n",
      "\t partial train loss (single batch): 0.062515\n",
      "Epoch 352/1000 train loss: [array(0.06251504, dtype=float32)], val loss: 0.06155632808804512\n",
      "\t partial train loss (single batch): 0.062506\n",
      "Epoch 353/1000 train loss: [array(0.06250629, dtype=float32)], val loss: 0.061060141772031784\n",
      "\t partial train loss (single batch): 0.060930\n",
      "Epoch 354/1000 train loss: [array(0.06092977, dtype=float32)], val loss: 0.0604957714676857\n",
      "\t partial train loss (single batch): 0.062119\n",
      "Epoch 355/1000 train loss: [array(0.06211916, dtype=float32)], val loss: 0.06003102660179138\n",
      "\t partial train loss (single batch): 0.061425\n",
      "Epoch 356/1000 train loss: [array(0.06142545, dtype=float32)], val loss: 0.06173817813396454\n",
      "\t partial train loss (single batch): 0.060196\n",
      "Epoch 357/1000 train loss: [array(0.06019579, dtype=float32)], val loss: 0.061872370541095734\n",
      "\t partial train loss (single batch): 0.062601\n",
      "Epoch 358/1000 train loss: [array(0.06260089, dtype=float32)], val loss: 0.06103930249810219\n",
      "\t partial train loss (single batch): 0.061053\n",
      "Epoch 359/1000 train loss: [array(0.06105275, dtype=float32)], val loss: 0.060992635786533356\n",
      "\t partial train loss (single batch): 0.060991\n",
      "Epoch 360/1000 train loss: [array(0.06099118, dtype=float32)], val loss: 0.06182055175304413\n",
      "\t partial train loss (single batch): 0.061934\n",
      "Epoch 361/1000 train loss: [array(0.06193406, dtype=float32)], val loss: 0.05943668261170387\n",
      "\t partial train loss (single batch): 0.060269\n",
      "Epoch 362/1000 train loss: [array(0.06026859, dtype=float32)], val loss: 0.06084305793046951\n",
      "\t partial train loss (single batch): 0.061096\n",
      "Epoch 363/1000 train loss: [array(0.06109634, dtype=float32)], val loss: 0.05995301157236099\n",
      "\t partial train loss (single batch): 0.061248\n",
      "Epoch 364/1000 train loss: [array(0.06124758, dtype=float32)], val loss: 0.062096983194351196\n",
      "\t partial train loss (single batch): 0.060969\n",
      "Epoch 365/1000 train loss: [array(0.06096875, dtype=float32)], val loss: 0.06142904981970787\n",
      "\t partial train loss (single batch): 0.061272\n",
      "Epoch 366/1000 train loss: [array(0.06127175, dtype=float32)], val loss: 0.05892280489206314\n",
      "\t partial train loss (single batch): 0.059374\n",
      "Epoch 367/1000 train loss: [array(0.05937357, dtype=float32)], val loss: 0.0604388527572155\n",
      "\t partial train loss (single batch): 0.060543\n",
      "Epoch 368/1000 train loss: [array(0.06054346, dtype=float32)], val loss: 0.059602852910757065\n",
      "\t partial train loss (single batch): 0.057563\n",
      "Epoch 369/1000 train loss: [array(0.05756338, dtype=float32)], val loss: 0.06099431961774826\n",
      "\t partial train loss (single batch): 0.061736\n",
      "Epoch 370/1000 train loss: [array(0.06173597, dtype=float32)], val loss: 0.06215021386742592\n",
      "\t partial train loss (single batch): 0.060259\n",
      "Epoch 371/1000 train loss: [array(0.0602593, dtype=float32)], val loss: 0.061608556658029556\n",
      "\t partial train loss (single batch): 0.059646\n",
      "Epoch 372/1000 train loss: [array(0.05964587, dtype=float32)], val loss: 0.060022033751010895\n",
      "\t partial train loss (single batch): 0.060667\n",
      "Epoch 373/1000 train loss: [array(0.06066708, dtype=float32)], val loss: 0.059910207986831665\n",
      "\t partial train loss (single batch): 0.059513\n",
      "Epoch 374/1000 train loss: [array(0.05951295, dtype=float32)], val loss: 0.060980174690485\n",
      "\t partial train loss (single batch): 0.060206\n",
      "Epoch 375/1000 train loss: [array(0.06020632, dtype=float32)], val loss: 0.060902003198862076\n",
      "\t partial train loss (single batch): 0.060031\n",
      "Epoch 376/1000 train loss: [array(0.06003118, dtype=float32)], val loss: 0.06256897747516632\n",
      "\t partial train loss (single batch): 0.059630\n",
      "Epoch 377/1000 train loss: [array(0.05962985, dtype=float32)], val loss: 0.06052015721797943\n",
      "\t partial train loss (single batch): 0.059991\n",
      "Epoch 378/1000 train loss: [array(0.05999096, dtype=float32)], val loss: 0.05988869443535805\n",
      "\t partial train loss (single batch): 0.060914\n",
      "Epoch 379/1000 train loss: [array(0.06091387, dtype=float32)], val loss: 0.05887956917285919\n",
      "\t partial train loss (single batch): 0.058522\n",
      "Epoch 380/1000 train loss: [array(0.05852164, dtype=float32)], val loss: 0.06068241223692894\n",
      "\t partial train loss (single batch): 0.059600\n",
      "Epoch 381/1000 train loss: [array(0.05959979, dtype=float32)], val loss: 0.06049878150224686\n",
      "\t partial train loss (single batch): 0.061219\n",
      "Epoch 382/1000 train loss: [array(0.061219, dtype=float32)], val loss: 0.06136273965239525\n",
      "\t partial train loss (single batch): 0.058942\n",
      "Epoch 383/1000 train loss: [array(0.05894203, dtype=float32)], val loss: 0.06043505668640137\n",
      "\t partial train loss (single batch): 0.059399\n",
      "Epoch 384/1000 train loss: [array(0.05939936, dtype=float32)], val loss: 0.06049109622836113\n",
      "\t partial train loss (single batch): 0.060307\n",
      "Epoch 385/1000 train loss: [array(0.06030701, dtype=float32)], val loss: 0.05810898169875145\n",
      "\t partial train loss (single batch): 0.059880\n",
      "Epoch 386/1000 train loss: [array(0.05987995, dtype=float32)], val loss: 0.05878717079758644\n",
      "\t partial train loss (single batch): 0.060335\n",
      "Epoch 387/1000 train loss: [array(0.06033502, dtype=float32)], val loss: 0.05715401843190193\n",
      "\t partial train loss (single batch): 0.061083\n",
      "Epoch 388/1000 train loss: [array(0.06108261, dtype=float32)], val loss: 0.05946363881230354\n",
      "\t partial train loss (single batch): 0.059616\n",
      "Epoch 389/1000 train loss: [array(0.059616, dtype=float32)], val loss: 0.060278769582509995\n",
      "\t partial train loss (single batch): 0.060386\n",
      "Epoch 390/1000 train loss: [array(0.06038602, dtype=float32)], val loss: 0.06046843156218529\n",
      "\t partial train loss (single batch): 0.057470\n",
      "Epoch 391/1000 train loss: [array(0.05747031, dtype=float32)], val loss: 0.06017667055130005\n",
      "\t partial train loss (single batch): 0.060985\n",
      "Epoch 392/1000 train loss: [array(0.06098505, dtype=float32)], val loss: 0.05912157893180847\n",
      "\t partial train loss (single batch): 0.058806\n",
      "Epoch 393/1000 train loss: [array(0.05880608, dtype=float32)], val loss: 0.05942578241229057\n",
      "\t partial train loss (single batch): 0.059813\n",
      "Epoch 394/1000 train loss: [array(0.05981344, dtype=float32)], val loss: 0.06083549186587334\n",
      "\t partial train loss (single batch): 0.059009\n",
      "Epoch 395/1000 train loss: [array(0.05900933, dtype=float32)], val loss: 0.059905461966991425\n",
      "\t partial train loss (single batch): 0.058977\n",
      "Epoch 396/1000 train loss: [array(0.05897709, dtype=float32)], val loss: 0.06079212948679924\n",
      "\t partial train loss (single batch): 0.060433\n",
      "Epoch 397/1000 train loss: [array(0.06043332, dtype=float32)], val loss: 0.05753602832555771\n",
      "\t partial train loss (single batch): 0.058341\n",
      "Epoch 398/1000 train loss: [array(0.05834064, dtype=float32)], val loss: 0.05860884487628937\n",
      "\t partial train loss (single batch): 0.058740\n",
      "Epoch 399/1000 train loss: [array(0.05873963, dtype=float32)], val loss: 0.0569395050406456\n",
      "\t partial train loss (single batch): 0.059743\n",
      "Epoch 400/1000 train loss: [array(0.05974275, dtype=float32)], val loss: 0.059844307601451874\n",
      "\t partial train loss (single batch): 0.058474\n",
      "Epoch 401/1000 train loss: [array(0.05847426, dtype=float32)], val loss: 0.05777638405561447\n",
      "\t partial train loss (single batch): 0.058878\n",
      "Epoch 402/1000 train loss: [array(0.05887765, dtype=float32)], val loss: 0.05811159312725067\n",
      "\t partial train loss (single batch): 0.058161\n",
      "Epoch 403/1000 train loss: [array(0.05816136, dtype=float32)], val loss: 0.05801146477460861\n",
      "\t partial train loss (single batch): 0.058013\n",
      "Epoch 404/1000 train loss: [array(0.05801328, dtype=float32)], val loss: 0.05973030626773834\n",
      "\t partial train loss (single batch): 0.058432\n",
      "Epoch 405/1000 train loss: [array(0.05843155, dtype=float32)], val loss: 0.05842912942171097\n",
      "\t partial train loss (single batch): 0.059749\n",
      "Epoch 406/1000 train loss: [array(0.05974878, dtype=float32)], val loss: 0.05831094831228256\n",
      "\t partial train loss (single batch): 0.059376\n",
      "Epoch 407/1000 train loss: [array(0.05937595, dtype=float32)], val loss: 0.06020795553922653\n",
      "\t partial train loss (single batch): 0.058125\n",
      "Epoch 408/1000 train loss: [array(0.05812501, dtype=float32)], val loss: 0.05876320227980614\n",
      "\t partial train loss (single batch): 0.057255\n",
      "Epoch 409/1000 train loss: [array(0.05725549, dtype=float32)], val loss: 0.05937053635716438\n",
      "\t partial train loss (single batch): 0.059752\n",
      "Epoch 410/1000 train loss: [array(0.05975237, dtype=float32)], val loss: 0.05874645337462425\n",
      "\t partial train loss (single batch): 0.059001\n",
      "Epoch 411/1000 train loss: [array(0.0590012, dtype=float32)], val loss: 0.05741589888930321\n",
      "\t partial train loss (single batch): 0.056756\n",
      "Epoch 412/1000 train loss: [array(0.05675631, dtype=float32)], val loss: 0.057590339332818985\n",
      "\t partial train loss (single batch): 0.058126\n",
      "Epoch 413/1000 train loss: [array(0.05812586, dtype=float32)], val loss: 0.05911758914589882\n",
      "\t partial train loss (single batch): 0.057717\n",
      "Epoch 414/1000 train loss: [array(0.05771672, dtype=float32)], val loss: 0.058225080370903015\n",
      "\t partial train loss (single batch): 0.059193\n",
      "Epoch 415/1000 train loss: [array(0.05919345, dtype=float32)], val loss: 0.05858593434095383\n",
      "\t partial train loss (single batch): 0.058167\n",
      "Epoch 416/1000 train loss: [array(0.05816707, dtype=float32)], val loss: 0.059274837374687195\n",
      "\t partial train loss (single batch): 0.057926\n",
      "Epoch 417/1000 train loss: [array(0.05792638, dtype=float32)], val loss: 0.058916978538036346\n",
      "\t partial train loss (single batch): 0.058482\n",
      "Epoch 418/1000 train loss: [array(0.05848172, dtype=float32)], val loss: 0.059502892196178436\n",
      "\t partial train loss (single batch): 0.058124\n",
      "Epoch 419/1000 train loss: [array(0.05812383, dtype=float32)], val loss: 0.057211559265851974\n",
      "\t partial train loss (single batch): 0.057515\n",
      "Epoch 420/1000 train loss: [array(0.05751533, dtype=float32)], val loss: 0.05865398794412613\n",
      "\t partial train loss (single batch): 0.057681\n",
      "Epoch 421/1000 train loss: [array(0.05768061, dtype=float32)], val loss: 0.05873493105173111\n",
      "\t partial train loss (single batch): 0.058603\n",
      "Epoch 422/1000 train loss: [array(0.05860323, dtype=float32)], val loss: 0.057770051062107086\n",
      "\t partial train loss (single batch): 0.057394\n",
      "Epoch 423/1000 train loss: [array(0.05739413, dtype=float32)], val loss: 0.05836866423487663\n",
      "\t partial train loss (single batch): 0.058660\n",
      "Epoch 424/1000 train loss: [array(0.0586596, dtype=float32)], val loss: 0.05857187882065773\n",
      "\t partial train loss (single batch): 0.058627\n",
      "Epoch 425/1000 train loss: [array(0.05862655, dtype=float32)], val loss: 0.05770767480134964\n",
      "\t partial train loss (single batch): 0.056943\n",
      "Epoch 426/1000 train loss: [array(0.05694288, dtype=float32)], val loss: 0.05822739750146866\n",
      "\t partial train loss (single batch): 0.057804\n",
      "Epoch 427/1000 train loss: [array(0.05780353, dtype=float32)], val loss: 0.05876820906996727\n",
      "\t partial train loss (single batch): 0.056399\n",
      "Epoch 428/1000 train loss: [array(0.0563992, dtype=float32)], val loss: 0.05906180292367935\n",
      "\t partial train loss (single batch): 0.054369\n",
      "Epoch 429/1000 train loss: [array(0.05436907, dtype=float32)], val loss: 0.05757579207420349\n",
      "\t partial train loss (single batch): 0.057577\n",
      "Epoch 430/1000 train loss: [array(0.05757658, dtype=float32)], val loss: 0.057024478912353516\n",
      "\t partial train loss (single batch): 0.057326\n",
      "Epoch 431/1000 train loss: [array(0.05732583, dtype=float32)], val loss: 0.05758591741323471\n",
      "\t partial train loss (single batch): 0.056817\n",
      "Epoch 432/1000 train loss: [array(0.05681744, dtype=float32)], val loss: 0.05784016102552414\n",
      "\t partial train loss (single batch): 0.057377\n",
      "Epoch 433/1000 train loss: [array(0.05737725, dtype=float32)], val loss: 0.059600431472063065\n",
      "\t partial train loss (single batch): 0.058819\n",
      "Epoch 434/1000 train loss: [array(0.05881864, dtype=float32)], val loss: 0.058456845581531525\n",
      "\t partial train loss (single batch): 0.056491\n",
      "Epoch 435/1000 train loss: [array(0.05649124, dtype=float32)], val loss: 0.05785664916038513\n",
      "\t partial train loss (single batch): 0.057840\n",
      "Epoch 436/1000 train loss: [array(0.05783994, dtype=float32)], val loss: 0.05699879303574562\n",
      "\t partial train loss (single batch): 0.057102\n",
      "Epoch 437/1000 train loss: [array(0.05710199, dtype=float32)], val loss: 0.05807553604245186\n",
      "\t partial train loss (single batch): 0.057753\n",
      "Epoch 438/1000 train loss: [array(0.0577531, dtype=float32)], val loss: 0.05821714177727699\n",
      "\t partial train loss (single batch): 0.057354\n",
      "Epoch 439/1000 train loss: [array(0.05735372, dtype=float32)], val loss: 0.05617392808198929\n",
      "\t partial train loss (single batch): 0.056839\n",
      "Epoch 440/1000 train loss: [array(0.05683916, dtype=float32)], val loss: 0.05792272463440895\n",
      "\t partial train loss (single batch): 0.058828\n",
      "Epoch 441/1000 train loss: [array(0.05882752, dtype=float32)], val loss: 0.056534767150878906\n",
      "\t partial train loss (single batch): 0.057434\n",
      "Epoch 442/1000 train loss: [array(0.05743368, dtype=float32)], val loss: 0.057327911257743835\n",
      "\t partial train loss (single batch): 0.057353\n",
      "Epoch 443/1000 train loss: [array(0.05735338, dtype=float32)], val loss: 0.0563776008784771\n",
      "\t partial train loss (single batch): 0.058272\n",
      "Epoch 444/1000 train loss: [array(0.05827152, dtype=float32)], val loss: 0.057613179087638855\n",
      "\t partial train loss (single batch): 0.057156\n",
      "Epoch 445/1000 train loss: [array(0.0571565, dtype=float32)], val loss: 0.056152381002902985\n",
      "\t partial train loss (single batch): 0.056776\n",
      "Epoch 446/1000 train loss: [array(0.05677603, dtype=float32)], val loss: 0.05618717148900032\n",
      "\t partial train loss (single batch): 0.058564\n",
      "Epoch 447/1000 train loss: [array(0.05856357, dtype=float32)], val loss: 0.056246642023324966\n",
      "\t partial train loss (single batch): 0.058018\n",
      "Epoch 448/1000 train loss: [array(0.05801815, dtype=float32)], val loss: 0.05738673731684685\n",
      "\t partial train loss (single batch): 0.057053\n",
      "Epoch 449/1000 train loss: [array(0.05705313, dtype=float32)], val loss: 0.056980911642313004\n",
      "\t partial train loss (single batch): 0.056464\n",
      "Epoch 450/1000 train loss: [array(0.05646442, dtype=float32)], val loss: 0.05761634558439255\n",
      "\t partial train loss (single batch): 0.056760\n",
      "Epoch 451/1000 train loss: [array(0.05676028, dtype=float32)], val loss: 0.057974908500909805\n",
      "\t partial train loss (single batch): 0.056745\n",
      "Epoch 452/1000 train loss: [array(0.05674521, dtype=float32)], val loss: 0.056930553168058395\n",
      "\t partial train loss (single batch): 0.056836\n",
      "Epoch 453/1000 train loss: [array(0.05683561, dtype=float32)], val loss: 0.057162195444107056\n",
      "\t partial train loss (single batch): 0.057126\n",
      "Epoch 454/1000 train loss: [array(0.0571262, dtype=float32)], val loss: 0.05910477042198181\n",
      "\t partial train loss (single batch): 0.055475\n",
      "Epoch 455/1000 train loss: [array(0.05547546, dtype=float32)], val loss: 0.05814962834119797\n",
      "\t partial train loss (single batch): 0.055712\n",
      "Epoch 456/1000 train loss: [array(0.05571221, dtype=float32)], val loss: 0.05579133331775665\n",
      "\t partial train loss (single batch): 0.056899\n",
      "Epoch 457/1000 train loss: [array(0.05689901, dtype=float32)], val loss: 0.05600239336490631\n",
      "\t partial train loss (single batch): 0.057434\n",
      "Epoch 458/1000 train loss: [array(0.05743448, dtype=float32)], val loss: 0.055710941553115845\n",
      "\t partial train loss (single batch): 0.056238\n",
      "Epoch 459/1000 train loss: [array(0.0562384, dtype=float32)], val loss: 0.05732759088277817\n",
      "\t partial train loss (single batch): 0.056305\n",
      "Epoch 460/1000 train loss: [array(0.05630487, dtype=float32)], val loss: 0.0560525581240654\n",
      "\t partial train loss (single batch): 0.055761\n",
      "Epoch 461/1000 train loss: [array(0.05576065, dtype=float32)], val loss: 0.055885881185531616\n",
      "\t partial train loss (single batch): 0.056742\n",
      "Epoch 462/1000 train loss: [array(0.05674178, dtype=float32)], val loss: 0.056209299713373184\n",
      "\t partial train loss (single batch): 0.058383\n",
      "Epoch 463/1000 train loss: [array(0.05838284, dtype=float32)], val loss: 0.054618675261735916\n",
      "\t partial train loss (single batch): 0.057169\n",
      "Epoch 464/1000 train loss: [array(0.0571688, dtype=float32)], val loss: 0.056060854345560074\n",
      "\t partial train loss (single batch): 0.057564\n",
      "Epoch 465/1000 train loss: [array(0.05756389, dtype=float32)], val loss: 0.057339444756507874\n",
      "\t partial train loss (single batch): 0.056800\n",
      "Epoch 466/1000 train loss: [array(0.05680018, dtype=float32)], val loss: 0.055613305419683456\n",
      "\t partial train loss (single batch): 0.056449\n",
      "Epoch 467/1000 train loss: [array(0.05644946, dtype=float32)], val loss: 0.05535212159156799\n",
      "\t partial train loss (single batch): 0.056249\n",
      "Epoch 468/1000 train loss: [array(0.05624935, dtype=float32)], val loss: 0.05688469856977463\n",
      "\t partial train loss (single batch): 0.057146\n",
      "Epoch 469/1000 train loss: [array(0.05714649, dtype=float32)], val loss: 0.055770695209503174\n",
      "\t partial train loss (single batch): 0.056751\n",
      "Epoch 470/1000 train loss: [array(0.05675127, dtype=float32)], val loss: 0.055729299783706665\n",
      "\t partial train loss (single batch): 0.056182\n",
      "Epoch 471/1000 train loss: [array(0.05618236, dtype=float32)], val loss: 0.05668002739548683\n",
      "\t partial train loss (single batch): 0.057350\n",
      "Epoch 472/1000 train loss: [array(0.05734973, dtype=float32)], val loss: 0.05513262748718262\n",
      "\t partial train loss (single batch): 0.056251\n",
      "Epoch 473/1000 train loss: [array(0.05625093, dtype=float32)], val loss: 0.05569535493850708\n",
      "\t partial train loss (single batch): 0.055351\n",
      "Epoch 474/1000 train loss: [array(0.05535119, dtype=float32)], val loss: 0.05655086785554886\n",
      "\t partial train loss (single batch): 0.055439\n",
      "Epoch 475/1000 train loss: [array(0.05543884, dtype=float32)], val loss: 0.0576399490237236\n",
      "\t partial train loss (single batch): 0.055668\n",
      "Epoch 476/1000 train loss: [array(0.05566835, dtype=float32)], val loss: 0.05677980184555054\n",
      "\t partial train loss (single batch): 0.054928\n",
      "Epoch 477/1000 train loss: [array(0.05492821, dtype=float32)], val loss: 0.057740483433008194\n",
      "\t partial train loss (single batch): 0.056328\n",
      "Epoch 478/1000 train loss: [array(0.05632799, dtype=float32)], val loss: 0.05778901278972626\n",
      "\t partial train loss (single batch): 0.056521\n",
      "Epoch 479/1000 train loss: [array(0.05652069, dtype=float32)], val loss: 0.05530995875597\n",
      "\t partial train loss (single batch): 0.057410\n",
      "Epoch 480/1000 train loss: [array(0.05740989, dtype=float32)], val loss: 0.054402321577072144\n",
      "\t partial train loss (single batch): 0.056970\n",
      "Epoch 481/1000 train loss: [array(0.05696952, dtype=float32)], val loss: 0.05686378479003906\n",
      "\t partial train loss (single batch): 0.056081\n",
      "Epoch 482/1000 train loss: [array(0.05608139, dtype=float32)], val loss: 0.054880235344171524\n",
      "\t partial train loss (single batch): 0.056620\n",
      "Epoch 483/1000 train loss: [array(0.0566199, dtype=float32)], val loss: 0.05788504704833031\n",
      "\t partial train loss (single batch): 0.056079\n",
      "Epoch 484/1000 train loss: [array(0.05607931, dtype=float32)], val loss: 0.05751784145832062\n",
      "\t partial train loss (single batch): 0.056423\n",
      "Epoch 485/1000 train loss: [array(0.05642255, dtype=float32)], val loss: 0.05587953329086304\n",
      "\t partial train loss (single batch): 0.054733\n",
      "Epoch 486/1000 train loss: [array(0.05473313, dtype=float32)], val loss: 0.055183082818984985\n",
      "\t partial train loss (single batch): 0.057283\n",
      "Epoch 487/1000 train loss: [array(0.05728324, dtype=float32)], val loss: 0.057779211550951004\n",
      "\t partial train loss (single batch): 0.056145\n",
      "Epoch 488/1000 train loss: [array(0.05614527, dtype=float32)], val loss: 0.056544166058301926\n",
      "\t partial train loss (single batch): 0.054564\n",
      "Epoch 489/1000 train loss: [array(0.05456396, dtype=float32)], val loss: 0.055972080677747726\n",
      "\t partial train loss (single batch): 0.055664\n",
      "Epoch 490/1000 train loss: [array(0.05566395, dtype=float32)], val loss: 0.0552200973033905\n",
      "\t partial train loss (single batch): 0.056390\n",
      "Epoch 491/1000 train loss: [array(0.05639009, dtype=float32)], val loss: 0.056534431874752045\n",
      "\t partial train loss (single batch): 0.053441\n",
      "Epoch 492/1000 train loss: [array(0.05344052, dtype=float32)], val loss: 0.056198835372924805\n",
      "\t partial train loss (single batch): 0.055225\n",
      "Epoch 493/1000 train loss: [array(0.05522496, dtype=float32)], val loss: 0.05475606396794319\n",
      "\t partial train loss (single batch): 0.055016\n",
      "Epoch 494/1000 train loss: [array(0.05501588, dtype=float32)], val loss: 0.05498633161187172\n",
      "\t partial train loss (single batch): 0.055377\n",
      "Epoch 495/1000 train loss: [array(0.05537714, dtype=float32)], val loss: 0.054005276411771774\n",
      "\t partial train loss (single batch): 0.053421\n",
      "Epoch 496/1000 train loss: [array(0.05342108, dtype=float32)], val loss: 0.053852301090955734\n",
      "\t partial train loss (single batch): 0.055175\n",
      "Epoch 497/1000 train loss: [array(0.05517475, dtype=float32)], val loss: 0.056389257311820984\n",
      "\t partial train loss (single batch): 0.056571\n",
      "Epoch 498/1000 train loss: [array(0.05657061, dtype=float32)], val loss: 0.05440596491098404\n",
      "\t partial train loss (single batch): 0.055658\n",
      "Epoch 499/1000 train loss: [array(0.0556582, dtype=float32)], val loss: 0.05687090754508972\n",
      "\t partial train loss (single batch): 0.055581\n",
      "Epoch 500/1000 train loss: [array(0.05558062, dtype=float32)], val loss: 0.0557747408747673\n",
      "\t partial train loss (single batch): 0.055595\n",
      "Epoch 501/1000 train loss: [array(0.05559543, dtype=float32)], val loss: 0.05462716519832611\n",
      "\t partial train loss (single batch): 0.054898\n",
      "Epoch 502/1000 train loss: [array(0.05489774, dtype=float32)], val loss: 0.05644100904464722\n",
      "\t partial train loss (single batch): 0.055156\n",
      "Epoch 503/1000 train loss: [array(0.05515631, dtype=float32)], val loss: 0.055448658764362335\n",
      "\t partial train loss (single batch): 0.054625\n",
      "Epoch 504/1000 train loss: [array(0.05462499, dtype=float32)], val loss: 0.056304074823856354\n",
      "\t partial train loss (single batch): 0.055572\n",
      "Epoch 505/1000 train loss: [array(0.0555723, dtype=float32)], val loss: 0.054924022406339645\n",
      "\t partial train loss (single batch): 0.055271\n",
      "Epoch 506/1000 train loss: [array(0.05527128, dtype=float32)], val loss: 0.05558600276708603\n",
      "\t partial train loss (single batch): 0.055623\n",
      "Epoch 507/1000 train loss: [array(0.05562253, dtype=float32)], val loss: 0.056933704763650894\n",
      "\t partial train loss (single batch): 0.056466\n",
      "Epoch 508/1000 train loss: [array(0.05646618, dtype=float32)], val loss: 0.05622749775648117\n",
      "\t partial train loss (single batch): 0.055308\n",
      "Epoch 509/1000 train loss: [array(0.05530811, dtype=float32)], val loss: 0.05516200140118599\n",
      "\t partial train loss (single batch): 0.054704\n",
      "Epoch 510/1000 train loss: [array(0.05470422, dtype=float32)], val loss: 0.05825253576040268\n",
      "\t partial train loss (single batch): 0.054820\n",
      "Epoch 511/1000 train loss: [array(0.05481956, dtype=float32)], val loss: 0.05365479364991188\n",
      "\t partial train loss (single batch): 0.055323\n",
      "Epoch 512/1000 train loss: [array(0.05532333, dtype=float32)], val loss: 0.054936882108449936\n",
      "\t partial train loss (single batch): 0.054330\n",
      "Epoch 513/1000 train loss: [array(0.05433017, dtype=float32)], val loss: 0.054333850741386414\n",
      "\t partial train loss (single batch): 0.054127\n",
      "Epoch 514/1000 train loss: [array(0.05412697, dtype=float32)], val loss: 0.05514116212725639\n",
      "\t partial train loss (single batch): 0.055219\n",
      "Epoch 515/1000 train loss: [array(0.05521886, dtype=float32)], val loss: 0.057277996093034744\n",
      "\t partial train loss (single batch): 0.056619\n",
      "Epoch 516/1000 train loss: [array(0.05661944, dtype=float32)], val loss: 0.054990846663713455\n",
      "\t partial train loss (single batch): 0.054062\n",
      "Epoch 517/1000 train loss: [array(0.05406195, dtype=float32)], val loss: 0.05535567179322243\n",
      "\t partial train loss (single batch): 0.054990\n",
      "Epoch 518/1000 train loss: [array(0.05499042, dtype=float32)], val loss: 0.05348246544599533\n",
      "\t partial train loss (single batch): 0.056539\n",
      "Epoch 519/1000 train loss: [array(0.05653903, dtype=float32)], val loss: 0.05526556447148323\n",
      "\t partial train loss (single batch): 0.054479\n",
      "Epoch 520/1000 train loss: [array(0.05447916, dtype=float32)], val loss: 0.05505084991455078\n",
      "\t partial train loss (single batch): 0.055207\n",
      "Epoch 521/1000 train loss: [array(0.05520684, dtype=float32)], val loss: 0.05492968484759331\n",
      "\t partial train loss (single batch): 0.053456\n",
      "Epoch 522/1000 train loss: [array(0.05345559, dtype=float32)], val loss: 0.05359538272023201\n",
      "\t partial train loss (single batch): 0.054654\n",
      "Epoch 523/1000 train loss: [array(0.05465425, dtype=float32)], val loss: 0.055313415825366974\n",
      "\t partial train loss (single batch): 0.055160\n",
      "Epoch 524/1000 train loss: [array(0.05515986, dtype=float32)], val loss: 0.05473458394408226\n",
      "\t partial train loss (single batch): 0.054994\n",
      "Epoch 525/1000 train loss: [array(0.05499383, dtype=float32)], val loss: 0.05418679118156433\n",
      "\t partial train loss (single batch): 0.054612\n",
      "Epoch 526/1000 train loss: [array(0.05461168, dtype=float32)], val loss: 0.05453680083155632\n",
      "\t partial train loss (single batch): 0.055186\n",
      "Epoch 527/1000 train loss: [array(0.05518551, dtype=float32)], val loss: 0.056336112320423126\n",
      "\t partial train loss (single batch): 0.053378\n",
      "Epoch 528/1000 train loss: [array(0.05337822, dtype=float32)], val loss: 0.055090758949518204\n",
      "\t partial train loss (single batch): 0.054740\n",
      "Epoch 529/1000 train loss: [array(0.05473971, dtype=float32)], val loss: 0.05613447353243828\n",
      "\t partial train loss (single batch): 0.055696\n",
      "Epoch 530/1000 train loss: [array(0.05569602, dtype=float32)], val loss: 0.05480523034930229\n",
      "\t partial train loss (single batch): 0.055538\n",
      "Epoch 531/1000 train loss: [array(0.05553785, dtype=float32)], val loss: 0.05595426261425018\n",
      "\t partial train loss (single batch): 0.054321\n",
      "Epoch 532/1000 train loss: [array(0.054321, dtype=float32)], val loss: 0.05530634522438049\n",
      "\t partial train loss (single batch): 0.054033\n",
      "Epoch 533/1000 train loss: [array(0.05403304, dtype=float32)], val loss: 0.05373738706111908\n",
      "\t partial train loss (single batch): 0.053071\n",
      "Epoch 534/1000 train loss: [array(0.05307126, dtype=float32)], val loss: 0.05577307939529419\n",
      "\t partial train loss (single batch): 0.055043\n",
      "Epoch 535/1000 train loss: [array(0.05504255, dtype=float32)], val loss: 0.05706910043954849\n",
      "\t partial train loss (single batch): 0.053281\n",
      "Epoch 536/1000 train loss: [array(0.05328088, dtype=float32)], val loss: 0.05428715795278549\n",
      "\t partial train loss (single batch): 0.054247\n",
      "Epoch 537/1000 train loss: [array(0.05424717, dtype=float32)], val loss: 0.05536620318889618\n",
      "\t partial train loss (single batch): 0.055061\n",
      "Epoch 538/1000 train loss: [array(0.0550608, dtype=float32)], val loss: 0.0540655292570591\n",
      "\t partial train loss (single batch): 0.054477\n",
      "Epoch 539/1000 train loss: [array(0.05447683, dtype=float32)], val loss: 0.05509400740265846\n",
      "\t partial train loss (single batch): 0.055916\n",
      "Epoch 540/1000 train loss: [array(0.05591593, dtype=float32)], val loss: 0.05531378835439682\n",
      "\t partial train loss (single batch): 0.055073\n",
      "Epoch 541/1000 train loss: [array(0.05507305, dtype=float32)], val loss: 0.053283195942640305\n",
      "\t partial train loss (single batch): 0.054015\n",
      "Epoch 542/1000 train loss: [array(0.05401526, dtype=float32)], val loss: 0.05409833416342735\n",
      "\t partial train loss (single batch): 0.053537\n",
      "Epoch 543/1000 train loss: [array(0.05353681, dtype=float32)], val loss: 0.054071445018053055\n",
      "\t partial train loss (single batch): 0.054220\n",
      "Epoch 544/1000 train loss: [array(0.05422012, dtype=float32)], val loss: 0.05394405126571655\n",
      "\t partial train loss (single batch): 0.052766\n",
      "Epoch 545/1000 train loss: [array(0.05276634, dtype=float32)], val loss: 0.053803298622369766\n",
      "\t partial train loss (single batch): 0.054551\n",
      "Epoch 546/1000 train loss: [array(0.05455091, dtype=float32)], val loss: 0.05413242429494858\n",
      "\t partial train loss (single batch): 0.054794\n",
      "Epoch 547/1000 train loss: [array(0.05479432, dtype=float32)], val loss: 0.054349809885025024\n",
      "\t partial train loss (single batch): 0.054658\n",
      "Epoch 548/1000 train loss: [array(0.05465751, dtype=float32)], val loss: 0.05495825037360191\n",
      "\t partial train loss (single batch): 0.054405\n",
      "Epoch 549/1000 train loss: [array(0.05440518, dtype=float32)], val loss: 0.055539779365062714\n",
      "\t partial train loss (single batch): 0.053080\n",
      "Epoch 550/1000 train loss: [array(0.05308005, dtype=float32)], val loss: 0.0565333366394043\n",
      "\t partial train loss (single batch): 0.056676\n",
      "Epoch 551/1000 train loss: [array(0.05667586, dtype=float32)], val loss: 0.05385980010032654\n",
      "\t partial train loss (single batch): 0.054713\n",
      "Epoch 552/1000 train loss: [array(0.05471277, dtype=float32)], val loss: 0.055045079439878464\n",
      "\t partial train loss (single batch): 0.055196\n",
      "Epoch 553/1000 train loss: [array(0.05519635, dtype=float32)], val loss: 0.05629238858819008\n",
      "\t partial train loss (single batch): 0.054058\n",
      "Epoch 554/1000 train loss: [array(0.05405821, dtype=float32)], val loss: 0.05514926835894585\n",
      "\t partial train loss (single batch): 0.054562\n",
      "Epoch 555/1000 train loss: [array(0.05456152, dtype=float32)], val loss: 0.05532760173082352\n",
      "\t partial train loss (single batch): 0.053774\n",
      "Epoch 556/1000 train loss: [array(0.05377375, dtype=float32)], val loss: 0.053845178335905075\n",
      "\t partial train loss (single batch): 0.056516\n",
      "Epoch 557/1000 train loss: [array(0.05651612, dtype=float32)], val loss: 0.054599810391664505\n",
      "\t partial train loss (single batch): 0.053629\n",
      "Epoch 558/1000 train loss: [array(0.05362917, dtype=float32)], val loss: 0.054350610822439194\n",
      "\t partial train loss (single batch): 0.053742\n",
      "Epoch 559/1000 train loss: [array(0.05374154, dtype=float32)], val loss: 0.0550154373049736\n",
      "\t partial train loss (single batch): 0.052965\n",
      "Epoch 560/1000 train loss: [array(0.05296528, dtype=float32)], val loss: 0.05328577756881714\n",
      "\t partial train loss (single batch): 0.053084\n",
      "Epoch 561/1000 train loss: [array(0.05308399, dtype=float32)], val loss: 0.05431857705116272\n",
      "\t partial train loss (single batch): 0.055177\n",
      "Epoch 562/1000 train loss: [array(0.05517688, dtype=float32)], val loss: 0.054853443056344986\n",
      "\t partial train loss (single batch): 0.055191\n",
      "Epoch 563/1000 train loss: [array(0.05519123, dtype=float32)], val loss: 0.0541982464492321\n",
      "\t partial train loss (single batch): 0.054696\n",
      "Epoch 564/1000 train loss: [array(0.05469606, dtype=float32)], val loss: 0.056148916482925415\n",
      "\t partial train loss (single batch): 0.053320\n",
      "Epoch 565/1000 train loss: [array(0.05331989, dtype=float32)], val loss: 0.053566932678222656\n",
      "\t partial train loss (single batch): 0.054123\n",
      "Epoch 566/1000 train loss: [array(0.05412283, dtype=float32)], val loss: 0.054260194301605225\n",
      "\t partial train loss (single batch): 0.054399\n",
      "Epoch 567/1000 train loss: [array(0.05439943, dtype=float32)], val loss: 0.05507676303386688\n",
      "\t partial train loss (single batch): 0.054324\n",
      "Epoch 568/1000 train loss: [array(0.05432378, dtype=float32)], val loss: 0.05372491106390953\n",
      "\t partial train loss (single batch): 0.054116\n",
      "Epoch 569/1000 train loss: [array(0.05411628, dtype=float32)], val loss: 0.054789911955595016\n",
      "\t partial train loss (single batch): 0.054899\n",
      "Epoch 570/1000 train loss: [array(0.05489896, dtype=float32)], val loss: 0.05401389300823212\n",
      "\t partial train loss (single batch): 0.054752\n",
      "Epoch 571/1000 train loss: [array(0.05475241, dtype=float32)], val loss: 0.05230476334691048\n",
      "\t partial train loss (single batch): 0.054415\n",
      "Epoch 572/1000 train loss: [array(0.05441475, dtype=float32)], val loss: 0.054715026170015335\n",
      "\t partial train loss (single batch): 0.054287\n",
      "Epoch 573/1000 train loss: [array(0.05428683, dtype=float32)], val loss: 0.05406266823410988\n",
      "\t partial train loss (single batch): 0.053912\n",
      "Epoch 574/1000 train loss: [array(0.0539123, dtype=float32)], val loss: 0.052340272814035416\n",
      "\t partial train loss (single batch): 0.053486\n",
      "Epoch 575/1000 train loss: [array(0.0534856, dtype=float32)], val loss: 0.0535859614610672\n",
      "\t partial train loss (single batch): 0.054435\n",
      "Epoch 576/1000 train loss: [array(0.0544352, dtype=float32)], val loss: 0.05359271168708801\n",
      "\t partial train loss (single batch): 0.052306\n",
      "Epoch 577/1000 train loss: [array(0.05230646, dtype=float32)], val loss: 0.05315727740526199\n",
      "\t partial train loss (single batch): 0.054633\n",
      "Epoch 578/1000 train loss: [array(0.05463295, dtype=float32)], val loss: 0.053573865443468094\n",
      "\t partial train loss (single batch): 0.053917\n",
      "Epoch 579/1000 train loss: [array(0.0539165, dtype=float32)], val loss: 0.05520148202776909\n",
      "\t partial train loss (single batch): 0.054138\n",
      "Epoch 580/1000 train loss: [array(0.05413766, dtype=float32)], val loss: 0.055276237428188324\n",
      "\t partial train loss (single batch): 0.054135\n",
      "Epoch 581/1000 train loss: [array(0.05413498, dtype=float32)], val loss: 0.053948067128658295\n",
      "\t partial train loss (single batch): 0.051989\n",
      "Epoch 582/1000 train loss: [array(0.05198937, dtype=float32)], val loss: 0.05399252474308014\n",
      "\t partial train loss (single batch): 0.052721\n",
      "Epoch 583/1000 train loss: [array(0.05272092, dtype=float32)], val loss: 0.05404626578092575\n",
      "\t partial train loss (single batch): 0.054419\n",
      "Epoch 584/1000 train loss: [array(0.05441923, dtype=float32)], val loss: 0.05442483723163605\n",
      "\t partial train loss (single batch): 0.054307\n",
      "Epoch 585/1000 train loss: [array(0.05430715, dtype=float32)], val loss: 0.05333782732486725\n",
      "\t partial train loss (single batch): 0.051707\n",
      "Epoch 586/1000 train loss: [array(0.05170722, dtype=float32)], val loss: 0.05186471343040466\n",
      "\t partial train loss (single batch): 0.054689\n",
      "Epoch 587/1000 train loss: [array(0.05468912, dtype=float32)], val loss: 0.052392758429050446\n",
      "\t partial train loss (single batch): 0.054089\n",
      "Epoch 588/1000 train loss: [array(0.05408873, dtype=float32)], val loss: 0.05352984741330147\n",
      "\t partial train loss (single batch): 0.054266\n",
      "Epoch 589/1000 train loss: [array(0.0542659, dtype=float32)], val loss: 0.053070809692144394\n",
      "\t partial train loss (single batch): 0.054024\n",
      "Epoch 590/1000 train loss: [array(0.05402392, dtype=float32)], val loss: 0.05432121083140373\n",
      "\t partial train loss (single batch): 0.053528\n",
      "Epoch 591/1000 train loss: [array(0.05352829, dtype=float32)], val loss: 0.05217237025499344\n",
      "\t partial train loss (single batch): 0.054186\n",
      "Epoch 592/1000 train loss: [array(0.05418626, dtype=float32)], val loss: 0.0529896579682827\n",
      "\t partial train loss (single batch): 0.052601\n",
      "Epoch 593/1000 train loss: [array(0.05260114, dtype=float32)], val loss: 0.05291227996349335\n",
      "\t partial train loss (single batch): 0.051907\n",
      "Epoch 594/1000 train loss: [array(0.05190691, dtype=float32)], val loss: 0.05394481122493744\n",
      "\t partial train loss (single batch): 0.054010\n",
      "Epoch 595/1000 train loss: [array(0.0540099, dtype=float32)], val loss: 0.05309971049427986\n",
      "\t partial train loss (single batch): 0.052840\n",
      "Epoch 596/1000 train loss: [array(0.05284026, dtype=float32)], val loss: 0.053641095757484436\n",
      "\t partial train loss (single batch): 0.054003\n",
      "Epoch 597/1000 train loss: [array(0.05400322, dtype=float32)], val loss: 0.05367470905184746\n",
      "\t partial train loss (single batch): 0.054297\n",
      "Epoch 598/1000 train loss: [array(0.05429686, dtype=float32)], val loss: 0.05368828400969505\n",
      "\t partial train loss (single batch): 0.054575\n",
      "Epoch 599/1000 train loss: [array(0.05457453, dtype=float32)], val loss: 0.05270025506615639\n",
      "\t partial train loss (single batch): 0.052315\n",
      "Epoch 600/1000 train loss: [array(0.05231489, dtype=float32)], val loss: 0.05249969661235809\n",
      "\t partial train loss (single batch): 0.053773\n",
      "Epoch 601/1000 train loss: [array(0.05377334, dtype=float32)], val loss: 0.05277828127145767\n",
      "\t partial train loss (single batch): 0.053169\n",
      "Epoch 602/1000 train loss: [array(0.05316881, dtype=float32)], val loss: 0.053089242428541183\n",
      "\t partial train loss (single batch): 0.052940\n",
      "Epoch 603/1000 train loss: [array(0.05294028, dtype=float32)], val loss: 0.053899768739938736\n",
      "\t partial train loss (single batch): 0.052834\n",
      "Epoch 604/1000 train loss: [array(0.05283393, dtype=float32)], val loss: 0.05327959358692169\n",
      "\t partial train loss (single batch): 0.051340\n",
      "Epoch 605/1000 train loss: [array(0.05133992, dtype=float32)], val loss: 0.052263207733631134\n",
      "\t partial train loss (single batch): 0.052096\n",
      "Epoch 606/1000 train loss: [array(0.05209634, dtype=float32)], val loss: 0.05149240419268608\n",
      "\t partial train loss (single batch): 0.051644\n",
      "Epoch 607/1000 train loss: [array(0.05164381, dtype=float32)], val loss: 0.05400925129652023\n",
      "\t partial train loss (single batch): 0.053181\n",
      "Epoch 608/1000 train loss: [array(0.0531815, dtype=float32)], val loss: 0.053276583552360535\n",
      "\t partial train loss (single batch): 0.051606\n",
      "Epoch 609/1000 train loss: [array(0.05160577, dtype=float32)], val loss: 0.05278691649436951\n",
      "\t partial train loss (single batch): 0.050356\n",
      "Epoch 610/1000 train loss: [array(0.05035581, dtype=float32)], val loss: 0.052989836782217026\n",
      "\t partial train loss (single batch): 0.052463\n",
      "Epoch 611/1000 train loss: [array(0.05246319, dtype=float32)], val loss: 0.05329098924994469\n",
      "\t partial train loss (single batch): 0.051862\n",
      "Epoch 612/1000 train loss: [array(0.05186209, dtype=float32)], val loss: 0.05347936972975731\n",
      "\t partial train loss (single batch): 0.053021\n",
      "Epoch 613/1000 train loss: [array(0.05302082, dtype=float32)], val loss: 0.05270574986934662\n",
      "\t partial train loss (single batch): 0.053484\n",
      "Epoch 614/1000 train loss: [array(0.05348437, dtype=float32)], val loss: 0.050861116498708725\n",
      "\t partial train loss (single batch): 0.052839\n",
      "Epoch 615/1000 train loss: [array(0.05283941, dtype=float32)], val loss: 0.05252990871667862\n",
      "\t partial train loss (single batch): 0.052466\n",
      "Epoch 616/1000 train loss: [array(0.05246571, dtype=float32)], val loss: 0.052196405827999115\n",
      "\t partial train loss (single batch): 0.051477\n",
      "Epoch 617/1000 train loss: [array(0.05147723, dtype=float32)], val loss: 0.05366608873009682\n",
      "\t partial train loss (single batch): 0.052809\n",
      "Epoch 618/1000 train loss: [array(0.05280882, dtype=float32)], val loss: 0.052551671862602234\n",
      "\t partial train loss (single batch): 0.054317\n",
      "Epoch 619/1000 train loss: [array(0.05431728, dtype=float32)], val loss: 0.05372709780931473\n",
      "\t partial train loss (single batch): 0.053206\n",
      "Epoch 620/1000 train loss: [array(0.05320565, dtype=float32)], val loss: 0.05443277582526207\n",
      "\t partial train loss (single batch): 0.052082\n",
      "Epoch 621/1000 train loss: [array(0.05208228, dtype=float32)], val loss: 0.05374046787619591\n",
      "\t partial train loss (single batch): 0.054051\n",
      "Epoch 622/1000 train loss: [array(0.05405077, dtype=float32)], val loss: 0.052443742752075195\n",
      "\t partial train loss (single batch): 0.052865\n",
      "Epoch 623/1000 train loss: [array(0.05286485, dtype=float32)], val loss: 0.05239122733473778\n",
      "\t partial train loss (single batch): 0.052942\n",
      "Epoch 624/1000 train loss: [array(0.05294156, dtype=float32)], val loss: 0.05381523817777634\n",
      "\t partial train loss (single batch): 0.052591\n",
      "Epoch 625/1000 train loss: [array(0.05259109, dtype=float32)], val loss: 0.05385138466954231\n",
      "\t partial train loss (single batch): 0.051204\n",
      "Epoch 626/1000 train loss: [array(0.0512043, dtype=float32)], val loss: 0.05262237787246704\n",
      "\t partial train loss (single batch): 0.054182\n",
      "Epoch 627/1000 train loss: [array(0.05418153, dtype=float32)], val loss: 0.05276145413517952\n",
      "\t partial train loss (single batch): 0.052580\n",
      "Epoch 628/1000 train loss: [array(0.05257956, dtype=float32)], val loss: 0.052960317581892014\n",
      "\t partial train loss (single batch): 0.052824\n",
      "Epoch 629/1000 train loss: [array(0.05282391, dtype=float32)], val loss: 0.05286916345357895\n",
      "\t partial train loss (single batch): 0.050982\n",
      "Epoch 630/1000 train loss: [array(0.05098172, dtype=float32)], val loss: 0.052394215017557144\n",
      "\t partial train loss (single batch): 0.053267\n",
      "Epoch 631/1000 train loss: [array(0.05326748, dtype=float32)], val loss: 0.052068088203668594\n",
      "\t partial train loss (single batch): 0.054400\n",
      "Epoch 632/1000 train loss: [array(0.05440003, dtype=float32)], val loss: 0.05278997868299484\n",
      "\t partial train loss (single batch): 0.051791\n",
      "Epoch 633/1000 train loss: [array(0.05179108, dtype=float32)], val loss: 0.05251947045326233\n",
      "\t partial train loss (single batch): 0.052761\n",
      "Epoch 634/1000 train loss: [array(0.05276073, dtype=float32)], val loss: 0.05266936868429184\n",
      "\t partial train loss (single batch): 0.053578\n",
      "Epoch 635/1000 train loss: [array(0.05357822, dtype=float32)], val loss: 0.051486819982528687\n",
      "\t partial train loss (single batch): 0.052605\n",
      "Epoch 636/1000 train loss: [array(0.0526049, dtype=float32)], val loss: 0.05301786959171295\n",
      "\t partial train loss (single batch): 0.052943\n",
      "Epoch 637/1000 train loss: [array(0.05294311, dtype=float32)], val loss: 0.05215509980916977\n",
      "\t partial train loss (single batch): 0.052780\n",
      "Epoch 638/1000 train loss: [array(0.05277993, dtype=float32)], val loss: 0.05412888154387474\n",
      "\t partial train loss (single batch): 0.053061\n",
      "Epoch 639/1000 train loss: [array(0.0530611, dtype=float32)], val loss: 0.05286720395088196\n",
      "\t partial train loss (single batch): 0.052808\n",
      "Epoch 640/1000 train loss: [array(0.05280838, dtype=float32)], val loss: 0.05221858248114586\n",
      "\t partial train loss (single batch): 0.053962\n",
      "Epoch 641/1000 train loss: [array(0.05396174, dtype=float32)], val loss: 0.0534493587911129\n",
      "\t partial train loss (single batch): 0.051739\n",
      "Epoch 642/1000 train loss: [array(0.05173877, dtype=float32)], val loss: 0.052826035767793655\n",
      "\t partial train loss (single batch): 0.052268\n",
      "Epoch 643/1000 train loss: [array(0.05226812, dtype=float32)], val loss: 0.052701666951179504\n",
      "\t partial train loss (single batch): 0.052483\n",
      "Epoch 644/1000 train loss: [array(0.05248347, dtype=float32)], val loss: 0.051965758204460144\n",
      "\t partial train loss (single batch): 0.052852\n",
      "Epoch 645/1000 train loss: [array(0.05285241, dtype=float32)], val loss: 0.051743343472480774\n",
      "\t partial train loss (single batch): 0.052150\n",
      "Epoch 646/1000 train loss: [array(0.05215039, dtype=float32)], val loss: 0.05308782681822777\n",
      "\t partial train loss (single batch): 0.051846\n",
      "Epoch 647/1000 train loss: [array(0.051846, dtype=float32)], val loss: 0.05204005539417267\n",
      "\t partial train loss (single batch): 0.051405\n",
      "Epoch 648/1000 train loss: [array(0.05140475, dtype=float32)], val loss: 0.05210280790925026\n",
      "\t partial train loss (single batch): 0.049879\n",
      "Epoch 649/1000 train loss: [array(0.04987907, dtype=float32)], val loss: 0.0520230308175087\n",
      "\t partial train loss (single batch): 0.052625\n",
      "Epoch 650/1000 train loss: [array(0.05262542, dtype=float32)], val loss: 0.05225445330142975\n",
      "\t partial train loss (single batch): 0.052594\n",
      "Epoch 651/1000 train loss: [array(0.05259382, dtype=float32)], val loss: 0.05238771811127663\n",
      "\t partial train loss (single batch): 0.053892\n",
      "Epoch 652/1000 train loss: [array(0.05389179, dtype=float32)], val loss: 0.052257221192121506\n",
      "\t partial train loss (single batch): 0.051781\n",
      "Epoch 653/1000 train loss: [array(0.05178123, dtype=float32)], val loss: 0.051657840609550476\n",
      "\t partial train loss (single batch): 0.051640\n",
      "Epoch 654/1000 train loss: [array(0.05163957, dtype=float32)], val loss: 0.05221718177199364\n",
      "\t partial train loss (single batch): 0.051168\n",
      "Epoch 655/1000 train loss: [array(0.0511677, dtype=float32)], val loss: 0.05193287879228592\n",
      "\t partial train loss (single batch): 0.050940\n",
      "Epoch 656/1000 train loss: [array(0.05093981, dtype=float32)], val loss: 0.05394718423485756\n",
      "\t partial train loss (single batch): 0.052093\n",
      "Epoch 657/1000 train loss: [array(0.05209259, dtype=float32)], val loss: 0.051906343549489975\n",
      "\t partial train loss (single batch): 0.052427\n",
      "Epoch 658/1000 train loss: [array(0.05242685, dtype=float32)], val loss: 0.05110752210021019\n",
      "\t partial train loss (single batch): 0.052408\n",
      "Epoch 659/1000 train loss: [array(0.05240839, dtype=float32)], val loss: 0.05373696982860565\n",
      "\t partial train loss (single batch): 0.053455\n",
      "Epoch 660/1000 train loss: [array(0.05345455, dtype=float32)], val loss: 0.051971085369586945\n",
      "\t partial train loss (single batch): 0.050851\n",
      "Epoch 661/1000 train loss: [array(0.05085072, dtype=float32)], val loss: 0.05418560281395912\n",
      "\t partial train loss (single batch): 0.053040\n",
      "Epoch 662/1000 train loss: [array(0.05304021, dtype=float32)], val loss: 0.05275050923228264\n",
      "\t partial train loss (single batch): 0.050664\n",
      "Epoch 663/1000 train loss: [array(0.0506636, dtype=float32)], val loss: 0.05245101824402809\n",
      "\t partial train loss (single batch): 0.053138\n",
      "Epoch 664/1000 train loss: [array(0.05313842, dtype=float32)], val loss: 0.05208338052034378\n",
      "\t partial train loss (single batch): 0.051500\n",
      "Epoch 665/1000 train loss: [array(0.05150018, dtype=float32)], val loss: 0.052935633808374405\n",
      "\t partial train loss (single batch): 0.051958\n",
      "Epoch 666/1000 train loss: [array(0.05195842, dtype=float32)], val loss: 0.052984558045864105\n",
      "\t partial train loss (single batch): 0.052382\n",
      "Epoch 667/1000 train loss: [array(0.05238175, dtype=float32)], val loss: 0.05257973447442055\n",
      "\t partial train loss (single batch): 0.051349\n",
      "Epoch 668/1000 train loss: [array(0.05134873, dtype=float32)], val loss: 0.05132254213094711\n",
      "\t partial train loss (single batch): 0.052103\n",
      "Epoch 669/1000 train loss: [array(0.05210342, dtype=float32)], val loss: 0.05171114206314087\n",
      "\t partial train loss (single batch): 0.051255\n",
      "Epoch 670/1000 train loss: [array(0.05125541, dtype=float32)], val loss: 0.050461672246456146\n",
      "\t partial train loss (single batch): 0.051437\n",
      "Epoch 671/1000 train loss: [array(0.05143664, dtype=float32)], val loss: 0.05268529802560806\n",
      "\t partial train loss (single batch): 0.052102\n",
      "Epoch 672/1000 train loss: [array(0.05210179, dtype=float32)], val loss: 0.05272146314382553\n",
      "\t partial train loss (single batch): 0.051705\n",
      "Epoch 673/1000 train loss: [array(0.05170506, dtype=float32)], val loss: 0.05151829496026039\n",
      "\t partial train loss (single batch): 0.050937\n",
      "Epoch 674/1000 train loss: [array(0.0509371, dtype=float32)], val loss: 0.05173409730195999\n",
      "\t partial train loss (single batch): 0.052855\n",
      "Epoch 675/1000 train loss: [array(0.0528552, dtype=float32)], val loss: 0.051554158329963684\n",
      "\t partial train loss (single batch): 0.052498\n",
      "Epoch 676/1000 train loss: [array(0.0524978, dtype=float32)], val loss: 0.051924701780080795\n",
      "\t partial train loss (single batch): 0.052566\n",
      "Epoch 677/1000 train loss: [array(0.05256609, dtype=float32)], val loss: 0.051762327551841736\n",
      "\t partial train loss (single batch): 0.053303\n",
      "Epoch 678/1000 train loss: [array(0.05330291, dtype=float32)], val loss: 0.05142459645867348\n",
      "\t partial train loss (single batch): 0.052811\n",
      "Epoch 679/1000 train loss: [array(0.05281105, dtype=float32)], val loss: 0.0532272644340992\n",
      "\t partial train loss (single batch): 0.052453\n",
      "Epoch 680/1000 train loss: [array(0.05245315, dtype=float32)], val loss: 0.051835350692272186\n",
      "\t partial train loss (single batch): 0.052487\n",
      "Epoch 681/1000 train loss: [array(0.05248671, dtype=float32)], val loss: 0.052315209060907364\n",
      "\t partial train loss (single batch): 0.050850\n",
      "Epoch 682/1000 train loss: [array(0.05084975, dtype=float32)], val loss: 0.05160241201519966\n",
      "\t partial train loss (single batch): 0.052057\n",
      "Epoch 683/1000 train loss: [array(0.05205694, dtype=float32)], val loss: 0.05306970700621605\n",
      "\t partial train loss (single batch): 0.050643\n",
      "Epoch 684/1000 train loss: [array(0.05064258, dtype=float32)], val loss: 0.052413612604141235\n",
      "\t partial train loss (single batch): 0.051793\n",
      "Epoch 685/1000 train loss: [array(0.05179277, dtype=float32)], val loss: 0.05332775413990021\n",
      "\t partial train loss (single batch): 0.052315\n",
      "Epoch 686/1000 train loss: [array(0.05231499, dtype=float32)], val loss: 0.05256487429141998\n",
      "\t partial train loss (single batch): 0.051589\n",
      "Epoch 687/1000 train loss: [array(0.05158907, dtype=float32)], val loss: 0.05087053403258324\n",
      "\t partial train loss (single batch): 0.051063\n",
      "Epoch 688/1000 train loss: [array(0.05106318, dtype=float32)], val loss: 0.05203816294670105\n",
      "\t partial train loss (single batch): 0.050343\n",
      "Epoch 689/1000 train loss: [array(0.05034331, dtype=float32)], val loss: 0.05085911229252815\n",
      "\t partial train loss (single batch): 0.051988\n",
      "Epoch 690/1000 train loss: [array(0.05198807, dtype=float32)], val loss: 0.05196105316281319\n",
      "\t partial train loss (single batch): 0.052407\n",
      "Epoch 691/1000 train loss: [array(0.05240729, dtype=float32)], val loss: 0.05086855590343475\n",
      "\t partial train loss (single batch): 0.050636\n",
      "Epoch 692/1000 train loss: [array(0.05063574, dtype=float32)], val loss: 0.05198025703430176\n",
      "\t partial train loss (single batch): 0.052270\n",
      "Epoch 693/1000 train loss: [array(0.0522702, dtype=float32)], val loss: 0.05387596786022186\n",
      "\t partial train loss (single batch): 0.051838\n",
      "Epoch 694/1000 train loss: [array(0.05183777, dtype=float32)], val loss: 0.05113522708415985\n",
      "\t partial train loss (single batch): 0.050974\n",
      "Epoch 695/1000 train loss: [array(0.05097356, dtype=float32)], val loss: 0.05251067504286766\n",
      "\t partial train loss (single batch): 0.052461\n",
      "Epoch 696/1000 train loss: [array(0.05246112, dtype=float32)], val loss: 0.0509973019361496\n",
      "\t partial train loss (single batch): 0.050954\n",
      "Epoch 697/1000 train loss: [array(0.05095389, dtype=float32)], val loss: 0.051315706223249435\n",
      "\t partial train loss (single batch): 0.051482\n",
      "Epoch 698/1000 train loss: [array(0.05148186, dtype=float32)], val loss: 0.05236447975039482\n",
      "\t partial train loss (single batch): 0.052066\n",
      "Epoch 699/1000 train loss: [array(0.05206621, dtype=float32)], val loss: 0.05157027393579483\n",
      "\t partial train loss (single batch): 0.052971\n",
      "Epoch 700/1000 train loss: [array(0.05297109, dtype=float32)], val loss: 0.049717556685209274\n",
      "\t partial train loss (single batch): 0.050112\n",
      "Epoch 701/1000 train loss: [array(0.05011176, dtype=float32)], val loss: 0.051498472690582275\n",
      "\t partial train loss (single batch): 0.050585\n",
      "Epoch 702/1000 train loss: [array(0.05058534, dtype=float32)], val loss: 0.05155780538916588\n",
      "\t partial train loss (single batch): 0.051850\n",
      "Epoch 703/1000 train loss: [array(0.05184972, dtype=float32)], val loss: 0.051279544830322266\n",
      "\t partial train loss (single batch): 0.050810\n",
      "Epoch 704/1000 train loss: [array(0.05081021, dtype=float32)], val loss: 0.051622480154037476\n",
      "\t partial train loss (single batch): 0.051034\n",
      "Epoch 705/1000 train loss: [array(0.05103364, dtype=float32)], val loss: 0.05130143463611603\n",
      "\t partial train loss (single batch): 0.050460\n",
      "Epoch 706/1000 train loss: [array(0.05046037, dtype=float32)], val loss: 0.05095680058002472\n",
      "\t partial train loss (single batch): 0.051867\n",
      "Epoch 707/1000 train loss: [array(0.0518667, dtype=float32)], val loss: 0.05190281942486763\n",
      "\t partial train loss (single batch): 0.051161\n",
      "Epoch 708/1000 train loss: [array(0.05116133, dtype=float32)], val loss: 0.050301678478717804\n",
      "\t partial train loss (single batch): 0.050818\n",
      "Epoch 709/1000 train loss: [array(0.05081787, dtype=float32)], val loss: 0.05127739533782005\n",
      "\t partial train loss (single batch): 0.052242\n",
      "Epoch 710/1000 train loss: [array(0.05224234, dtype=float32)], val loss: 0.05164148285984993\n",
      "\t partial train loss (single batch): 0.053209\n",
      "Epoch 711/1000 train loss: [array(0.05320879, dtype=float32)], val loss: 0.05156925320625305\n",
      "\t partial train loss (single batch): 0.052263\n",
      "Epoch 712/1000 train loss: [array(0.0522633, dtype=float32)], val loss: 0.051415424793958664\n",
      "\t partial train loss (single batch): 0.053574\n",
      "Epoch 713/1000 train loss: [array(0.05357439, dtype=float32)], val loss: 0.051756907254457474\n",
      "\t partial train loss (single batch): 0.052119\n",
      "Epoch 714/1000 train loss: [array(0.05211913, dtype=float32)], val loss: 0.051191531121730804\n",
      "\t partial train loss (single batch): 0.051645\n",
      "Epoch 715/1000 train loss: [array(0.05164534, dtype=float32)], val loss: 0.0511588379740715\n",
      "\t partial train loss (single batch): 0.050746\n",
      "Epoch 716/1000 train loss: [array(0.05074639, dtype=float32)], val loss: 0.050868257880210876\n",
      "\t partial train loss (single batch): 0.049797\n",
      "Epoch 717/1000 train loss: [array(0.04979733, dtype=float32)], val loss: 0.05161256343126297\n",
      "\t partial train loss (single batch): 0.051330\n",
      "Epoch 718/1000 train loss: [array(0.0513304, dtype=float32)], val loss: 0.05133487284183502\n",
      "\t partial train loss (single batch): 0.052251\n",
      "Epoch 719/1000 train loss: [array(0.05225061, dtype=float32)], val loss: 0.052008580416440964\n",
      "\t partial train loss (single batch): 0.051570\n",
      "Epoch 720/1000 train loss: [array(0.05156976, dtype=float32)], val loss: 0.05019712448120117\n",
      "\t partial train loss (single batch): 0.051101\n",
      "Epoch 721/1000 train loss: [array(0.05110124, dtype=float32)], val loss: 0.052107203751802444\n",
      "\t partial train loss (single batch): 0.051725\n",
      "Epoch 722/1000 train loss: [array(0.05172501, dtype=float32)], val loss: 0.05303053557872772\n",
      "\t partial train loss (single batch): 0.051268\n",
      "Epoch 723/1000 train loss: [array(0.05126826, dtype=float32)], val loss: 0.05003851652145386\n",
      "\t partial train loss (single batch): 0.051220\n",
      "Epoch 724/1000 train loss: [array(0.05122048, dtype=float32)], val loss: 0.050878237932920456\n",
      "\t partial train loss (single batch): 0.051463\n",
      "Epoch 725/1000 train loss: [array(0.05146279, dtype=float32)], val loss: 0.05178362876176834\n",
      "\t partial train loss (single batch): 0.050814\n",
      "Epoch 726/1000 train loss: [array(0.05081442, dtype=float32)], val loss: 0.052315808832645416\n",
      "\t partial train loss (single batch): 0.051906\n",
      "Epoch 727/1000 train loss: [array(0.05190574, dtype=float32)], val loss: 0.05157937854528427\n",
      "\t partial train loss (single batch): 0.050820\n",
      "Epoch 728/1000 train loss: [array(0.05081974, dtype=float32)], val loss: 0.0516652911901474\n",
      "\t partial train loss (single batch): 0.051595\n",
      "Epoch 729/1000 train loss: [array(0.05159459, dtype=float32)], val loss: 0.05052340775728226\n",
      "\t partial train loss (single batch): 0.050260\n",
      "Epoch 730/1000 train loss: [array(0.05026029, dtype=float32)], val loss: 0.051791153848171234\n",
      "\t partial train loss (single batch): 0.049874\n",
      "Epoch 731/1000 train loss: [array(0.04987422, dtype=float32)], val loss: 0.05000826343894005\n",
      "\t partial train loss (single batch): 0.050521\n",
      "Epoch 732/1000 train loss: [array(0.05052134, dtype=float32)], val loss: 0.0516580231487751\n",
      "\t partial train loss (single batch): 0.051759\n",
      "Epoch 733/1000 train loss: [array(0.05175909, dtype=float32)], val loss: 0.051481299102306366\n",
      "\t partial train loss (single batch): 0.050488\n",
      "Epoch 734/1000 train loss: [array(0.0504876, dtype=float32)], val loss: 0.05029268562793732\n",
      "\t partial train loss (single batch): 0.050163\n",
      "Epoch 735/1000 train loss: [array(0.05016295, dtype=float32)], val loss: 0.05075618997216225\n",
      "\t partial train loss (single batch): 0.050105\n",
      "Epoch 736/1000 train loss: [array(0.05010464, dtype=float32)], val loss: 0.05140986293554306\n",
      "\t partial train loss (single batch): 0.049416\n",
      "Epoch 737/1000 train loss: [array(0.04941592, dtype=float32)], val loss: 0.05068215727806091\n",
      "\t partial train loss (single batch): 0.050863\n",
      "Epoch 738/1000 train loss: [array(0.05086338, dtype=float32)], val loss: 0.0505145825445652\n",
      "\t partial train loss (single batch): 0.052814\n",
      "Epoch 739/1000 train loss: [array(0.05281388, dtype=float32)], val loss: 0.05130276456475258\n",
      "\t partial train loss (single batch): 0.050558\n",
      "Epoch 740/1000 train loss: [array(0.050558, dtype=float32)], val loss: 0.05189279466867447\n",
      "\t partial train loss (single batch): 0.051254\n",
      "Epoch 741/1000 train loss: [array(0.05125429, dtype=float32)], val loss: 0.05136797949671745\n",
      "\t partial train loss (single batch): 0.050605\n",
      "Epoch 742/1000 train loss: [array(0.05060479, dtype=float32)], val loss: 0.05144365131855011\n",
      "\t partial train loss (single batch): 0.051515\n",
      "Epoch 743/1000 train loss: [array(0.05151463, dtype=float32)], val loss: 0.049706511199474335\n",
      "\t partial train loss (single batch): 0.050198\n",
      "Epoch 744/1000 train loss: [array(0.05019788, dtype=float32)], val loss: 0.04990818724036217\n",
      "\t partial train loss (single batch): 0.051137\n",
      "Epoch 745/1000 train loss: [array(0.05113708, dtype=float32)], val loss: 0.05005083605647087\n",
      "\t partial train loss (single batch): 0.050407\n",
      "Epoch 746/1000 train loss: [array(0.05040678, dtype=float32)], val loss: 0.05198359861969948\n",
      "\t partial train loss (single batch): 0.050274\n",
      "Epoch 747/1000 train loss: [array(0.05027356, dtype=float32)], val loss: 0.052218321710824966\n",
      "\t partial train loss (single batch): 0.051657\n",
      "Epoch 748/1000 train loss: [array(0.0516573, dtype=float32)], val loss: 0.05197616666555405\n",
      "\t partial train loss (single batch): 0.052197\n",
      "Epoch 749/1000 train loss: [array(0.05219715, dtype=float32)], val loss: 0.05245143547654152\n",
      "\t partial train loss (single batch): 0.050306\n",
      "Epoch 750/1000 train loss: [array(0.05030581, dtype=float32)], val loss: 0.052903126925230026\n",
      "\t partial train loss (single batch): 0.049916\n",
      "Epoch 751/1000 train loss: [array(0.0499157, dtype=float32)], val loss: 0.05133089795708656\n",
      "\t partial train loss (single batch): 0.050119\n",
      "Epoch 752/1000 train loss: [array(0.05011931, dtype=float32)], val loss: 0.051194313913583755\n",
      "\t partial train loss (single batch): 0.051219\n",
      "Epoch 753/1000 train loss: [array(0.05121862, dtype=float32)], val loss: 0.05116155743598938\n",
      "\t partial train loss (single batch): 0.049911\n",
      "Epoch 754/1000 train loss: [array(0.04991096, dtype=float32)], val loss: 0.052002616226673126\n",
      "\t partial train loss (single batch): 0.051931\n",
      "Epoch 755/1000 train loss: [array(0.05193105, dtype=float32)], val loss: 0.05114303156733513\n",
      "\t partial train loss (single batch): 0.050663\n",
      "Epoch 756/1000 train loss: [array(0.05066342, dtype=float32)], val loss: 0.051256995648145676\n",
      "\t partial train loss (single batch): 0.050570\n",
      "Epoch 757/1000 train loss: [array(0.05057042, dtype=float32)], val loss: 0.050929613411426544\n",
      "\t partial train loss (single batch): 0.050619\n",
      "Epoch 758/1000 train loss: [array(0.05061916, dtype=float32)], val loss: 0.05209984630346298\n",
      "\t partial train loss (single batch): 0.050650\n",
      "Epoch 759/1000 train loss: [array(0.05064962, dtype=float32)], val loss: 0.05067817494273186\n",
      "\t partial train loss (single batch): 0.052125\n",
      "Epoch 760/1000 train loss: [array(0.05212545, dtype=float32)], val loss: 0.05144503340125084\n",
      "\t partial train loss (single batch): 0.050423\n",
      "Epoch 761/1000 train loss: [array(0.05042301, dtype=float32)], val loss: 0.050761591643095016\n",
      "\t partial train loss (single batch): 0.049050\n",
      "Epoch 762/1000 train loss: [array(0.04905043, dtype=float32)], val loss: 0.050096094608306885\n",
      "\t partial train loss (single batch): 0.050855\n",
      "Epoch 763/1000 train loss: [array(0.0508546, dtype=float32)], val loss: 0.049786899238824844\n",
      "\t partial train loss (single batch): 0.049992\n",
      "Epoch 764/1000 train loss: [array(0.04999239, dtype=float32)], val loss: 0.05259143188595772\n",
      "\t partial train loss (single batch): 0.051054\n",
      "Epoch 765/1000 train loss: [array(0.05105369, dtype=float32)], val loss: 0.0524907223880291\n",
      "\t partial train loss (single batch): 0.050310\n",
      "Epoch 766/1000 train loss: [array(0.05030991, dtype=float32)], val loss: 0.04830068349838257\n",
      "\t partial train loss (single batch): 0.049462\n",
      "Epoch 767/1000 train loss: [array(0.0494624, dtype=float32)], val loss: 0.051318004727363586\n",
      "\t partial train loss (single batch): 0.049774\n",
      "Epoch 768/1000 train loss: [array(0.04977367, dtype=float32)], val loss: 0.05031488835811615\n",
      "\t partial train loss (single batch): 0.048886\n",
      "Epoch 769/1000 train loss: [array(0.04888559, dtype=float32)], val loss: 0.0510844811797142\n",
      "\t partial train loss (single batch): 0.050937\n",
      "Epoch 770/1000 train loss: [array(0.05093743, dtype=float32)], val loss: 0.05094340443611145\n",
      "\t partial train loss (single batch): 0.051624\n",
      "Epoch 771/1000 train loss: [array(0.05162397, dtype=float32)], val loss: 0.05061616376042366\n",
      "\t partial train loss (single batch): 0.050071\n",
      "Epoch 772/1000 train loss: [array(0.05007096, dtype=float32)], val loss: 0.05107811093330383\n",
      "\t partial train loss (single batch): 0.050080\n",
      "Epoch 773/1000 train loss: [array(0.05007954, dtype=float32)], val loss: 0.05240844190120697\n",
      "\t partial train loss (single batch): 0.050965\n",
      "Epoch 774/1000 train loss: [array(0.05096455, dtype=float32)], val loss: 0.05203517526388168\n",
      "\t partial train loss (single batch): 0.048896\n",
      "Epoch 775/1000 train loss: [array(0.04889588, dtype=float32)], val loss: 0.052275121212005615\n",
      "\t partial train loss (single batch): 0.049810\n",
      "Epoch 776/1000 train loss: [array(0.04980955, dtype=float32)], val loss: 0.05068475380539894\n",
      "\t partial train loss (single batch): 0.050474\n",
      "Epoch 777/1000 train loss: [array(0.05047449, dtype=float32)], val loss: 0.05177164450287819\n",
      "\t partial train loss (single batch): 0.049436\n",
      "Epoch 778/1000 train loss: [array(0.04943591, dtype=float32)], val loss: 0.05074332281947136\n",
      "\t partial train loss (single batch): 0.050182\n",
      "Epoch 779/1000 train loss: [array(0.05018225, dtype=float32)], val loss: 0.05243566259741783\n",
      "\t partial train loss (single batch): 0.049831\n",
      "Epoch 780/1000 train loss: [array(0.04983085, dtype=float32)], val loss: 0.051595430821180344\n",
      "\t partial train loss (single batch): 0.052860\n",
      "Epoch 781/1000 train loss: [array(0.05286007, dtype=float32)], val loss: 0.05109382048249245\n",
      "\t partial train loss (single batch): 0.049966\n",
      "Epoch 782/1000 train loss: [array(0.04996591, dtype=float32)], val loss: 0.05050331726670265\n",
      "\t partial train loss (single batch): 0.050226\n",
      "Epoch 783/1000 train loss: [array(0.05022588, dtype=float32)], val loss: 0.05062292143702507\n",
      "\t partial train loss (single batch): 0.051282\n",
      "Epoch 784/1000 train loss: [array(0.05128244, dtype=float32)], val loss: 0.0519702173769474\n",
      "\t partial train loss (single batch): 0.050465\n",
      "Epoch 785/1000 train loss: [array(0.0504646, dtype=float32)], val loss: 0.04979097470641136\n",
      "\t partial train loss (single batch): 0.050396\n",
      "Epoch 786/1000 train loss: [array(0.05039647, dtype=float32)], val loss: 0.04891180619597435\n",
      "\t partial train loss (single batch): 0.050970\n",
      "Epoch 787/1000 train loss: [array(0.05097027, dtype=float32)], val loss: 0.048967450857162476\n",
      "\t partial train loss (single batch): 0.050270\n",
      "Epoch 788/1000 train loss: [array(0.05026968, dtype=float32)], val loss: 0.05213544890284538\n",
      "\t partial train loss (single batch): 0.049801\n",
      "Epoch 789/1000 train loss: [array(0.04980143, dtype=float32)], val loss: 0.05061598867177963\n",
      "\t partial train loss (single batch): 0.050639\n",
      "Epoch 790/1000 train loss: [array(0.05063868, dtype=float32)], val loss: 0.05020757019519806\n",
      "\t partial train loss (single batch): 0.049172\n",
      "Epoch 791/1000 train loss: [array(0.04917165, dtype=float32)], val loss: 0.0500810481607914\n",
      "\t partial train loss (single batch): 0.051113\n",
      "Epoch 792/1000 train loss: [array(0.05111264, dtype=float32)], val loss: 0.0514647476375103\n",
      "\t partial train loss (single batch): 0.049428\n",
      "Epoch 793/1000 train loss: [array(0.0494285, dtype=float32)], val loss: 0.05010363459587097\n",
      "\t partial train loss (single batch): 0.049769\n",
      "Epoch 794/1000 train loss: [array(0.04976931, dtype=float32)], val loss: 0.049101874232292175\n",
      "\t partial train loss (single batch): 0.052452\n",
      "Epoch 795/1000 train loss: [array(0.05245182, dtype=float32)], val loss: 0.05075040087103844\n",
      "\t partial train loss (single batch): 0.049842\n",
      "Epoch 796/1000 train loss: [array(0.04984222, dtype=float32)], val loss: 0.05002497509121895\n",
      "\t partial train loss (single batch): 0.050235\n",
      "Epoch 797/1000 train loss: [array(0.05023494, dtype=float32)], val loss: 0.04975078999996185\n",
      "\t partial train loss (single batch): 0.049817\n",
      "Epoch 798/1000 train loss: [array(0.04981706, dtype=float32)], val loss: 0.050294943153858185\n",
      "\t partial train loss (single batch): 0.050299\n",
      "Epoch 799/1000 train loss: [array(0.05029907, dtype=float32)], val loss: 0.04999862238764763\n",
      "\t partial train loss (single batch): 0.050040\n",
      "Epoch 800/1000 train loss: [array(0.05003966, dtype=float32)], val loss: 0.051168277859687805\n",
      "\t partial train loss (single batch): 0.049372\n",
      "Epoch 801/1000 train loss: [array(0.04937201, dtype=float32)], val loss: 0.05030938237905502\n",
      "\t partial train loss (single batch): 0.050156\n",
      "Epoch 802/1000 train loss: [array(0.05015637, dtype=float32)], val loss: 0.05195587873458862\n",
      "\t partial train loss (single batch): 0.049488\n",
      "Epoch 803/1000 train loss: [array(0.04948771, dtype=float32)], val loss: 0.05092795565724373\n",
      "\t partial train loss (single batch): 0.051262\n",
      "Epoch 804/1000 train loss: [array(0.05126223, dtype=float32)], val loss: 0.05009181797504425\n",
      "\t partial train loss (single batch): 0.050220\n",
      "Epoch 805/1000 train loss: [array(0.05022002, dtype=float32)], val loss: 0.050748806446790695\n",
      "\t partial train loss (single batch): 0.049660\n",
      "Epoch 806/1000 train loss: [array(0.0496604, dtype=float32)], val loss: 0.048050589859485626\n",
      "\t partial train loss (single batch): 0.049751\n",
      "Epoch 807/1000 train loss: [array(0.04975066, dtype=float32)], val loss: 0.05070435255765915\n",
      "\t partial train loss (single batch): 0.048690\n",
      "Epoch 808/1000 train loss: [array(0.04868986, dtype=float32)], val loss: 0.04978937655687332\n",
      "\t partial train loss (single batch): 0.047951\n",
      "Epoch 809/1000 train loss: [array(0.04795053, dtype=float32)], val loss: 0.049917347729206085\n",
      "\t partial train loss (single batch): 0.048940\n",
      "Epoch 810/1000 train loss: [array(0.04893989, dtype=float32)], val loss: 0.05038737133145332\n",
      "\t partial train loss (single batch): 0.049239\n",
      "Epoch 811/1000 train loss: [array(0.04923874, dtype=float32)], val loss: 0.049200329929590225\n",
      "\t partial train loss (single batch): 0.050248\n",
      "Epoch 812/1000 train loss: [array(0.05024835, dtype=float32)], val loss: 0.05032472684979439\n",
      "\t partial train loss (single batch): 0.049710\n",
      "Epoch 813/1000 train loss: [array(0.0497101, dtype=float32)], val loss: 0.05035111680626869\n",
      "\t partial train loss (single batch): 0.050435\n",
      "Epoch 814/1000 train loss: [array(0.05043473, dtype=float32)], val loss: 0.0484100766479969\n",
      "\t partial train loss (single batch): 0.049325\n",
      "Epoch 815/1000 train loss: [array(0.04932523, dtype=float32)], val loss: 0.0499117411673069\n",
      "\t partial train loss (single batch): 0.049219\n",
      "Epoch 816/1000 train loss: [array(0.04921931, dtype=float32)], val loss: 0.05119386315345764\n",
      "\t partial train loss (single batch): 0.049630\n",
      "Epoch 817/1000 train loss: [array(0.04963033, dtype=float32)], val loss: 0.05017002299427986\n",
      "\t partial train loss (single batch): 0.051063\n",
      "Epoch 818/1000 train loss: [array(0.05106311, dtype=float32)], val loss: 0.05014222487807274\n",
      "\t partial train loss (single batch): 0.048905\n",
      "Epoch 819/1000 train loss: [array(0.04890484, dtype=float32)], val loss: 0.05003068968653679\n",
      "\t partial train loss (single batch): 0.049721\n",
      "Epoch 820/1000 train loss: [array(0.04972107, dtype=float32)], val loss: 0.04818544536828995\n",
      "\t partial train loss (single batch): 0.049355\n",
      "Epoch 821/1000 train loss: [array(0.04935538, dtype=float32)], val loss: 0.050812117755413055\n",
      "\t partial train loss (single batch): 0.051081\n",
      "Epoch 822/1000 train loss: [array(0.05108133, dtype=float32)], val loss: 0.05038347840309143\n",
      "\t partial train loss (single batch): 0.049027\n",
      "Epoch 823/1000 train loss: [array(0.04902663, dtype=float32)], val loss: 0.04992225766181946\n",
      "\t partial train loss (single batch): 0.049875\n",
      "Epoch 824/1000 train loss: [array(0.04987545, dtype=float32)], val loss: 0.05109217390418053\n",
      "\t partial train loss (single batch): 0.049118\n",
      "Epoch 825/1000 train loss: [array(0.04911813, dtype=float32)], val loss: 0.049282632768154144\n",
      "\t partial train loss (single batch): 0.048027\n",
      "Epoch 826/1000 train loss: [array(0.04802708, dtype=float32)], val loss: 0.049440354108810425\n",
      "\t partial train loss (single batch): 0.046543\n",
      "Epoch 827/1000 train loss: [array(0.04654337, dtype=float32)], val loss: 0.05034426227211952\n",
      "\t partial train loss (single batch): 0.050413\n",
      "Epoch 828/1000 train loss: [array(0.05041257, dtype=float32)], val loss: 0.05016423761844635\n",
      "\t partial train loss (single batch): 0.050050\n",
      "Epoch 829/1000 train loss: [array(0.05004969, dtype=float32)], val loss: 0.05026882141828537\n",
      "\t partial train loss (single batch): 0.048707\n",
      "Epoch 830/1000 train loss: [array(0.04870677, dtype=float32)], val loss: 0.05033217743039131\n",
      "\t partial train loss (single batch): 0.049854\n",
      "Epoch 831/1000 train loss: [array(0.04985378, dtype=float32)], val loss: 0.050966497510671616\n",
      "\t partial train loss (single batch): 0.049889\n",
      "Epoch 832/1000 train loss: [array(0.04988865, dtype=float32)], val loss: 0.049674142152071\n",
      "\t partial train loss (single batch): 0.050452\n",
      "Epoch 833/1000 train loss: [array(0.05045222, dtype=float32)], val loss: 0.05042388662695885\n",
      "\t partial train loss (single batch): 0.049537\n",
      "Epoch 834/1000 train loss: [array(0.04953666, dtype=float32)], val loss: 0.05095440894365311\n",
      "\t partial train loss (single batch): 0.050602\n",
      "Epoch 835/1000 train loss: [array(0.05060156, dtype=float32)], val loss: 0.050779789686203\n",
      "\t partial train loss (single batch): 0.049141\n",
      "Epoch 836/1000 train loss: [array(0.04914095, dtype=float32)], val loss: 0.04831256717443466\n",
      "\t partial train loss (single batch): 0.050614\n",
      "Epoch 837/1000 train loss: [array(0.05061427, dtype=float32)], val loss: 0.0492803230881691\n",
      "\t partial train loss (single batch): 0.048350\n",
      "Epoch 838/1000 train loss: [array(0.0483497, dtype=float32)], val loss: 0.04911985993385315\n",
      "\t partial train loss (single batch): 0.049748\n",
      "Epoch 839/1000 train loss: [array(0.04974809, dtype=float32)], val loss: 0.04960521310567856\n",
      "\t partial train loss (single batch): 0.050193\n",
      "Epoch 840/1000 train loss: [array(0.05019277, dtype=float32)], val loss: 0.052265431731939316\n",
      "\t partial train loss (single batch): 0.050039\n",
      "Epoch 841/1000 train loss: [array(0.0500392, dtype=float32)], val loss: 0.04906269907951355\n",
      "\t partial train loss (single batch): 0.050048\n",
      "Epoch 842/1000 train loss: [array(0.0500476, dtype=float32)], val loss: 0.049967117607593536\n",
      "\t partial train loss (single batch): 0.051361\n",
      "Epoch 843/1000 train loss: [array(0.05136111, dtype=float32)], val loss: 0.0490526407957077\n",
      "\t partial train loss (single batch): 0.049894\n",
      "Epoch 844/1000 train loss: [array(0.04989359, dtype=float32)], val loss: 0.050035446882247925\n",
      "\t partial train loss (single batch): 0.048618\n",
      "Epoch 845/1000 train loss: [array(0.04861827, dtype=float32)], val loss: 0.04872274398803711\n",
      "\t partial train loss (single batch): 0.048362\n",
      "Epoch 846/1000 train loss: [array(0.04836176, dtype=float32)], val loss: 0.05119878426194191\n",
      "\t partial train loss (single batch): 0.048355\n",
      "Epoch 847/1000 train loss: [array(0.04835502, dtype=float32)], val loss: 0.04986313357949257\n",
      "\t partial train loss (single batch): 0.050926\n",
      "Epoch 848/1000 train loss: [array(0.05092586, dtype=float32)], val loss: 0.05068035423755646\n",
      "\t partial train loss (single batch): 0.048195\n",
      "Epoch 849/1000 train loss: [array(0.04819481, dtype=float32)], val loss: 0.050036996603012085\n",
      "\t partial train loss (single batch): 0.051009\n",
      "Epoch 850/1000 train loss: [array(0.05100892, dtype=float32)], val loss: 0.048940520733594894\n",
      "\t partial train loss (single batch): 0.046340\n",
      "Epoch 851/1000 train loss: [array(0.04633961, dtype=float32)], val loss: 0.04750028997659683\n",
      "\t partial train loss (single batch): 0.049639\n",
      "Epoch 852/1000 train loss: [array(0.0496386, dtype=float32)], val loss: 0.05116894841194153\n",
      "\t partial train loss (single batch): 0.049278\n",
      "Epoch 853/1000 train loss: [array(0.04927764, dtype=float32)], val loss: 0.048971354961395264\n",
      "\t partial train loss (single batch): 0.049752\n",
      "Epoch 854/1000 train loss: [array(0.04975184, dtype=float32)], val loss: 0.049222104251384735\n",
      "\t partial train loss (single batch): 0.049546\n",
      "Epoch 855/1000 train loss: [array(0.04954606, dtype=float32)], val loss: 0.051170676946640015\n",
      "\t partial train loss (single batch): 0.048105\n",
      "Epoch 856/1000 train loss: [array(0.0481047, dtype=float32)], val loss: 0.050120607018470764\n",
      "\t partial train loss (single batch): 0.048445\n",
      "Epoch 857/1000 train loss: [array(0.04844506, dtype=float32)], val loss: 0.050157710909843445\n",
      "\t partial train loss (single batch): 0.048599\n",
      "Epoch 858/1000 train loss: [array(0.04859877, dtype=float32)], val loss: 0.05130281299352646\n",
      "\t partial train loss (single batch): 0.049910\n",
      "Epoch 859/1000 train loss: [array(0.04991048, dtype=float32)], val loss: 0.050111737102270126\n",
      "\t partial train loss (single batch): 0.049127\n",
      "Epoch 860/1000 train loss: [array(0.04912734, dtype=float32)], val loss: 0.049036458134651184\n",
      "\t partial train loss (single batch): 0.049283\n",
      "Epoch 861/1000 train loss: [array(0.04928255, dtype=float32)], val loss: 0.04873710125684738\n",
      "\t partial train loss (single batch): 0.049514\n",
      "Epoch 862/1000 train loss: [array(0.04951417, dtype=float32)], val loss: 0.04930533841252327\n",
      "\t partial train loss (single batch): 0.048071\n",
      "Epoch 863/1000 train loss: [array(0.04807051, dtype=float32)], val loss: 0.04819197952747345\n",
      "\t partial train loss (single batch): 0.048355\n",
      "Epoch 864/1000 train loss: [array(0.04835503, dtype=float32)], val loss: 0.051388926804065704\n",
      "\t partial train loss (single batch): 0.049630\n",
      "Epoch 865/1000 train loss: [array(0.04963038, dtype=float32)], val loss: 0.04933811351656914\n",
      "\t partial train loss (single batch): 0.049651\n",
      "Epoch 866/1000 train loss: [array(0.04965099, dtype=float32)], val loss: 0.04910818487405777\n",
      "\t partial train loss (single batch): 0.050421\n",
      "Epoch 867/1000 train loss: [array(0.05042102, dtype=float32)], val loss: 0.04954741895198822\n",
      "\t partial train loss (single batch): 0.049244\n",
      "Epoch 868/1000 train loss: [array(0.04924382, dtype=float32)], val loss: 0.04960110783576965\n",
      "\t partial train loss (single batch): 0.049001\n",
      "Epoch 869/1000 train loss: [array(0.04900085, dtype=float32)], val loss: 0.05084025114774704\n",
      "\t partial train loss (single batch): 0.048788\n",
      "Epoch 870/1000 train loss: [array(0.04878818, dtype=float32)], val loss: 0.04824911803007126\n",
      "\t partial train loss (single batch): 0.048227\n",
      "Epoch 871/1000 train loss: [array(0.04822683, dtype=float32)], val loss: 0.0489177405834198\n",
      "\t partial train loss (single batch): 0.049990\n",
      "Epoch 872/1000 train loss: [array(0.04999003, dtype=float32)], val loss: 0.04849901422858238\n",
      "\t partial train loss (single batch): 0.049760\n",
      "Epoch 873/1000 train loss: [array(0.0497601, dtype=float32)], val loss: 0.048647440969944\n",
      "\t partial train loss (single batch): 0.049836\n",
      "Epoch 874/1000 train loss: [array(0.04983614, dtype=float32)], val loss: 0.05036991834640503\n",
      "\t partial train loss (single batch): 0.047860\n",
      "Epoch 875/1000 train loss: [array(0.04785991, dtype=float32)], val loss: 0.0483570396900177\n",
      "\t partial train loss (single batch): 0.048236\n",
      "Epoch 876/1000 train loss: [array(0.04823646, dtype=float32)], val loss: 0.04968060180544853\n",
      "\t partial train loss (single batch): 0.047333\n",
      "Epoch 877/1000 train loss: [array(0.04733257, dtype=float32)], val loss: 0.05023902654647827\n",
      "\t partial train loss (single batch): 0.048889\n",
      "Epoch 878/1000 train loss: [array(0.04888925, dtype=float32)], val loss: 0.049516815692186356\n",
      "\t partial train loss (single batch): 0.049360\n",
      "Epoch 879/1000 train loss: [array(0.04936029, dtype=float32)], val loss: 0.04835638776421547\n",
      "\t partial train loss (single batch): 0.048743\n",
      "Epoch 880/1000 train loss: [array(0.04874274, dtype=float32)], val loss: 0.050503745675086975\n",
      "\t partial train loss (single batch): 0.049070\n",
      "Epoch 881/1000 train loss: [array(0.04907021, dtype=float32)], val loss: 0.04789014160633087\n",
      "\t partial train loss (single batch): 0.049411\n",
      "Epoch 882/1000 train loss: [array(0.04941118, dtype=float32)], val loss: 0.049882955849170685\n",
      "\t partial train loss (single batch): 0.048069\n",
      "Epoch 883/1000 train loss: [array(0.04806927, dtype=float32)], val loss: 0.04910038784146309\n",
      "\t partial train loss (single batch): 0.050639\n",
      "Epoch 884/1000 train loss: [array(0.05063897, dtype=float32)], val loss: 0.048282261937856674\n",
      "\t partial train loss (single batch): 0.049586\n",
      "Epoch 885/1000 train loss: [array(0.04958615, dtype=float32)], val loss: 0.04905358701944351\n",
      "\t partial train loss (single batch): 0.048790\n",
      "Epoch 886/1000 train loss: [array(0.04879031, dtype=float32)], val loss: 0.04937925562262535\n",
      "\t partial train loss (single batch): 0.048479\n",
      "Epoch 887/1000 train loss: [array(0.04847942, dtype=float32)], val loss: 0.04922911524772644\n",
      "\t partial train loss (single batch): 0.048721\n",
      "Epoch 888/1000 train loss: [array(0.04872119, dtype=float32)], val loss: 0.04980982095003128\n",
      "\t partial train loss (single batch): 0.048052\n",
      "Epoch 889/1000 train loss: [array(0.04805228, dtype=float32)], val loss: 0.048903778195381165\n",
      "\t partial train loss (single batch): 0.049221\n",
      "Epoch 890/1000 train loss: [array(0.04922071, dtype=float32)], val loss: 0.05004299059510231\n",
      "\t partial train loss (single batch): 0.047847\n",
      "Epoch 891/1000 train loss: [array(0.04784686, dtype=float32)], val loss: 0.049840718507766724\n",
      "\t partial train loss (single batch): 0.046430\n",
      "Epoch 892/1000 train loss: [array(0.04642968, dtype=float32)], val loss: 0.048740483820438385\n",
      "\t partial train loss (single batch): 0.047659\n",
      "Epoch 893/1000 train loss: [array(0.04765932, dtype=float32)], val loss: 0.05041295662522316\n",
      "\t partial train loss (single batch): 0.049195\n",
      "Epoch 894/1000 train loss: [array(0.0491954, dtype=float32)], val loss: 0.04749844968318939\n",
      "\t partial train loss (single batch): 0.049837\n",
      "Epoch 895/1000 train loss: [array(0.0498369, dtype=float32)], val loss: 0.047719474881887436\n",
      "\t partial train loss (single batch): 0.049626\n",
      "Epoch 896/1000 train loss: [array(0.04962621, dtype=float32)], val loss: 0.05023704096674919\n",
      "\t partial train loss (single batch): 0.049391\n",
      "Epoch 897/1000 train loss: [array(0.04939087, dtype=float32)], val loss: 0.04981708526611328\n",
      "\t partial train loss (single batch): 0.049200\n",
      "Epoch 898/1000 train loss: [array(0.04920004, dtype=float32)], val loss: 0.05031973496079445\n",
      "\t partial train loss (single batch): 0.049131\n",
      "Epoch 899/1000 train loss: [array(0.04913107, dtype=float32)], val loss: 0.048579197376966476\n",
      "\t partial train loss (single batch): 0.048175\n",
      "Epoch 900/1000 train loss: [array(0.04817484, dtype=float32)], val loss: 0.04757094010710716\n",
      "\t partial train loss (single batch): 0.048406\n",
      "Epoch 901/1000 train loss: [array(0.0484062, dtype=float32)], val loss: 0.04858146235346794\n",
      "\t partial train loss (single batch): 0.049001\n",
      "Epoch 902/1000 train loss: [array(0.04900114, dtype=float32)], val loss: 0.04911413788795471\n",
      "\t partial train loss (single batch): 0.049229\n",
      "Epoch 903/1000 train loss: [array(0.04922926, dtype=float32)], val loss: 0.04954065755009651\n",
      "\t partial train loss (single batch): 0.048709\n",
      "Epoch 904/1000 train loss: [array(0.04870875, dtype=float32)], val loss: 0.04922597110271454\n",
      "\t partial train loss (single batch): 0.048763\n",
      "Epoch 905/1000 train loss: [array(0.04876276, dtype=float32)], val loss: 0.05043529346585274\n",
      "\t partial train loss (single batch): 0.048705\n",
      "Epoch 906/1000 train loss: [array(0.04870499, dtype=float32)], val loss: 0.05021781474351883\n",
      "\t partial train loss (single batch): 0.049745\n",
      "Epoch 907/1000 train loss: [array(0.0497453, dtype=float32)], val loss: 0.04796752333641052\n",
      "\t partial train loss (single batch): 0.049039\n",
      "Epoch 908/1000 train loss: [array(0.04903901, dtype=float32)], val loss: 0.04799676313996315\n",
      "\t partial train loss (single batch): 0.047165\n",
      "Epoch 909/1000 train loss: [array(0.04716517, dtype=float32)], val loss: 0.049310777336359024\n",
      "\t partial train loss (single batch): 0.047361\n",
      "Epoch 910/1000 train loss: [array(0.04736088, dtype=float32)], val loss: 0.04736357182264328\n",
      "\t partial train loss (single batch): 0.048808\n",
      "Epoch 911/1000 train loss: [array(0.04880792, dtype=float32)], val loss: 0.048665933310985565\n",
      "\t partial train loss (single batch): 0.048403\n",
      "Epoch 912/1000 train loss: [array(0.04840265, dtype=float32)], val loss: 0.04835835099220276\n",
      "\t partial train loss (single batch): 0.048408\n",
      "Epoch 913/1000 train loss: [array(0.04840788, dtype=float32)], val loss: 0.04963245987892151\n",
      "\t partial train loss (single batch): 0.048843\n",
      "Epoch 914/1000 train loss: [array(0.04884348, dtype=float32)], val loss: 0.050011295825242996\n",
      "\t partial train loss (single batch): 0.049163\n",
      "Epoch 915/1000 train loss: [array(0.049163, dtype=float32)], val loss: 0.04797288402915001\n",
      "\t partial train loss (single batch): 0.048690\n",
      "Epoch 916/1000 train loss: [array(0.04869015, dtype=float32)], val loss: 0.04939465597271919\n",
      "\t partial train loss (single batch): 0.046512\n",
      "Epoch 917/1000 train loss: [array(0.04651166, dtype=float32)], val loss: 0.04814799129962921\n",
      "\t partial train loss (single batch): 0.047731\n",
      "Epoch 918/1000 train loss: [array(0.04773096, dtype=float32)], val loss: 0.048146285116672516\n",
      "\t partial train loss (single batch): 0.049009\n",
      "Epoch 919/1000 train loss: [array(0.04900948, dtype=float32)], val loss: 0.04820482060313225\n",
      "\t partial train loss (single batch): 0.047647\n",
      "Epoch 920/1000 train loss: [array(0.04764684, dtype=float32)], val loss: 0.04748140648007393\n",
      "\t partial train loss (single batch): 0.048904\n",
      "Epoch 921/1000 train loss: [array(0.04890413, dtype=float32)], val loss: 0.049748312681913376\n",
      "\t partial train loss (single batch): 0.047672\n",
      "Epoch 922/1000 train loss: [array(0.04767162, dtype=float32)], val loss: 0.050012510269880295\n",
      "\t partial train loss (single batch): 0.049732\n",
      "Epoch 923/1000 train loss: [array(0.04973191, dtype=float32)], val loss: 0.050315409898757935\n",
      "\t partial train loss (single batch): 0.048194\n",
      "Epoch 924/1000 train loss: [array(0.04819362, dtype=float32)], val loss: 0.04746810346841812\n",
      "\t partial train loss (single batch): 0.049209\n",
      "Epoch 925/1000 train loss: [array(0.04920892, dtype=float32)], val loss: 0.0492015965282917\n",
      "\t partial train loss (single batch): 0.046655\n",
      "Epoch 926/1000 train loss: [array(0.04665513, dtype=float32)], val loss: 0.04852970317006111\n",
      "\t partial train loss (single batch): 0.049585\n",
      "Epoch 927/1000 train loss: [array(0.04958451, dtype=float32)], val loss: 0.04808518663048744\n",
      "\t partial train loss (single batch): 0.045901\n",
      "Epoch 928/1000 train loss: [array(0.04590126, dtype=float32)], val loss: 0.048855967819690704\n",
      "\t partial train loss (single batch): 0.047885\n",
      "Epoch 929/1000 train loss: [array(0.04788548, dtype=float32)], val loss: 0.04780849814414978\n",
      "\t partial train loss (single batch): 0.048343\n",
      "Epoch 930/1000 train loss: [array(0.04834278, dtype=float32)], val loss: 0.048279426991939545\n",
      "\t partial train loss (single batch): 0.047566\n",
      "Epoch 931/1000 train loss: [array(0.04756569, dtype=float32)], val loss: 0.047442566603422165\n",
      "\t partial train loss (single batch): 0.047497\n",
      "Epoch 932/1000 train loss: [array(0.0474967, dtype=float32)], val loss: 0.048776838928461075\n",
      "\t partial train loss (single batch): 0.047618\n",
      "Epoch 933/1000 train loss: [array(0.04761769, dtype=float32)], val loss: 0.04843742027878761\n",
      "\t partial train loss (single batch): 0.047118\n",
      "Epoch 934/1000 train loss: [array(0.04711798, dtype=float32)], val loss: 0.048356931656599045\n",
      "\t partial train loss (single batch): 0.048144\n",
      "Epoch 935/1000 train loss: [array(0.04814428, dtype=float32)], val loss: 0.047801293432712555\n",
      "\t partial train loss (single batch): 0.048726\n",
      "Epoch 936/1000 train loss: [array(0.04872575, dtype=float32)], val loss: 0.049463432282209396\n",
      "\t partial train loss (single batch): 0.047652\n",
      "Epoch 937/1000 train loss: [array(0.04765249, dtype=float32)], val loss: 0.04855562746524811\n",
      "\t partial train loss (single batch): 0.048227\n",
      "Epoch 938/1000 train loss: [array(0.04822703, dtype=float32)], val loss: 0.050568316131830215\n",
      "\t partial train loss (single batch): 0.048140\n",
      "Epoch 939/1000 train loss: [array(0.04813997, dtype=float32)], val loss: 0.04867035150527954\n",
      "\t partial train loss (single batch): 0.048718\n",
      "Epoch 940/1000 train loss: [array(0.04871837, dtype=float32)], val loss: 0.04968930780887604\n",
      "\t partial train loss (single batch): 0.048062\n",
      "Epoch 941/1000 train loss: [array(0.04806182, dtype=float32)], val loss: 0.046278003603219986\n",
      "\t partial train loss (single batch): 0.048691\n",
      "Epoch 942/1000 train loss: [array(0.04869068, dtype=float32)], val loss: 0.0488768108189106\n",
      "\t partial train loss (single batch): 0.048312\n",
      "Epoch 943/1000 train loss: [array(0.04831189, dtype=float32)], val loss: 0.0461718775331974\n",
      "\t partial train loss (single batch): 0.048439\n",
      "Epoch 944/1000 train loss: [array(0.04843944, dtype=float32)], val loss: 0.0486181303858757\n",
      "\t partial train loss (single batch): 0.049176\n",
      "Epoch 945/1000 train loss: [array(0.04917626, dtype=float32)], val loss: 0.04990379139780998\n",
      "\t partial train loss (single batch): 0.047296\n",
      "Epoch 946/1000 train loss: [array(0.04729585, dtype=float32)], val loss: 0.04670260474085808\n",
      "\t partial train loss (single batch): 0.047488\n",
      "Epoch 947/1000 train loss: [array(0.04748823, dtype=float32)], val loss: 0.048004068434238434\n",
      "\t partial train loss (single batch): 0.049245\n",
      "Epoch 948/1000 train loss: [array(0.04924547, dtype=float32)], val loss: 0.050331659615039825\n",
      "\t partial train loss (single batch): 0.048853\n",
      "Epoch 949/1000 train loss: [array(0.04885306, dtype=float32)], val loss: 0.04917468875646591\n",
      "\t partial train loss (single batch): 0.048711\n",
      "Epoch 950/1000 train loss: [array(0.04871098, dtype=float32)], val loss: 0.04662220925092697\n",
      "\t partial train loss (single batch): 0.049675\n",
      "Epoch 951/1000 train loss: [array(0.04967508, dtype=float32)], val loss: 0.04783711954951286\n",
      "\t partial train loss (single batch): 0.048295\n",
      "Epoch 952/1000 train loss: [array(0.04829549, dtype=float32)], val loss: 0.04759462550282478\n",
      "\t partial train loss (single batch): 0.048537\n",
      "Epoch 953/1000 train loss: [array(0.04853689, dtype=float32)], val loss: 0.04771608114242554\n",
      "\t partial train loss (single batch): 0.048646\n",
      "Epoch 954/1000 train loss: [array(0.04864614, dtype=float32)], val loss: 0.04724375903606415\n",
      "\t partial train loss (single batch): 0.047292\n",
      "Epoch 955/1000 train loss: [array(0.04729168, dtype=float32)], val loss: 0.04755803942680359\n",
      "\t partial train loss (single batch): 0.049344\n",
      "Epoch 956/1000 train loss: [array(0.04934367, dtype=float32)], val loss: 0.04773281142115593\n",
      "\t partial train loss (single batch): 0.046769\n",
      "Epoch 957/1000 train loss: [array(0.04676946, dtype=float32)], val loss: 0.04962688684463501\n",
      "\t partial train loss (single batch): 0.047204\n",
      "Epoch 958/1000 train loss: [array(0.04720358, dtype=float32)], val loss: 0.04934977367520332\n",
      "\t partial train loss (single batch): 0.048838\n",
      "Epoch 959/1000 train loss: [array(0.04883756, dtype=float32)], val loss: 0.048104461282491684\n",
      "\t partial train loss (single batch): 0.048878\n",
      "Epoch 960/1000 train loss: [array(0.04887783, dtype=float32)], val loss: 0.04684687778353691\n",
      "\t partial train loss (single batch): 0.046574\n",
      "Epoch 961/1000 train loss: [array(0.04657359, dtype=float32)], val loss: 0.04848840460181236\n",
      "\t partial train loss (single batch): 0.049239\n",
      "Epoch 962/1000 train loss: [array(0.04923924, dtype=float32)], val loss: 0.0468362495303154\n",
      "\t partial train loss (single batch): 0.047924\n",
      "Epoch 963/1000 train loss: [array(0.04792437, dtype=float32)], val loss: 0.04811187833547592\n",
      "\t partial train loss (single batch): 0.047724\n",
      "Epoch 964/1000 train loss: [array(0.04772361, dtype=float32)], val loss: 0.04778164625167847\n",
      "\t partial train loss (single batch): 0.047388\n",
      "Epoch 965/1000 train loss: [array(0.04738789, dtype=float32)], val loss: 0.047702740877866745\n",
      "\t partial train loss (single batch): 0.049715\n",
      "Epoch 966/1000 train loss: [array(0.0497153, dtype=float32)], val loss: 0.04840060696005821\n",
      "\t partial train loss (single batch): 0.048745\n",
      "Epoch 967/1000 train loss: [array(0.04874486, dtype=float32)], val loss: 0.047359175980091095\n",
      "\t partial train loss (single batch): 0.048300\n",
      "Epoch 968/1000 train loss: [array(0.04829982, dtype=float32)], val loss: 0.04771146923303604\n",
      "\t partial train loss (single batch): 0.048521\n",
      "Epoch 969/1000 train loss: [array(0.0485211, dtype=float32)], val loss: 0.04767364263534546\n",
      "\t partial train loss (single batch): 0.047042\n",
      "Epoch 970/1000 train loss: [array(0.04704225, dtype=float32)], val loss: 0.047453224658966064\n",
      "\t partial train loss (single batch): 0.047457\n",
      "Epoch 971/1000 train loss: [array(0.04745688, dtype=float32)], val loss: 0.04837825894355774\n",
      "\t partial train loss (single batch): 0.046276\n",
      "Epoch 972/1000 train loss: [array(0.04627609, dtype=float32)], val loss: 0.048214852809906006\n",
      "\t partial train loss (single batch): 0.047190\n",
      "Epoch 973/1000 train loss: [array(0.04718953, dtype=float32)], val loss: 0.04872376099228859\n",
      "\t partial train loss (single batch): 0.048266\n",
      "Epoch 974/1000 train loss: [array(0.04826565, dtype=float32)], val loss: 0.04741496592760086\n",
      "\t partial train loss (single batch): 0.048263\n",
      "Epoch 975/1000 train loss: [array(0.04826347, dtype=float32)], val loss: 0.047641366720199585\n",
      "\t partial train loss (single batch): 0.048435\n",
      "Epoch 976/1000 train loss: [array(0.04843483, dtype=float32)], val loss: 0.047870442271232605\n",
      "\t partial train loss (single batch): 0.046610\n",
      "Epoch 977/1000 train loss: [array(0.04660952, dtype=float32)], val loss: 0.047513339668512344\n",
      "\t partial train loss (single batch): 0.046783\n",
      "Epoch 978/1000 train loss: [array(0.04678308, dtype=float32)], val loss: 0.04837159812450409\n",
      "\t partial train loss (single batch): 0.048761\n",
      "Epoch 979/1000 train loss: [array(0.04876085, dtype=float32)], val loss: 0.048278965055942535\n",
      "\t partial train loss (single batch): 0.047462\n",
      "Epoch 980/1000 train loss: [array(0.04746196, dtype=float32)], val loss: 0.04733991250395775\n",
      "\t partial train loss (single batch): 0.047572\n",
      "Epoch 981/1000 train loss: [array(0.04757172, dtype=float32)], val loss: 0.04820811375975609\n",
      "\t partial train loss (single batch): 0.046145\n",
      "Epoch 982/1000 train loss: [array(0.04614477, dtype=float32)], val loss: 0.04788101837038994\n",
      "\t partial train loss (single batch): 0.047826\n",
      "Epoch 983/1000 train loss: [array(0.04782571, dtype=float32)], val loss: 0.048110172152519226\n",
      "\t partial train loss (single batch): 0.047846\n",
      "Epoch 984/1000 train loss: [array(0.0478465, dtype=float32)], val loss: 0.04734956845641136\n",
      "\t partial train loss (single batch): 0.048625\n",
      "Epoch 985/1000 train loss: [array(0.04862455, dtype=float32)], val loss: 0.047814369201660156\n",
      "\t partial train loss (single batch): 0.046921\n",
      "Epoch 986/1000 train loss: [array(0.04692127, dtype=float32)], val loss: 0.04647102579474449\n",
      "\t partial train loss (single batch): 0.047943\n",
      "Epoch 987/1000 train loss: [array(0.04794271, dtype=float32)], val loss: 0.04817156866192818\n",
      "\t partial train loss (single batch): 0.047332\n",
      "Epoch 988/1000 train loss: [array(0.0473318, dtype=float32)], val loss: 0.0487978421151638\n",
      "\t partial train loss (single batch): 0.046502\n",
      "Epoch 989/1000 train loss: [array(0.04650159, dtype=float32)], val loss: 0.04882489889860153\n",
      "\t partial train loss (single batch): 0.048021\n",
      "Epoch 990/1000 train loss: [array(0.04802064, dtype=float32)], val loss: 0.04862917587161064\n",
      "\t partial train loss (single batch): 0.048432\n",
      "Epoch 991/1000 train loss: [array(0.04843216, dtype=float32)], val loss: 0.04708830267190933\n",
      "\t partial train loss (single batch): 0.048515\n",
      "Epoch 992/1000 train loss: [array(0.0485153, dtype=float32)], val loss: 0.04804724082350731\n",
      "\t partial train loss (single batch): 0.047655\n",
      "Epoch 993/1000 train loss: [array(0.04765464, dtype=float32)], val loss: 0.04815398529171944\n",
      "\t partial train loss (single batch): 0.046324\n",
      "Epoch 994/1000 train loss: [array(0.04632395, dtype=float32)], val loss: 0.04726347327232361\n",
      "\t partial train loss (single batch): 0.047506\n",
      "Epoch 995/1000 train loss: [array(0.04750619, dtype=float32)], val loss: 0.04763248562812805\n",
      "\t partial train loss (single batch): 0.048546\n",
      "Epoch 996/1000 train loss: [array(0.04854588, dtype=float32)], val loss: 0.04934206232428551\n",
      "\t partial train loss (single batch): 0.048540\n",
      "Epoch 997/1000 train loss: [array(0.0485399, dtype=float32)], val loss: 0.04795366898179054\n",
      "\t partial train loss (single batch): 0.046554\n",
      "Epoch 998/1000 train loss: [array(0.04655366, dtype=float32)], val loss: 0.047812286764383316\n",
      "\t partial train loss (single batch): 0.047322\n",
      "Epoch 999/1000 train loss: [array(0.0473224, dtype=float32)], val loss: 0.046972207725048065\n",
      "\t partial train loss (single batch): 0.046014\n",
      "Epoch 1000/1000 train loss: [array(0.04601391, dtype=float32)], val loss: 0.04748940095305443\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import random \n",
    "num_epochs = 1000\n",
    "history={'train_loss':[],'val_loss':[]}\n",
    "for epoch in range(num_epochs):\n",
    "   img_train, labels_train = generator.get_random_batch(training=True)\n",
    "   img_test, labels_test = generator.get_random_batch(training=False)\n",
    "   #print(img_train[0])\n",
    "   #img = img_train[0]\n",
    "   #plt.imshow(np.array(img).reshape((28, 28)))\n",
    "\n",
    "\n",
    "   img_train = torch.tensor(img_train)\n",
    "   img_train = img_train.type(torch.FloatTensor)\n",
    "   img_test = torch.tensor(img_test)\n",
    "   img_test = img_test.type(torch.FloatTensor)\n",
    "   train_loss = train_epoch(encoder,decoder,loss_fn,optim, img_train)\n",
    "   val_loss = test_epoch(encoder,decoder,loss_fn, img_test)\n",
    "   print(f\"Epoch {epoch+1}/{num_epochs} train loss: {train_loss}, val loss: {val_loss}\")\n",
    "   #print('\\n EPOCH {}/{} \\t train loss {:.3f} \\t val loss {:.3f}'.format(epoch + 1, num_epochs,train_loss,val_loss))\n",
    "   history['train_loss'].append(train_loss)\n",
    "   history['val_loss'].append(val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "plt.semilogy(history['train_loss'], label='Train')\n",
    "plt.semilogy(history['val_loss'], label='Valid')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Average Loss')\n",
    "#plt.grid()\n",
    "plt.legend()\n",
    "#plt.title('loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7ff135cbbd30>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "z = np.random.randn(100, 8)\n",
    "z = torch.Tensor(z)\n",
    "with torch.no_grad():\n",
    "    x_hat = decoder(z)\n",
    "x = np.array(x_hat[0]).reshape((28, 28))\n",
    "plt.imshow(x)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "49ff9a1d55cbad36b515c3ded8837e12145fab330794be4b4ac6e95d3772d975"
  },
  "kernelspec": {
   "display_name": "Python 3.8.0 ('tensorflow')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

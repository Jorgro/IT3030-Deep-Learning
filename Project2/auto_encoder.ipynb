{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras import layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AE Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_4 (InputLayer)        [(None, 28, 28, 1)]       0         \n",
      "                                                                 \n",
      " conv2d_12 (Conv2D)          (None, 28, 28, 16)        160       \n",
      "                                                                 \n",
      " max_pooling2d_9 (MaxPooling  (None, 14, 14, 16)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_13 (Conv2D)          (None, 14, 14, 2)         290       \n",
      "                                                                 \n",
      " max_pooling2d_10 (MaxPoolin  (None, 7, 7, 2)          0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_14 (Conv2D)          (None, 7, 7, 2)           38        \n",
      "                                                                 \n",
      " max_pooling2d_11 (MaxPoolin  (None, 7, 7, 2)          0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_transpose_8 (Conv2DT  (None, 14, 14, 4)        76        \n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      " conv2d_transpose_9 (Conv2DT  (None, 28, 28, 8)        296       \n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      " conv2d_transpose_10 (Conv2D  (None, 56, 56, 16)       1168      \n",
      " Transpose)                                                      \n",
      "                                                                 \n",
      " conv2d_15 (Conv2D)          (None, 56, 56, 1)         145       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,173\n",
      "Trainable params: 2,173\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_img = keras.Input(shape=(28, 28, 1))\n",
    "\n",
    "# Encoder\n",
    "x = layers.Conv2D(16, (3, 3), activation=\"relu\", padding=\"same\")(input_img)\n",
    "x = layers.MaxPooling2D((2, 2), padding=\"same\")(x)\n",
    "x = layers.Conv2D(2, (3, 3), activation=\"relu\", padding=\"same\")(x)\n",
    "x = layers.MaxPooling2D((2, 2), padding=\"same\")(x)\n",
    "x = layers.Conv2D(2, (3, 3), activation=\"relu\", padding=\"same\")(x)\n",
    "encoded = layers.MaxPooling2D((1, 1), padding=\"same\")(x)\n",
    "\n",
    "# Decoder\n",
    "decoder_input = layers.Conv2DTranspose(4, (3, 3), strides=2, activation=\"relu\", padding=\"same\")(encoded)\n",
    "x = layers.Conv2DTranspose(8, (3, 3), strides=2, activation=\"relu\", padding=\"same\")(decoder_input)\n",
    "x = layers.Conv2DTranspose(16, (3, 3), strides=2, activation=\"relu\", padding=\"same\")(x)\n",
    "decoded = layers.Conv2D(1, (3, 3), activation=\"sigmoid\", padding=\"same\")(x)\n",
    "autoencoder = keras.Model(input_img, decoded)\n",
    "autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "autoencoder.save_weights('initial_model.h5')\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stacked_mnist import StackedMNISTData, DataMode\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "\n",
    "generator = StackedMNISTData(mode=DataMode.MONO_BINARY_COMPLETE, default_batch_size=2048)\n",
    "\n",
    "x_train, y_train = generator.get_full_data_set(training=True)\n",
    "x_test, y_test = generator.get_full_data_set(training=False)\n",
    "\n",
    "# \"Translate\": Only look at \"red\" channel; only use the last digit. Use one-hot for labels during training\n",
    "x_train = x_train[:, :, :, [0]]\n",
    "y_train = keras.utils.to_categorical((y_train % 10).astype(np.int), 10)\n",
    "x_test = x_test[:, :, :, [0]]\n",
    "y_test = keras.utils.to_categorical((y_test % 10).astype(np.int), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/usr/local/anaconda3/envs/tensorflow/lib/python3.8/site-packages/keras/engine/training.py\", line 878, in train_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/anaconda3/envs/tensorflow/lib/python3.8/site-packages/keras/engine/training.py\", line 867, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/anaconda3/envs/tensorflow/lib/python3.8/site-packages/keras/engine/training.py\", line 860, in run_step  **\n        outputs = model.train_step(data)\n    File \"/usr/local/anaconda3/envs/tensorflow/lib/python3.8/site-packages/keras/engine/training.py\", line 809, in train_step\n        loss = self.compiled_loss(\n    File \"/usr/local/anaconda3/envs/tensorflow/lib/python3.8/site-packages/keras/engine/compile_utils.py\", line 201, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/usr/local/anaconda3/envs/tensorflow/lib/python3.8/site-packages/keras/losses.py\", line 141, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"/usr/local/anaconda3/envs/tensorflow/lib/python3.8/site-packages/keras/losses.py\", line 245, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/usr/local/anaconda3/envs/tensorflow/lib/python3.8/site-packages/keras/losses.py\", line 1807, in binary_crossentropy\n        backend.binary_crossentropy(y_true, y_pred, from_logits=from_logits),\n    File \"/usr/local/anaconda3/envs/tensorflow/lib/python3.8/site-packages/keras/backend.py\", line 5158, in binary_crossentropy\n        return tf.nn.sigmoid_cross_entropy_with_logits(labels=target, logits=output)\n\n    ValueError: `logits` and `labels` must have the same shape, received ((None, 32, 32, 1) vs (None, 28, 28, 1)).\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/w2/4g3c4yrn38g2nwwdhjdlx6fc0000gn/T/ipykernel_75915/3509386347.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m autoencoder.fit(x_train, x_train,\n\u001b[0m\u001b[1;32m      2\u001b[0m                 \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                 \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                 \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                 validation_data=(x_test, x_test))\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/tensorflow/lib/python3.8/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/tensorflow/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mautograph_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1127\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1128\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1129\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1130\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/usr/local/anaconda3/envs/tensorflow/lib/python3.8/site-packages/keras/engine/training.py\", line 878, in train_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/anaconda3/envs/tensorflow/lib/python3.8/site-packages/keras/engine/training.py\", line 867, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/anaconda3/envs/tensorflow/lib/python3.8/site-packages/keras/engine/training.py\", line 860, in run_step  **\n        outputs = model.train_step(data)\n    File \"/usr/local/anaconda3/envs/tensorflow/lib/python3.8/site-packages/keras/engine/training.py\", line 809, in train_step\n        loss = self.compiled_loss(\n    File \"/usr/local/anaconda3/envs/tensorflow/lib/python3.8/site-packages/keras/engine/compile_utils.py\", line 201, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/usr/local/anaconda3/envs/tensorflow/lib/python3.8/site-packages/keras/losses.py\", line 141, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"/usr/local/anaconda3/envs/tensorflow/lib/python3.8/site-packages/keras/losses.py\", line 245, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/usr/local/anaconda3/envs/tensorflow/lib/python3.8/site-packages/keras/losses.py\", line 1807, in binary_crossentropy\n        backend.binary_crossentropy(y_true, y_pred, from_logits=from_logits),\n    File \"/usr/local/anaconda3/envs/tensorflow/lib/python3.8/site-packages/keras/backend.py\", line 5158, in binary_crossentropy\n        return tf.nn.sigmoid_cross_entropy_with_logits(labels=target, logits=output)\n\n    ValueError: `logits` and `labels` must have the same shape, received ((None, 32, 32, 1) vs (None, 28, 28, 1)).\n"
     ]
    }
   ],
   "source": [
    "autoencoder.fit(x_train, x_train,\n",
    "                epochs=5,\n",
    "                batch_size=128,\n",
    "                shuffle=True,\n",
    "                validation_data=(x_test, x_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AE Reconstruction Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1440x288 with 20 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "decoded_imgs = autoencoder.predict(x_test)\n",
    "\n",
    "n = 10\n",
    "plt.figure(figsize=(20, 4))\n",
    "for i in range(1, n + 1):\n",
    "    # Display original\n",
    "    ax = plt.subplot(2, n, i)\n",
    "    plt.imshow(x_test[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "\n",
    "    # Display reconstruction\n",
    "    ax = plt.subplot(2, n, i + n)\n",
    "    plt.imshow(decoded_imgs[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy AE: 82.96%\n",
      "Accuracy Original: 95.80%\n"
     ]
    }
   ],
   "source": [
    "from verification_net import VerificationNet\n",
    "\n",
    "net = VerificationNet(force_learn=False, file_name=\"../models/verification_model\")\n",
    "img, labels = generator.get_random_batch(training=False, batch_size=25000)\n",
    "decoded_imgs = autoencoder.predict(img)\n",
    "_, acc_ae = net.check_predictability(data=decoded_imgs, correct_labels=labels)\n",
    "_, acc_orig = net.check_predictability(data=img, correct_labels=labels)\n",
    "print(f\"Accuracy AE: {100 * acc_ae:.2f}%\")\n",
    "print(f\"Accuracy Original: {100 * acc_orig:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AE as a generative model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "z = np.random.randn(100, 14, 14, 8)\n",
    "\n",
    "decoder = keras.Model(decoder_input, decoded)\n",
    "decoded_z = decoder.predict(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/w2/4g3c4yrn38g2nwwdhjdlx6fc0000gn/T/ipykernel_5869/3342549245.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m# Display original\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "n = 10\n",
    "plt.figure(figsize=(20, 4))\n",
    "for i in range(1, n + 1):\n",
    "    # Display original\n",
    "    ax = plt.subplot(2, n, i)\n",
    "    plt.imshow(decoded_z[i-1].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coverage: 20.00%\n"
     ]
    }
   ],
   "source": [
    "cov = net.check_class_coverage(data=decoded_z, tolerance=0.98)\n",
    "print(f\"Coverage: {100*cov:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AE as an anomaly detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.load_weights('initial_model.h5')\n",
    "\n",
    "generator = StackedMNISTData(mode=DataMode.MONO_BINARY_MISSING, default_batch_size=2048)\n",
    "\n",
    "x_train, y_train = generator.get_full_data_set(training=True)\n",
    "x_test, y_test = generator.get_full_data_set(training=False)\n",
    "\n",
    "# \"Translate\": Only look at \"red\" channel; only use the last digit. Use one-hot for labels during training\n",
    "x_train = x_train[:, :, :, [0]]\n",
    "y_train = keras.utils.to_categorical((y_train % 10).astype(np.int), 10)\n",
    "x_test = x_test[:, :, :, [0]]\n",
    "y_test = keras.utils.to_categorical((y_test % 10).astype(np.int), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.fit(x_train, x_train,\n",
    "                epochs=10,\n",
    "                batch_size=128,\n",
    "                shuffle=True,\n",
    "                validation_data=(x_test, x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstructed = autoencoder.predict(x_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.losses import binary_crossentropy\n",
    "losses = binary_crossentropy(x_test, reconstructed).numpy()\n",
    "largest_loss = np.argpartition(losses, -5)[-5:]\n",
    "\n",
    "plt.figure(figsize=(5, 4))\n",
    "for i in range(1, 5):\n",
    "    # Display original\n",
    "    ax = plt.subplot(i, 5)\n",
    "    plt.imshow(decoded_z[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "49ff9a1d55cbad36b515c3ded8837e12145fab330794be4b4ac6e95d3772d975"
  },
  "kernelspec": {
   "display_name": "Python 3.8.0 ('tensorflow')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
